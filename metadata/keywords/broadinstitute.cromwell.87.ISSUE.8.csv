id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/issues/4914:1914,Security,secur,security,1914,"rror: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4914:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914
https://github.com/broadinstitute/cromwell/issues/4915:155,Modifiability,variab,variable,155,"#4492 added a new `${CROMWELL_BUILD_CENTAUR_TEST_ADDITIONAL_PARAMETERS}`. This parameter is not used in most places. (Anywhere at the moment?) The way the variable is wired it currently resolves to `''`. When fed into `cromwell_test.sh` this empty `''` causes [`getopts`](https://github.com/broadinstitute/cromwell/blob/2326dd82cf579af7f50bcf92c59a44171ff26c8c/centaur/test_cromwell.sh#L45) to stop processing arguments. Thus, any `-e excluded`, `-i included`, `-d directory` etc. placed *after* `${CROMWELL_BUILD_CENTAUR_TEST_ADDITIONAL_PARAMETERS}` confusingly stops being parsed. A/C:; - Remove `${CROMWELL_BUILD_CENTAUR_TEST_ADDITIONAL_PARAMETERS}` if it really is unused. _OR_. - Wire in `${CROMWELL_BUILD_CENTAUR_TEST_ADDITIONAL_PARAMETERS}` any-other-way such that when it isn't provided it doesn't accidentally short circuit `getopts`. Example discussion: https://unix.stackexchange.com/questions/278544/how-to-pass-array-to-bash-shell-script",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4915
https://github.com/broadinstitute/cromwell/issues/4916:494,Availability,ERROR,ERROR,494,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:563,Availability,Error,Error,563,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:632,Availability,error,error,632,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:768,Availability,Error,Error,768,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:837,Availability,error,error,837,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:2996,Energy Efficiency,adapt,adapted,2996,val$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:27); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.bruteForcePathBuilders$1(WorkflowActor.scala:432); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:436); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:2996,Modifiability,adapt,adapted,2996,val$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:27); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.bruteForcePathBuilders$1(WorkflowActor.scala:432); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:436); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:477,Safety,abort,abortable,477,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:577,Security,access,access,577,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:782,Security,access,access,782,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:963,Security,validat,validateCredentials,963,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:1060,Security,validat,validateCredentials,1060,"default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:1166,Security,validat,validateCredentials,1166,"ed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:3642,Testability,Log,LoggingFSM,3642,or$$anonfun$8.applyOrElse(WorkflowActor.scala:436); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$Timers$$super$aroundReceive(WorkflowActor.scala:186); 	at akka.actor.Timers.aroundReceive(Timers.scala:55); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:186); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:3714,Testability,Log,LoggingFSM,3714,well.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$Timers$$super$aroundReceive(WorkflowActor.scala:186); 	at akka.actor.Timers.aroundReceive(Timers.scala:55); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:186); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:3769,Testability,Log,LoggingFSM,3769,se(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$Timers$$super$aroundReceive(WorkflowActor.scala:186); 	at akka.actor.Timers.aroundReceive(Timers.scala:55); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:186); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4916:158,Usability,Simpl,Simply,158,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916
https://github.com/broadinstitute/cromwell/issues/4917:115,Availability,ERROR,ERROR,115,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:316,Availability,Error,Error,316,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:334,Availability,Error,Error,334,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:593,Availability,error,errors,593,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1607,Availability,error,error,1607,"robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1659,Availability,error,error,1659,"n:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1982,Availability,Error,Error,1982,"{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:2000,Availability,Error,Error,2000,"{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:2259,Availability,error,errors,2259,"and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3273,Availability,error,error,3273,robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiR,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3325,Availability,error,error,3325,n:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:26,Deployability,Pipeline,PipelinesApiRequestManager,26,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3625,Deployability,pipeline,pipelines,3625,x}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3646,Deployability,Pipeline,PipelinesApiRequestWorker,3646,-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3681,Deployability,Pipeline,PipelinesApiRequestWorker,3681,ground:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3746,Deployability,pipeline,pipelines,3746,lelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.Fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3767,Deployability,Pipeline,PipelinesApiRequestWorker,3767,-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3817,Deployability,pipeline,pipelines,3817,s/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3838,Deployability,Pipeline,PipelinesApiRequestWorker,3838,s/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3877,Deployability,Pipeline,PipelinesApiRequestWorker,3877,s/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3942,Deployability,pipeline,pipelines,3942,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3963,Deployability,Pipeline,PipelinesApiRequestWorker,3963,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:4020,Deployability,Pipeline,PipelinesApiRequestWorker,4020,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:4192,Deployability,pipeline,pipelines,4192,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:4213,Deployability,Pipeline,PipelinesApiRequestWorker,4213,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:4253,Deployability,Pipeline,PipelinesApiRequestWorker,4253,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:841,Testability,log,logo,841,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1014,Testability,log,logo,1014,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1300,Testability,log,logo,1300,"cale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1444,Testability,log,logo,1444,";padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:1541,Testability,log,logo,1541,"kground:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/brandi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:2507,Testability,log,logo,2507,"epeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:2680,Testability,log,logo,2680,"orary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.Pipelin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:2966,Testability,log,logo,2966,"cale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3110,Testability,log,logo,3110,;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.acto,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4917:3207,Testability,log,logo,3207,kground:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917
https://github.com/broadinstitute/cromwell/issues/4918:295,Availability,ERROR,ERROR,295,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2731,Availability,robust,robustExecuteOrRecover,2731,per.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:51,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:3074,Availability,robust,robustExecuteOrRecover,3074,yncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.aroundReceive(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1384,Deployability,pipeline,pipelines,1384,uld be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1401,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1401,SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1481,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1481,mwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncEx,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1562,Deployability,pipeline,pipelines,1562,e validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1579,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1579,ings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.execu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1648,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1648,'/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1966,Deployability,pipeline,pipelines,1966,dValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1983,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1983,r$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2059,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2059,ardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2141,Deployability,pipeline,pipelines,2141,ardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2158,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2158,ntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2223,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2223, 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2536,Deployability,pipeline,pipelines,2536,mwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFun,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2553,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2553,mmon.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.Parti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:2612,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2612,idatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:3840,Deployability,pipeline,pipelines,3840,esApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.aroundReceive(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:3857,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3857,esApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.aroundReceive(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:3913,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3913,esApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.aroundReceive(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:350,Security,validat,validation,350,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:497,Security,validat,validation,497,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:508,Security,Validat,ValidatedRuntimeAttributesBuilder,508,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:569,Security,validat,validation,569,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:720,Security,validat,validation,720,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:731,Security,Validat,ValidatedRuntimeAttributesBuilder,731,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:771,Security,Validat,ValidatedRuntimeAttributesBuilder,771,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:837,Security,validat,validation,837,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:848,Security,Validat,ValidatedRuntimeAttributesBuilder,848,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:889,Security,Validat,ValidatedRuntimeAttributesBuilder,889,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1168,Security,validat,validatedRuntimeAttributes,1168,untime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1290,Security,validat,validatedRuntimeAttributes,1290,rOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsy,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1443,Security,validat,validatedRuntimeAttributes,1443,mwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncEx,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/issues/4918:1621,Security,validat,validatedRuntimeAttributes,1621,'/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918
https://github.com/broadinstitute/cromwell/pull/4919:149,Modifiability,config,config,149,"As discussed in #4759, including migration changes and workarounds for auto-incremented columns and large objects. I started modifying the travis-ci config but I'm probably going to need an assist there. Note: I have been testing this with Postgresql 9.6.1, because that's what we're using here. Other versions are TODO, it's looking like I'll have time to check v10 at least this week.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919
https://github.com/broadinstitute/cromwell/pull/4919:222,Testability,test,testing,222,"As discussed in #4759, including migration changes and workarounds for auto-incremented columns and large objects. I started modifying the travis-ci config but I'm probably going to need an assist there. Note: I have been testing this with Postgresql 9.6.1, because that's what we're using here. Other versions are TODO, it's looking like I'll have time to check v10 at least this week.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919
https://github.com/broadinstitute/cromwell/issues/4920:77,Availability,ERROR,ERROR,77,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:342,Availability,error,error,342,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:431,Availability,avail,available,431,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1469,Availability,recover,recoverWith,1469, type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:521,Deployability,pipeline,pipelines,521,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:538,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,538,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:599,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,599,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:680,Deployability,pipeline,pipelines,680,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:697,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,697,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:763,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,763,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:845,Deployability,pipeline,pipelines,845,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:862,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,862,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:927,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,927,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1009,Deployability,pipeline,pipelines,1009,7 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecuto,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1026,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1026,actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1091,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1091,flowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(A,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1442,Performance,concurren,concurrent,1442,fill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1512,Performance,concurren,concurrent,1512,gle.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1590,Performance,concurren,concurrent,1590,BackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1915,Performance,concurren,concurrent,1915,BackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/issues/4920:1469,Safety,recover,recoverWith,1469, type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920
https://github.com/broadinstitute/cromwell/pull/4922:132,Safety,detect,detect,132,"Inspired by stuff I've seen on other projects, e.g. https://github.com/atom/atom/blob/master/CONTRIBUTING.md. I believe Github will detect this file and show a little ""read this first"" indicator when opening a pull request: https://github.blog/2012-09-17-contributing-guidelines/. This is potentially a team discussion broader than ""two thumbs & merge"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922
https://github.com/broadinstitute/cromwell/pull/4922:268,Usability,guid,guidelines,268,"Inspired by stuff I've seen on other projects, e.g. https://github.com/atom/atom/blob/master/CONTRIBUTING.md. I believe Github will detect this file and show a little ""read this first"" indicator when opening a pull request: https://github.blog/2012-09-17-contributing-guidelines/. This is potentially a team discussion broader than ""two thumbs & merge"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922
https://github.com/broadinstitute/cromwell/pull/4924:45,Safety,sanity check,sanity check,45,Closes #4908. Combines the changes in #4909 (sanity check in case this happens again) and #4923 (should prevent it happening in the first place) and adds a test case for one scenario which was found to cause this problem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4924
https://github.com/broadinstitute/cromwell/pull/4924:156,Testability,test,test,156,Closes #4908. Combines the changes in #4909 (sanity check in case this happens again) and #4923 (should prevent it happening in the first place) and adds a test case for one scenario which was found to cause this problem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4924
https://github.com/broadinstitute/cromwell/pull/4930:42,Availability,heartbeat,heartbeats,42,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930
https://github.com/broadinstitute/cromwell/pull/4930:200,Modifiability,variab,variable,200,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930
https://github.com/broadinstitute/cromwell/pull/4930:63,Testability,test,tests,63,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930
https://github.com/broadinstitute/cromwell/pull/4930:90,Testability,test,tests,90,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930
https://github.com/broadinstitute/cromwell/pull/4930:113,Testability,test,tests,113,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930
https://github.com/broadinstitute/cromwell/pull/4931:0,Deployability,Hotfix,Hotfix,0,Hotfix edition of https://github.com/broadinstitute/cromwell/pull/4927,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4931
https://github.com/broadinstitute/cromwell/issues/4933:168,Availability,heartbeat,heartbeat,168,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4933:547,Availability,error,error,547,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4933:658,Availability,down,downstream,658,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4933:675,Availability,error,errors,675,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4933:100,Safety,timeout,timeouts,100,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4933:215,Safety,timeout,timeout,215,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4933:141,Testability,test,tests,141,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933
https://github.com/broadinstitute/cromwell/issues/4935:155,Availability,error,errors,155,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:239,Availability,Error,Error,239,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:299,Deployability,install,installed,299,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:1124,Deployability,configurat,configuration,1124,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:1124,Modifiability,config,configuration,1124,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:256,Performance,load,load,256,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:1169,Security,PASSWORD,PASSWORDS,1169,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/issues/4935:462,Usability,feedback,feedback,462,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935
https://github.com/broadinstitute/cromwell/pull/4938:1414,Security,access,access,1414,"This change adds source code line number information to the WOM structures. It works for WDL, but not for CWL. The overall approach is to add a `LexicalInformation` structure to the top level wom package, and pass this information from Hermes, to the *wdlom*, and then *wom*. Here is a glossary of the changes made for the WDL draft3 case. ; ; `wdl.transforms.base.ast2wdlom.GenericAst`; Definition of the AST we get as a result of running Hermes. This is what a workflow starts as. `LexicalInformation`; Defined in wom.LexicalInformation. `model.draft3.elements.WorkflowDefinitionElement`; The wdlom definition for a workflow. has lexical information structure. `wom.callable.WorkflowDefinition`; The WOM definition for a workflow. Added a lexInfo field. `wdl.transforms.base.ast2wdlom.AstToWorkflowDefinitionElement`; conversion from; `wdl.transforms.base.ast2wdlom.GenericAst` (which is supposed to have been generated from a WDL workflow) to ; `model.draft3.elements.WorkflowDefinitionElement`. `wdl.transforms.base.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition`; conversion from; `model.draft3.elements.WorkflowDefinitionElement`; to; `wom.callable.WorkflowDefinition`. Compilation and testing were done on projects `wdlTransformsBiscayne, wdlTransformsDraft2, wdlTransformsDraft3, wom, womtool`. The top level tests, from the root project, require various initialization files that I don't have access to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938
https://github.com/broadinstitute/cromwell/pull/4938:1204,Testability,test,testing,1204,"This change adds source code line number information to the WOM structures. It works for WDL, but not for CWL. The overall approach is to add a `LexicalInformation` structure to the top level wom package, and pass this information from Hermes, to the *wdlom*, and then *wom*. Here is a glossary of the changes made for the WDL draft3 case. ; ; `wdl.transforms.base.ast2wdlom.GenericAst`; Definition of the AST we get as a result of running Hermes. This is what a workflow starts as. `LexicalInformation`; Defined in wom.LexicalInformation. `model.draft3.elements.WorkflowDefinitionElement`; The wdlom definition for a workflow. has lexical information structure. `wom.callable.WorkflowDefinition`; The WOM definition for a workflow. Added a lexInfo field. `wdl.transforms.base.ast2wdlom.AstToWorkflowDefinitionElement`; conversion from; `wdl.transforms.base.ast2wdlom.GenericAst` (which is supposed to have been generated from a WDL workflow) to ; `model.draft3.elements.WorkflowDefinitionElement`. `wdl.transforms.base.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition`; conversion from; `model.draft3.elements.WorkflowDefinitionElement`; to; `wom.callable.WorkflowDefinition`. Compilation and testing were done on projects `wdlTransformsBiscayne, wdlTransformsDraft2, wdlTransformsDraft3, wom, womtool`. The top level tests, from the root project, require various initialization files that I don't have access to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938
https://github.com/broadinstitute/cromwell/pull/4938:1329,Testability,test,tests,1329,"This change adds source code line number information to the WOM structures. It works for WDL, but not for CWL. The overall approach is to add a `LexicalInformation` structure to the top level wom package, and pass this information from Hermes, to the *wdlom*, and then *wom*. Here is a glossary of the changes made for the WDL draft3 case. ; ; `wdl.transforms.base.ast2wdlom.GenericAst`; Definition of the AST we get as a result of running Hermes. This is what a workflow starts as. `LexicalInformation`; Defined in wom.LexicalInformation. `model.draft3.elements.WorkflowDefinitionElement`; The wdlom definition for a workflow. has lexical information structure. `wom.callable.WorkflowDefinition`; The WOM definition for a workflow. Added a lexInfo field. `wdl.transforms.base.ast2wdlom.AstToWorkflowDefinitionElement`; conversion from; `wdl.transforms.base.ast2wdlom.GenericAst` (which is supposed to have been generated from a WDL workflow) to ; `model.draft3.elements.WorkflowDefinitionElement`. `wdl.transforms.base.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition`; conversion from; `model.draft3.elements.WorkflowDefinitionElement`; to; `wom.callable.WorkflowDefinition`. Compilation and testing were done on projects `wdlTransformsBiscayne, wdlTransformsDraft2, wdlTransformsDraft3, wom, womtool`. The top level tests, from the root project, require various initialization files that I don't have access to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938
https://github.com/broadinstitute/cromwell/pull/4940:85,Availability,error,error,85,"Closes #4824 . I didn't do; >a sort of input_errors map with input names as keys and error(s) as values. because; 1. The underlying WOM creation returns pre-formatted strings (e.g. `Required workflow input 'wf_hello.hello.addressee' not specified`) and changing that interface would be a huge undertaking; 2. There's no neat way to handle extraneous inputs, since they obviously would not match any input name key in the map",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4940
https://github.com/broadinstitute/cromwell/pull/4940:267,Integrability,interface,interface,267,"Closes #4824 . I didn't do; >a sort of input_errors map with input names as keys and error(s) as values. because; 1. The underlying WOM creation returns pre-formatted strings (e.g. `Required workflow input 'wf_hello.hello.addressee' not specified`) and changing that interface would be a huge undertaking; 2. There's no neat way to handle extraneous inputs, since they obviously would not match any input name key in the map",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4940
https://github.com/broadinstitute/cromwell/pull/4941:43,Deployability,update,update,43,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/pull/4941:321,Deployability,update,update,321,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/pull/4941:749,Deployability,Hotfix,Hotfix,749,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/pull/4941:777,Deployability,Hotfix,Hotfix,777,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/pull/4941:907,Deployability,hotfix,hotfix,907,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/pull/4941:375,Safety,detect,detect,375,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/pull/4941:293,Testability,test,tests,293,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941
https://github.com/broadinstitute/cromwell/issues/4942:186,Deployability,pipeline,pipelines,186,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:242,Deployability,pipeline,pipelines,242,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:259,Deployability,Pipeline,PipelinesApiRuntimeAttributes,259,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:408,Deployability,release,release,408,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:524,Deployability,install,install-driver-script,524,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:566,Deployability,release,release,566,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:611,Deployability,upgrade,upgrade,611,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/issues/4942:730,Deployability,deploy,deploy,730,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942
https://github.com/broadinstitute/cromwell/pull/4943:194,Deployability,Update,Update,194,1. BCS supports outputs glob; 2. BCS supports call caching feature; 3. BCS mounts support Alibaba Cloud NAS; 4. Fix OSS filesystem channel leak issue; 5. BCS supports attach cluster runtime; 6. Update BCS outputs directory,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4943
https://github.com/broadinstitute/cromwell/issues/4945:563,Modifiability,config,config,563,"Hi,. I'm finding that Cromwell is using too many resources in a shared environment. It's spawning other threads and using up to 400% CPU, which I can't see why when the logs just show it's watching for jobs (maybe it's trying to hash files for call-caching?). I'd love a way to limit it, even at the expense of speed of the program, it would also be great if there was tooling (maybe an endpoint) to gauge the resource usage, and see what Cromwell's actually doing. For context, I'm running Cromwell on a (shared) login node, submitting jobs to Slurm (custom SFS config). The workflow is scattering a subworkflow 25 times, each with 4 steps. All of those have actually executed prior and it's taking a long time (sometimes > 30 minutes) to look up a call result. I am using call-caching with a mysql database.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945
https://github.com/broadinstitute/cromwell/issues/4945:229,Security,hash,hash,229,"Hi,. I'm finding that Cromwell is using too many resources in a shared environment. It's spawning other threads and using up to 400% CPU, which I can't see why when the logs just show it's watching for jobs (maybe it's trying to hash files for call-caching?). I'd love a way to limit it, even at the expense of speed of the program, it would also be great if there was tooling (maybe an endpoint) to gauge the resource usage, and see what Cromwell's actually doing. For context, I'm running Cromwell on a (shared) login node, submitting jobs to Slurm (custom SFS config). The workflow is scattering a subworkflow 25 times, each with 4 steps. All of those have actually executed prior and it's taking a long time (sometimes > 30 minutes) to look up a call result. I am using call-caching with a mysql database.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945
https://github.com/broadinstitute/cromwell/issues/4945:169,Testability,log,logs,169,"Hi,. I'm finding that Cromwell is using too many resources in a shared environment. It's spawning other threads and using up to 400% CPU, which I can't see why when the logs just show it's watching for jobs (maybe it's trying to hash files for call-caching?). I'd love a way to limit it, even at the expense of speed of the program, it would also be great if there was tooling (maybe an endpoint) to gauge the resource usage, and see what Cromwell's actually doing. For context, I'm running Cromwell on a (shared) login node, submitting jobs to Slurm (custom SFS config). The workflow is scattering a subworkflow 25 times, each with 4 steps. All of those have actually executed prior and it's taking a long time (sometimes > 30 minutes) to look up a call result. I am using call-caching with a mysql database.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945
https://github.com/broadinstitute/cromwell/issues/4945:514,Testability,log,login,514,"Hi,. I'm finding that Cromwell is using too many resources in a shared environment. It's spawning other threads and using up to 400% CPU, which I can't see why when the logs just show it's watching for jobs (maybe it's trying to hash files for call-caching?). I'd love a way to limit it, even at the expense of speed of the program, it would also be great if there was tooling (maybe an endpoint) to gauge the resource usage, and see what Cromwell's actually doing. For context, I'm running Cromwell on a (shared) login node, submitting jobs to Slurm (custom SFS config). The workflow is scattering a subworkflow 25 times, each with 4 steps. All of those have actually executed prior and it's taking a long time (sometimes > 30 minutes) to look up a call result. I am using call-caching with a mysql database.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945
https://github.com/broadinstitute/cromwell/issues/4946:378,Deployability,Pipeline,Pipeline,378,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946
https://github.com/broadinstitute/cromwell/issues/4946:486,Deployability,pipeline,pipelines,486,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946
https://github.com/broadinstitute/cromwell/issues/4946:544,Deployability,pipeline,pipelines,544,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946
https://github.com/broadinstitute/cromwell/issues/4946:45,Safety,timeout,timeout,45,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946
https://github.com/broadinstitute/cromwell/issues/4946:347,Safety,timeout,timeout,347,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946
https://github.com/broadinstitute/cromwell/pull/4949:39,Testability,test,tests,39,Actually this only removes the centaur tests for V1. Removing/disabling the backend itself and dealing with the repercussions is still a TODO.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4949
https://github.com/broadinstitute/cromwell/issues/4950:30,Modifiability,config,config,30,"Would it be possible to add a config option specifying the path(s) from which call caching results can be retrieved? I.e. if you determine that a task call from a Cromwell analysis with given uuid can be reused, Cromwell would then look for it under each `<specified-call-caching-path>/<workflow_name>/<analysis_uuid>/calls/<callName>`; rather than just under the original absolute path. Then call caches from different locations (or maybe different engines, cf. #4616) can be combined; the analysis UUID will still ensure that only the relevant calls will be reused. Also, the cromwell-executions directory could then be moved, including to a different filesystem or cloud, and the analyses could still be reused.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4950
https://github.com/broadinstitute/cromwell/issues/4950:398,Performance,cache,caches,398,"Would it be possible to add a config option specifying the path(s) from which call caching results can be retrieved? I.e. if you determine that a task call from a Cromwell analysis with given uuid can be reused, Cromwell would then look for it under each `<specified-call-caching-path>/<workflow_name>/<analysis_uuid>/calls/<callName>`; rather than just under the original absolute path. Then call caches from different locations (or maybe different engines, cf. #4616) can be combined; the analysis UUID will still ensure that only the relevant calls will be reused. Also, the cromwell-executions directory could then be moved, including to a different filesystem or cloud, and the analyses could still be reused.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4950
https://github.com/broadinstitute/cromwell/pull/4951:237,Availability,failure,failure,237,"These changes are side effects of my investigation into the `invalidate_bad_caches_jes_no_copy`; test on PAPIv1:. - Don't retry `invalidate_bad_caches_jes_no_copy` because it will never work the second time (even on v2); - Print out any failure metadata we get if we expected `Succeeded` to speed up the debug cycle; - Don't log the ""metadata mismatch"" as errors just because we're waiting for metadata consistency.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951
https://github.com/broadinstitute/cromwell/pull/4951:356,Availability,error,errors,356,"These changes are side effects of my investigation into the `invalidate_bad_caches_jes_no_copy`; test on PAPIv1:. - Don't retry `invalidate_bad_caches_jes_no_copy` because it will never work the second time (even on v2); - Print out any failure metadata we get if we expected `Succeeded` to speed up the debug cycle; - Don't log the ""metadata mismatch"" as errors just because we're waiting for metadata consistency.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951
https://github.com/broadinstitute/cromwell/pull/4951:97,Testability,test,test,97,"These changes are side effects of my investigation into the `invalidate_bad_caches_jes_no_copy`; test on PAPIv1:. - Don't retry `invalidate_bad_caches_jes_no_copy` because it will never work the second time (even on v2); - Print out any failure metadata we get if we expected `Succeeded` to speed up the debug cycle; - Don't log the ""metadata mismatch"" as errors just because we're waiting for metadata consistency.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951
https://github.com/broadinstitute/cromwell/pull/4951:325,Testability,log,log,325,"These changes are side effects of my investigation into the `invalidate_bad_caches_jes_no_copy`; test on PAPIv1:. - Don't retry `invalidate_bad_caches_jes_no_copy` because it will never work the second time (even on v2); - Print out any failure metadata we get if we expected `Succeeded` to speed up the debug cycle; - Don't log the ""metadata mismatch"" as errors just because we're waiting for metadata consistency.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951
https://github.com/broadinstitute/cromwell/pull/4952:0,Testability,Test,Tested,0,Tested against 5GB stderr file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4952
https://github.com/broadinstitute/cromwell/issues/4960:20,Availability,error,error,20,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960
https://github.com/broadinstitute/cromwell/issues/4960:124,Safety,abort,aborted-intermittently-without-any-exception,124,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960
https://github.com/broadinstitute/cromwell/issues/4960:484,Safety,abort,abort,484,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960
https://github.com/broadinstitute/cromwell/issues/4960:518,Safety,abort,aborts,518,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960
https://github.com/broadinstitute/cromwell/issues/4960:393,Testability,log,logic,393,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960
https://github.com/broadinstitute/cromwell/issues/4960:653,Testability,log,logic,653,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960
https://github.com/broadinstitute/cromwell/pull/4962:86,Availability,redundant,redundant,86,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962
https://github.com/broadinstitute/cromwell/pull/4962:15,Deployability,hotfix,hotfix,15,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962
https://github.com/broadinstitute/cromwell/pull/4962:34,Deployability,release,release,34,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962
https://github.com/broadinstitute/cromwell/pull/4962:142,Deployability,release,release,142,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962
https://github.com/broadinstitute/cromwell/pull/4962:86,Safety,redund,redundant,86,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962
https://github.com/broadinstitute/cromwell/issues/4963:498,Availability,echo,echo,498,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:575,Availability,Error,Error,575,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:616,Availability,error,error,616,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:417,Integrability,message,message,417,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:473,Integrability,message,message,473,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:505,Integrability,message,message,505,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:1836,Testability,Log,LoggingFSM,1836,"g failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:1929,Testability,Log,LoggingFSM,1929,"ue=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4963:1984,Testability,Log,LoggingFSM,1984,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963
https://github.com/broadinstitute/cromwell/issues/4965:1948,Deployability,configurat,configuration,1948,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:1948,Modifiability,config,configuration,1948,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:1993,Security,PASSWORD,PASSWORDS,1993,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:337,Testability,test,test,337,"It seems that cromwell is not localizing input files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:401,Testability,test,testFile,401,"It seems that cromwell is not localizing input files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:850,Testability,test,test,850,"It seems that cromwell is not localizing input files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:962,Testability,test,testFile,962,"It seems that cromwell is not localizing input files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:1081,Testability,test,test,1081,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:1224,Testability,test,testFile,1224,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4965:1286,Usability,feedback,feedback,1286,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputs⟫ ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputs⟫ ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965
https://github.com/broadinstitute/cromwell/issues/4966:49,Deployability,Pipeline,Pipeline,49,"Jobs submitted by [dsub][0] and [Google Genomics Pipeline Tools][1] both have the ability to start an SSH container in the background to allow one to inspect the runtime environment of a google genomics job in real-time. This can often be a very helpful tool to aid in diagnosing various kinds of troublesome workflow jobs and appreciating workflow tasks containing complex scripts and programs. Does cromwell support this feature? If so, how would one enable this? Otherwise, would it be possible to incorporate this feature in future releases?. [0]: https://github.com/DataBiosphere/dsub/releases/tag/v0.2.1; [1]: https://github.com/googlegenomics/pipelines-tools",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966
https://github.com/broadinstitute/cromwell/issues/4966:536,Deployability,release,releases,536,"Jobs submitted by [dsub][0] and [Google Genomics Pipeline Tools][1] both have the ability to start an SSH container in the background to allow one to inspect the runtime environment of a google genomics job in real-time. This can often be a very helpful tool to aid in diagnosing various kinds of troublesome workflow jobs and appreciating workflow tasks containing complex scripts and programs. Does cromwell support this feature? If so, how would one enable this? Otherwise, would it be possible to incorporate this feature in future releases?. [0]: https://github.com/DataBiosphere/dsub/releases/tag/v0.2.1; [1]: https://github.com/googlegenomics/pipelines-tools",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966
https://github.com/broadinstitute/cromwell/issues/4966:590,Deployability,release,releases,590,"Jobs submitted by [dsub][0] and [Google Genomics Pipeline Tools][1] both have the ability to start an SSH container in the background to allow one to inspect the runtime environment of a google genomics job in real-time. This can often be a very helpful tool to aid in diagnosing various kinds of troublesome workflow jobs and appreciating workflow tasks containing complex scripts and programs. Does cromwell support this feature? If so, how would one enable this? Otherwise, would it be possible to incorporate this feature in future releases?. [0]: https://github.com/DataBiosphere/dsub/releases/tag/v0.2.1; [1]: https://github.com/googlegenomics/pipelines-tools",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966
https://github.com/broadinstitute/cromwell/issues/4966:650,Deployability,pipeline,pipelines-tools,650,"Jobs submitted by [dsub][0] and [Google Genomics Pipeline Tools][1] both have the ability to start an SSH container in the background to allow one to inspect the runtime environment of a google genomics job in real-time. This can often be a very helpful tool to aid in diagnosing various kinds of troublesome workflow jobs and appreciating workflow tasks containing complex scripts and programs. Does cromwell support this feature? If so, how would one enable this? Otherwise, would it be possible to incorporate this feature in future releases?. [0]: https://github.com/DataBiosphere/dsub/releases/tag/v0.2.1; [1]: https://github.com/googlegenomics/pipelines-tools",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966
https://github.com/broadinstitute/cromwell/issues/4967:409,Availability,error,errors,409,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:1200,Availability,error,error,1200,"is HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:1814,Availability,avail,avail,1814,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:325,Deployability,configurat,configuration,325,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:441,Deployability,configurat,configuration,441,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:1650,Energy Efficiency,allocate,allocated,1650,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:220,Modifiability,config,configured,220,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:325,Modifiability,config,configuration,325,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:441,Modifiability,config,configuration,441,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:1500,Modifiability,variab,variable,1500,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:1703,Modifiability,variab,variable,1703,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4967:1719,Modifiability,config,config,1719,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967
https://github.com/broadinstitute/cromwell/issues/4969:125,Availability,error,errors,125,"With Cromwell 41, when submitting an imports.zip containing workflow imports, if the zipfile has a subdirectory, I get these errors:. 2019-05-14 11:05:38,560 cromwell-system-akka.dispatchers.engine-dispatcher-101 ERROR - WorkflowManagerActor Workflow a3fb73f1-2976-456\; 3-a691-a356e2744dda failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.Materiali\; zeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; /tmp/imports_workflow_a3fb73f1-2976-4563-a691-a356e2744dda_9012613107684347004.zip7783627211874195701/tasks/tasks_assembly.wdl; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$ma\; terialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processE\; vent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescr\; iptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969
https://github.com/broadinstitute/cromwell/issues/4969:213,Availability,ERROR,ERROR,213,"With Cromwell 41, when submitting an imports.zip containing workflow imports, if the zipfile has a subdirectory, I get these errors:. 2019-05-14 11:05:38,560 cromwell-system-akka.dispatchers.engine-dispatcher-101 ERROR - WorkflowManagerActor Workflow a3fb73f1-2976-456\; 3-a691-a356e2744dda failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.Materiali\; zeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; /tmp/imports_workflow_a3fb73f1-2976-4563-a691-a356e2744dda_9012613107684347004.zip7783627211874195701/tasks/tasks_assembly.wdl; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$ma\; terialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processE\; vent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescr\; iptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969
https://github.com/broadinstitute/cromwell/issues/4969:1462,Testability,Log,LoggingFSM,1462,workflow_a3fb73f1-2976-4563-a691-a356e2744dda_9012613107684347004.zip7783627211874195701/tasks/tasks_assembly.wdl; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$ma\; terialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processE\; vent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescr\; iptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDesc\; riptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.Fork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969
https://github.com/broadinstitute/cromwell/issues/4969:1557,Testability,Log,LoggingFSM,1557,83627211874195701/tasks/tasks_assembly.wdl; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$ma\; terialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processE\; vent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescr\; iptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDesc\; riptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969
https://github.com/broadinstitute/cromwell/issues/4969:1611,Testability,Log,LoggingFSM,1611,l.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$ma\; terialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processE\; vent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescr\; iptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDesc\; riptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969
https://github.com/broadinstitute/cromwell/pull/4976:74,Deployability,Pipeline,PipelinesApiRequestManagerSpec,74,"Closes #4917 . The ""should be confirmed"" part of the story is handled by `PipelinesApiRequestManagerSpec` where we have a mock `PipelinesApiRequestWorker` explode in various ways and check that the manager retries.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4976
https://github.com/broadinstitute/cromwell/pull/4976:128,Deployability,Pipeline,PipelinesApiRequestWorker,128,"Closes #4917 . The ""should be confirmed"" part of the story is handled by `PipelinesApiRequestManagerSpec` where we have a mock `PipelinesApiRequestWorker` explode in various ways and check that the manager retries.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4976
https://github.com/broadinstitute/cromwell/pull/4976:122,Testability,mock,mock,122,"Closes #4917 . The ""should be confirmed"" part of the story is handled by `PipelinesApiRequestManagerSpec` where we have a mock `PipelinesApiRequestWorker` explode in various ways and check that the manager retries.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4976
https://github.com/broadinstitute/cromwell/pull/4977:173,Testability,test,testing,173,"Adding source lines for calls and tasks. . Non obvious changes I had to make:; 1) Sorting edges when reporting graph cycles. This ensures a deterministic result, and allows testing.; 2) Added aliases to calls in `no_input_no_output_workflow.wdl`. This allows checking line numbers for each of the calls.; 3) Added line numbers to ScatterElements, because they sometimes create calls.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977
https://github.com/broadinstitute/cromwell/issues/4978:297,Availability,error,error,297,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978
https://github.com/broadinstitute/cromwell/issues/4978:680,Availability,error,error,680,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978
https://github.com/broadinstitute/cromwell/issues/4978:1368,Availability,resilien,resilient,1368,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978
https://github.com/broadinstitute/cromwell/issues/4978:1418,Availability,error,error,1418,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978
https://github.com/broadinstitute/cromwell/issues/4978:1472,Availability,resilien,resilient,1472,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978
https://github.com/broadinstitute/cromwell/issues/4978:670,Testability,log,logs,670,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978
https://github.com/broadinstitute/cromwell/issues/4979:138,Deployability,update,update,138,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:1843,Deployability,update,update,1843,"de to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; ------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:614,Modifiability,layers,layers,614,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:546,Safety,timeout,timeout,546,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:567,Safety,timeout,timeout,567,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:222,Security,secur,security,222,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:1230,Security,secur,security-tracker,1230,"rspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:1510,Security,secur,security-tracker,1510,"oadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:2006,Security,secur,security-tracker,2006,"corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:2290,Security,attack,attackers,2290,"8-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:2401,Security,secur,security-tracker,2401,".c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:2779,Security,secur,security-tracker,2779,"------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a denial of service or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-14062; -----------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:3124,Security,secur,security-tracker,3124,"------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a denial of service or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-14062; -----------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:3383,Security,attack,attackers,3383,"------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a denial of service or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-14062; -----------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:3474,Security,secur,security-tracker,3474,"------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a denial of service or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-14062; -----------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4979:271,Usability,clear,clear,271,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979
https://github.com/broadinstitute/cromwell/issues/4980:74,Deployability,continuous,continuous-testing,74,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:159,Deployability,deploy,deploy,159,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:238,Deployability,deploy,deploy,238,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:259,Deployability,deploy,deploy,259,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:780,Deployability,release,release,780,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:896,Performance,perform,performance,896,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:85,Testability,test,testing,85,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:452,Testability,test,test-until-it-fails,452,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:562,Testability,test,test-until-it-fails,562,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:985,Testability,test,tests,985,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4980:665,Usability,simpl,simple,665,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980
https://github.com/broadinstitute/cromwell/issues/4981:328,Availability,echo,echo,328,"I've attached two workflows that are essentially the same, one that conforms to draft-2 and the other that conforms to 1.0. The former runs fine (Cromwell v40), the latter one fails. It looks like there's a regression in handling the following situation:. ```wdl; task bar {; input {; Array[Array[String]]? baz; }; command <<<; echo ~{ if defined(baz) then write_tsv(baz) else 'no file' }; >>>; ...; }. Error:. ```java; Failed to process task definition 'bar' (reason 1 of 1): Failed to process expression 'if defined(baz) then write_tsv(baz) else """"' (reason 1 of 1): Invalid parameter 'IdentifierLookup(baz)'. Expected 'Array[Array[String]]' but got 'Array[Array[String]]?'; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981
https://github.com/broadinstitute/cromwell/issues/4981:403,Availability,Error,Error,403,"I've attached two workflows that are essentially the same, one that conforms to draft-2 and the other that conforms to 1.0. The former runs fine (Cromwell v40), the latter one fails. It looks like there's a regression in handling the following situation:. ```wdl; task bar {; input {; Array[Array[String]]? baz; }; command <<<; echo ~{ if defined(baz) then write_tsv(baz) else 'no file' }; >>>; ...; }. Error:. ```java; Failed to process task definition 'bar' (reason 1 of 1): Failed to process expression 'if defined(baz) then write_tsv(baz) else """"' (reason 1 of 1): Invalid parameter 'IdentifierLookup(baz)'. Expected 'Array[Array[String]]' but got 'Array[Array[String]]?'; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981
https://github.com/broadinstitute/cromwell/issues/4981:1524,Testability,Log,LoggingFSM,1524, (reason 1 of 1): Invalid parameter 'IdentifierLookup(baz)'. Expected 'Array[Array[String]]' but got 'Array[Array[String]]?'; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981
https://github.com/broadinstitute/cromwell/issues/4981:1616,Testability,Log,LoggingFSM,1616,'Array[Array[String]]' but got 'Array[Array[String]]?'; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981
https://github.com/broadinstitute/cromwell/issues/4981:1670,Testability,Log,LoggingFSM,1670,; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.di,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981
https://github.com/broadinstitute/cromwell/issues/4982:648,Availability,Error,Error,648,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982
https://github.com/broadinstitute/cromwell/issues/4982:654,Integrability,Message,Message,654,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982
https://github.com/broadinstitute/cromwell/issues/4982:1037,Testability,log,logs,1037,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982
https://github.com/broadinstitute/cromwell/issues/4982:1084,Testability,log,log,1084,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982
https://github.com/broadinstitute/cromwell/issues/4982:1260,Testability,log,log,1260,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982
https://github.com/broadinstitute/cromwell/issues/4982:1397,Testability,log,log,1397,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982
https://github.com/broadinstitute/cromwell/issues/4983:127,Deployability,release,releaseHold,127,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983
https://github.com/broadinstitute/cromwell/issues/4983:273,Deployability,release,releaseHold,273,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983
https://github.com/broadinstitute/cromwell/issues/4983:509,Deployability,release,releaseHold,509,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983
https://github.com/broadinstitute/cromwell/issues/4983:878,Deployability,release,release,878,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983
https://github.com/broadinstitute/cromwell/issues/4983:4,Energy Efficiency,Green,Green,4,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983
https://github.com/broadinstitute/cromwell/issues/4983:444,Integrability,rout,routes,444,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983
https://github.com/broadinstitute/cromwell/issues/4986:439,Performance,queue,queued,439,"Find - or implement - and then document a good answer to the question ""my job has been running for longer than it should. Is everything ok?"" . Bonus points if this solution could one day (maybe not today, maybe not tomorrow, but one day...) become a ""self-help"" button in Terra. Possible reasons for this we know about:. * You're still waiting for an execution token; * Call caching reading took (or is taking) a long time; * Your job was queued in PAPI for hours; * Your job got stuck in PAPI while running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4986
https://github.com/broadinstitute/cromwell/issues/4987:33,Modifiability,refactor,refactor,33,"If I could wave a magic wand and refactor the Cromwell support playbook, I would aim for it to have the following properties:. * Make it obvious where the starting point is; * A document which starts by listing links to 20 other documents is hard to get started with; * Make it indexed by problem; * 99% of the time, I've come to the playbook with a specific problem. What I'd really like to find as quickly as possible is a simple debugging flowchart for that problem.; * An ""am I ready"" and/or ""troubleshooting my permissions"" section; * Could even be structured like any other problem? Eg ; * Q: I can't connect to the Cromwell database; * A: try these 4 steps, in this order; * Eg 2:; * Q: I just started, how do I get ready to be on user liaison support?; * A: run through these 4 steps, in this order; * [Nice to have]: clear distinction between ""on call"" and ""user liaison"" types of issues because the audience for each is very different (Cromwell expert vs non-expert. Able to investigate vs getting back to operational as fast as possible); * Maybe even two separate documents? Maybe a tree of connected documents?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4987
https://github.com/broadinstitute/cromwell/issues/4987:425,Usability,simpl,simple,425,"If I could wave a magic wand and refactor the Cromwell support playbook, I would aim for it to have the following properties:. * Make it obvious where the starting point is; * A document which starts by listing links to 20 other documents is hard to get started with; * Make it indexed by problem; * 99% of the time, I've come to the playbook with a specific problem. What I'd really like to find as quickly as possible is a simple debugging flowchart for that problem.; * An ""am I ready"" and/or ""troubleshooting my permissions"" section; * Could even be structured like any other problem? Eg ; * Q: I can't connect to the Cromwell database; * A: try these 4 steps, in this order; * Eg 2:; * Q: I just started, how do I get ready to be on user liaison support?; * A: run through these 4 steps, in this order; * [Nice to have]: clear distinction between ""on call"" and ""user liaison"" types of issues because the audience for each is very different (Cromwell expert vs non-expert. Able to investigate vs getting back to operational as fast as possible); * Maybe even two separate documents? Maybe a tree of connected documents?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4987
https://github.com/broadinstitute/cromwell/issues/4987:826,Usability,clear,clear,826,"If I could wave a magic wand and refactor the Cromwell support playbook, I would aim for it to have the following properties:. * Make it obvious where the starting point is; * A document which starts by listing links to 20 other documents is hard to get started with; * Make it indexed by problem; * 99% of the time, I've come to the playbook with a specific problem. What I'd really like to find as quickly as possible is a simple debugging flowchart for that problem.; * An ""am I ready"" and/or ""troubleshooting my permissions"" section; * Could even be structured like any other problem? Eg ; * Q: I can't connect to the Cromwell database; * A: try these 4 steps, in this order; * Eg 2:; * Q: I just started, how do I get ready to be on user liaison support?; * A: run through these 4 steps, in this order; * [Nice to have]: clear distinction between ""on call"" and ""user liaison"" types of issues because the audience for each is very different (Cromwell expert vs non-expert. Able to investigate vs getting back to operational as fast as possible); * Maybe even two separate documents? Maybe a tree of connected documents?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4987
https://github.com/broadinstitute/cromwell/issues/4990:135,Safety,safe,safe,135,In response to:. * Issue with being too generous in production https://broadworkbench.atlassian.net/browse/PROD-137; * The (hopefully) safe but low sanity limit introduced in https://github.com/broadinstitute/firecloud-develop/pull/1652. AC: We should be able to:; * Perf test various values for this field in a repeatable way; * Make sure we don't regress on the level we find; * Set the production value to something more informed than just a wild guess,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4990
https://github.com/broadinstitute/cromwell/issues/4990:272,Testability,test,test,272,In response to:. * Issue with being too generous in production https://broadworkbench.atlassian.net/browse/PROD-137; * The (hopefully) safe but low sanity limit introduced in https://github.com/broadinstitute/firecloud-develop/pull/1652. AC: We should be able to:; * Perf test various values for this field in a repeatable way; * Make sure we don't regress on the level we find; * Set the production value to something more informed than just a wild guess,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4990
https://github.com/broadinstitute/cromwell/pull/4991:284,Availability,recover,recover,284,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991
https://github.com/broadinstitute/cromwell/pull/4991:167,Deployability,release,release,167,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991
https://github.com/broadinstitute/cromwell/pull/4991:478,Deployability,update,update,478,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991
https://github.com/broadinstitute/cromwell/pull/4991:284,Safety,recover,recover,284,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991
https://github.com/broadinstitute/cromwell/pull/4993:66,Usability,simpl,simple,66,NB: genuine reviews would be nice this time. The rebase was not a simple cherry pick and required considerable fixing up,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4993
https://github.com/broadinstitute/cromwell/issues/4994:32,Deployability,release,release,32,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:250,Deployability,Release,Releases,250,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:632,Deployability,Release,Releases,632,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:925,Deployability,Release,Release,925,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:979,Deployability,release,release,979,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1109,Deployability,release,release,1109,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1192,Deployability,release,release,1192,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1316,Deployability,release,release,1316,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1416,Deployability,release,release,1416,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1489,Deployability,release,release,1489,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1589,Deployability,release,release,1589,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1674,Deployability,release,release,1674,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1760,Deployability,release,release,1760,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1833,Deployability,release,release,1833,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1868,Deployability,release,release,1868,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:19,Modifiability,evolve,evolve,19,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:119,Testability,test,test,119,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:189,Testability,test,test,189,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:512,Testability,test,test,512,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:595,Testability,test,test,595,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:671,Testability,test,test,671,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:745,Testability,test,test,745,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:819,Testability,test,test,819,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:961,Testability,test,test,961,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1076,Testability,test,test,1076,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1159,Testability,test,test,1159,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1242,Testability,test,test,1242,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1295,Testability,test,test,1295,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1468,Testability,test,test,1468,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1648,Testability,test,test,1648,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/issues/4994:1734,Testability,test,test,1734,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994
https://github.com/broadinstitute/cromwell/pull/4996:36,Deployability,continuous,continuous,36,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:242,Energy Efficiency,adapt,adapt,242,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:242,Modifiability,adapt,adapt,242,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:47,Performance,perform,performance,47,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:93,Performance,perform,performance,93,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:133,Performance,perform,performance,133,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:233,Performance,perform,perform,233,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:440,Performance,perform,performance-testing,440,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:59,Testability,assert,assertions,59,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:105,Testability,test,testing,105,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:145,Testability,test,tests,145,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/pull/4996:452,Testability,test,testing,452,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996
https://github.com/broadinstitute/cromwell/issues/4997:311,Deployability,configurat,configuration,311,"Hello,. I'm wondering if cromwell has support for loading environment modules somehow? I need to port the same workflow between AWS (so, the docker runtime attribute is handy) and a slurm cluster (which uses module environment). Can they be specified as a runtime parameter for example? Can they be part of the configuration file to cromwell? If yes, any suggestions as to how?. Thank you, ; Azza",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997
https://github.com/broadinstitute/cromwell/issues/4997:311,Modifiability,config,configuration,311,"Hello,. I'm wondering if cromwell has support for loading environment modules somehow? I need to port the same workflow between AWS (so, the docker runtime attribute is handy) and a slurm cluster (which uses module environment). Can they be specified as a runtime parameter for example? Can they be part of the configuration file to cromwell? If yes, any suggestions as to how?. Thank you, ; Azza",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997
https://github.com/broadinstitute/cromwell/issues/4997:50,Performance,load,loading,50,"Hello,. I'm wondering if cromwell has support for loading environment modules somehow? I need to port the same workflow between AWS (so, the docker runtime attribute is handy) and a slurm cluster (which uses module environment). Can they be specified as a runtime parameter for example? Can they be part of the configuration file to cromwell? If yes, any suggestions as to how?. Thank you, ; Azza",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997
https://github.com/broadinstitute/cromwell/issues/4998:481,Availability,failure,failure-mode,481,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:164,Deployability,configurat,configuration,164,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:164,Modifiability,config,configuration,164,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:305,Modifiability,config,config,305,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:312,Modifiability,Config,ConfigBackendLifecycleActorFactory,312,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:349,Modifiability,config,config,349,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:968,Modifiability,refactor,refactoring,968,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:1142,Modifiability,refactor,refactored,1142,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/4998:369,Safety,timeout,timeout-seconds,369,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998
https://github.com/broadinstitute/cromwell/issues/5001:78,Availability,failure,failure,78,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:466,Availability,ERROR,ERROR,466,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:712,Availability,error,error,712,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:815,Availability,avail,available,815,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1864,Availability,recover,recoverWith,1864,te)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:251,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,251,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:904,Deployability,pipeline,pipelines,904,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:921,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,921,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:982,Deployability,Pipeline,PipelinesApiAsyncBack,982,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1065,Deployability,pipeline,pipelines,1065,"orkflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1082,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1082,"ike a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Bat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1148,Deployability,Pipeline,PipelinesApiAsyn,1148,"ries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1232,Deployability,pipeline,pipelines,1232,"her-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1249,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1249,"ckendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1314,Deployability,Pipeline,PipelinesApiAsync,1314,"o.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockConte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1398,Deployability,pipeline,pipelines,1398,"2:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1415,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1415,patchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1480,Deployability,Pipeline,PipelinesApiAsync,1480,tor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(Abst,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1837,Performance,concurren,concurrent,1837,quest. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1906,Performance,concurren,concurrent,1906,es.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1983,Performance,concurren,concurrent,1983,"nesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2019-05-24 12:32:08,259 cromwell-system-akka.dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:2304,Performance,concurren,concurrent,2304,"629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2019-05-24 12:32:08,259 cromwell-system-akka.dispatchers.engine-dispatcher-74 INFO - WorkflowManagerActor WorkflowActor-a309b1f1-2b35\; -UU-:@----F2 cromwell.log.10329.txt 9% L305 (Text) Fri May 24 13:02 1.17 --------------------------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:1864,Safety,recover,recoverWith,1864,te)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/issues/5001:3092,Testability,log,log,3092,"629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2019-05-24 12:32:08,259 cromwell-system-akka.dispatchers.engine-dispatcher-74 INFO - WorkflowManagerActor WorkflowActor-a309b1f1-2b35\; -UU-:@----F2 cromwell.log.10329.txt 9% L305 (Text) Fri May 24 13:02 1.17 --------------------------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001
https://github.com/broadinstitute/cromwell/pull/5002:51,Deployability,deploy,deployment,51,Adding additional process detail and a link to the deployment guide for CAAS. Link to the new documentation in situ: https://github.com/broadinstitute/cromwell/tree/cjl_caas_process/processes/release_processes#how-to-deploy-cromwell-in-caas-prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5002
https://github.com/broadinstitute/cromwell/pull/5002:217,Deployability,deploy,deploy-cromwell-in-caas-prod,217,Adding additional process detail and a link to the deployment guide for CAAS. Link to the new documentation in situ: https://github.com/broadinstitute/cromwell/tree/cjl_caas_process/processes/release_processes#how-to-deploy-cromwell-in-caas-prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5002
https://github.com/broadinstitute/cromwell/pull/5002:62,Usability,guid,guide,62,Adding additional process detail and a link to the deployment guide for CAAS. Link to the new documentation in situ: https://github.com/broadinstitute/cromwell/tree/cjl_caas_process/processes/release_processes#how-to-deploy-cromwell-in-caas-prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5002
https://github.com/broadinstitute/cromwell/pull/5003:747,Modifiability,refactor,refactored,747,"Fixes #4998. A bit of context: We have a cluster that works using SGE and it kills jobs that use too much VMEM. This means that any java jobs get sometimes randomly killed by the cluster. These jobs do not get a proper RC file because the cluster killed them. This meant that Cromwell would be waiting eternally for the RC file. Then my former colleague ffinfo created a new feature that assumed an externall kill of no RC file was found and the cluster reported the job as finished. #4112 . This code creates the exit file and reports the job as failed in this scenario. The comments implied he meant to put `137` in the RC file, as that is the exit code for SIGKILL (`kill -9`) but instead the code put the exit code `9` in there. This code was refactored by @cjllanwarne, and 9 was changed to a variable for the exit code: SIGTERM. Which made perfect sense. Unfortunately this broke some functionality. Jobs that are externally killed are no longer retried. This is because cromwell assumes that jobs with an exit code SIGINT(`130`), SIGKILL(`137`) or SIGTERM(`143`) are killed by cromwell, and should therefore not be restarted. This makes perfect sense. . This PR does not change the retry behaviour of cromwell. It does change the exit code for externally (not killed by cromwell!) jobs so these can be retried.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003
https://github.com/broadinstitute/cromwell/pull/5003:798,Modifiability,variab,variable,798,"Fixes #4998. A bit of context: We have a cluster that works using SGE and it kills jobs that use too much VMEM. This means that any java jobs get sometimes randomly killed by the cluster. These jobs do not get a proper RC file because the cluster killed them. This meant that Cromwell would be waiting eternally for the RC file. Then my former colleague ffinfo created a new feature that assumed an externall kill of no RC file was found and the cluster reported the job as finished. #4112 . This code creates the exit file and reports the job as failed in this scenario. The comments implied he meant to put `137` in the RC file, as that is the exit code for SIGKILL (`kill -9`) but instead the code put the exit code `9` in there. This code was refactored by @cjllanwarne, and 9 was changed to a variable for the exit code: SIGTERM. Which made perfect sense. Unfortunately this broke some functionality. Jobs that are externally killed are no longer retried. This is because cromwell assumes that jobs with an exit code SIGINT(`130`), SIGKILL(`137`) or SIGTERM(`143`) are killed by cromwell, and should therefore not be restarted. This makes perfect sense. . This PR does not change the retry behaviour of cromwell. It does change the exit code for externally (not killed by cromwell!) jobs so these can be retried.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003
https://github.com/broadinstitute/cromwell/issues/5004:424,Availability,error,error,424,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:509,Availability,error,error,509,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:815,Availability,failure,failure,815,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:898,Availability,failure,failure,898,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1656,Availability,heartbeat,heartbeat,1656,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1720,Availability,heartbeat,heartbeatInterval,1720,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:10398,Availability,error,error,10398,"019-05-22 19:19:26,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: job id: 7c2d29c2-f04e-4b3f-8579-915a6fbc9033; [2019-05-22 19:19:26,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:16862,Availability,reliab,reliably,16862,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:16886,Availability,error,error,16886,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:65,Deployability,pipeline,pipeline,65,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1473,Deployability,configurat,configuration,1473,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1666,Deployability,configurat,configuration,1666,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:3541,Deployability,Pipeline,Pipeline,3541,"mitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Starting workflow 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Successfully started WorkflowActor-3997371c-9513-4386-a579-a72639c6e960; ...; [2019-05-22 19:15:20,74] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.SplitFilesByChromosome; [2019-05-22 19:15:21,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: set -e; for chr in grep -v '@' /cromwell_root/s4-pbg-hc/References/HC_Panel_v3.intervals | cut -f1 | sort | uniq; do; grep -v '@' /cromwell_root/s4-pbg-hc/References/HC_Panel_v3.intervals | grep -w $chr | awk '{ print $1"":""$2""-""$3 }' > $chr.intervals; samtools view -@ 15 -b -h /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Alignment/alignment.Alignment/6e782168-d056-4ac9-b83b-5fba843fffc1/call-baseRecalibrator/shard-0/RSM278260-6_8plex.dedup.recal.bam $chr > $chr.RSM278260-6_8plex.dedup.recal.bam; done; [2019-05-22 19:15:21,35] [info] Submitting job to AWS Batch; [2019-05-22 19:15:21,35] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:15:21,35] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:15:21,35] [info] taskId: Haplotypecaller.SplitFilesByChromosome-None-1; [2019-05-22 19:15:21,35] [info] hostpath root: hc.Haplotypecaller/hc.SplitFilesByChromosome/755021ae-948b-47f9-94a8-66b486bda47d/None/1; [2019-05-22 19:15:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: job id: 8ec19f2b-5b49-4422-9ad1-5b51e3db9414; [2019-05-22 19:15:21,77",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:5383,Deployability,Pipeline,Pipeline,5383,"tchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: job id: 8ec19f2b-5b49-4422-9ad1-5b51e3db9414; [2019-05-22 19:15:21,77] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from - to Initializing; [2019-05-22 19:15:26,67] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:5965,Deployability,Pipeline,Pipeline,5965,"e:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:6258,Deployability,Pipeline,Pipeline,6258,"aplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:6760,Deployability,Pipeline,Pipeline,6760,"g19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:7342,Deployability,Pipeline,Pipeline,7342,"r/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:7635,Deployability,Pipeline,Pipeline,7635,"aplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(1)/1; ...; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:10658,Deployability,Pipeline,Pipeline,10658,"obExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:10909,Deployability,Pipeline,Pipeline,10909,"kendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:11195,Deployability,Pipeline,Pipeline,11195,"9-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...; [2019-05-22 19:42:10,31] [info] WorkflowManagerActor WorkflowActor-3997371c-9513-4386-a579-a72639c6e960 is in a terminal state: WorkflowFailedState; [2019-05-22 19:42:59,50] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; ...; Workflow 399737",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:11446,Deployability,Pipeline,Pipeline,11446,"ecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...; [2019-05-22 19:42:10,31] [info] WorkflowManagerActor WorkflowActor-3997371c-9513-4386-a579-a72639c6e960 is in a terminal state: WorkflowFailedState; [2019-05-22 19:42:59,50] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; ...; Workflow 3997371c-9513-4386-a579-a72639c6e960 transitioned to state Failed; ```. Pulling the actual AWS Batch Job parameters for the ""failed"" job (7c2d29c2-f04e-4b3f-8579-915a6fbc9033) I see the following:; ```; {""jobs"": [{; ""status"": ""SUCCEEDED"", ; ""container"": {; ""mountPoints"": [{",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:11743,Deployability,Pipeline,Pipeline,11743,"r/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...; [2019-05-22 19:42:10,31] [info] WorkflowManagerActor WorkflowActor-3997371c-9513-4386-a579-a72639c6e960 is in a terminal state: WorkflowFailedState; [2019-05-22 19:42:59,50] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; ...; Workflow 3997371c-9513-4386-a579-a72639c6e960 transitioned to state Failed; ```. Pulling the actual AWS Batch Job parameters for the ""failed"" job (7c2d29c2-f04e-4b3f-8579-915a6fbc9033) I see the following:; ```; {""jobs"": [{; ""status"": ""SUCCEEDED"", ; ""container"": {; ""mountPoints"": [{""sourceVolume"": ""local-disk"", ""containerPath"": ""/cromwell_root""}], ; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""image"": ""260062248592.dkr.ecr.us-east",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:15824,Integrability,depend,dependsOn,15824,"E"", ""value"": ""/cromwell_root/HC_GVCF-23-rc.txt""}, ; {""name"": ""AWS_CROMWELL_WORKFLOW_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/""}; ], ; ""vcpus"": 16, ; ""command"": [; ""gzipdata"", ""/bin/bash"", ""-c"", ; ""...""; ], ; ""volumes"": [{""host"": {; ""sourcePath"": ""/cromwell_root/hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ""name"": ""local-disk""}], ; ""memory"": 32000, ""ulimits"": [], ""exitCode"": 0}, ; ""parameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching N",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:16484,Integrability,interface,interfaces,16484,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1473,Modifiability,config,configuration,1473,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1666,Modifiability,config,configuration,1666,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:1963,Modifiability,config,configured,1963,"url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Starting workflow 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Successfully started WorkflowActor-3997371c-9513-4386-a579-a72639c6e960; ...; [2019-05-22 19:15:20,74] [info] 755021ae-948b-47f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:2077,Modifiability,config,configured,2077,"nfo] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Starting workflow 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Successfully started WorkflowActor-3997371c-9513-4386-a579-a72639c6e960; ...; [2019-05-22 19:15:20,74] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.SplitFilesB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:2198,Modifiability,config,configured,2198,"000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Starting workflow 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Successfully started WorkflowActor-3997371c-9513-4386-a579-a72639c6e960; ...; [2019-05-22 19:15:20,74] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.SplitFilesByChromosome; [2019-05-22 19:15:21,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFiles",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:5754,Modifiability,sandbox,sandbox,5754,"endJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:5840,Modifiability,sandbox,sandbox,5840,"e from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a7263",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:7131,Modifiability,sandbox,sandbox,7131,"86bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:7217,Modifiability,sandbox,sandbox,7217,"ls \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:2600",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:10544,Modifiability,Enhance,EnhancedCromwellIoException,10544,"obExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:4074,Performance,queue,queue,4074,"romosome; [2019-05-22 19:15:21,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: set -e; for chr in grep -v '@' /cromwell_root/s4-pbg-hc/References/HC_Panel_v3.intervals | cut -f1 | sort | uniq; do; grep -v '@' /cromwell_root/s4-pbg-hc/References/HC_Panel_v3.intervals | grep -w $chr | awk '{ print $1"":""$2""-""$3 }' > $chr.intervals; samtools view -@ 15 -b -h /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Alignment/alignment.Alignment/6e782168-d056-4ac9-b83b-5fba843fffc1/call-baseRecalibrator/shard-0/RSM278260-6_8plex.dedup.recal.bam $chr > $chr.RSM278260-6_8plex.dedup.recal.bam; done; [2019-05-22 19:15:21,35] [info] Submitting job to AWS Batch; [2019-05-22 19:15:21,35] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:15:21,35] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:15:21,35] [info] taskId: Haplotypecaller.SplitFilesByChromosome-None-1; [2019-05-22 19:15:21,35] [info] hostpath root: hc.Haplotypecaller/hc.SplitFilesByChromosome/755021ae-948b-47f9-94a8-66b486bda47d/None/1; [2019-05-22 19:15:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: job id: 8ec19f2b-5b49-4422-9ad1-5b51e3db9414; [2019-05-22 19:15:21,77] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from - to Initializing; [2019-05-22 19:15:26,67] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:8224,Performance,queue,queue,8224,"g19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(1)/1; ...; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(6)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(6)/1; ...; [2019-05-22 19:19:19,51] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-05-22 19:19:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:8756,Performance,queue,queue,8756,"e-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(1)/1; ...; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(6)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(6)/1; ...; [2019-05-22 19:19:19,51] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-05-22 19:19:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: job id: 45a77017-89a7-45c0-8b8b-d40ae2420212; [2019-05-22 19:19:21,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from - to Initializing; [2019-05-22 19:19:26,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: job id: 7c2d29c2-f04e-4b3f-8579-915a6fbc9033; [2019-05-22 19:19:26,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:9089,Performance,Perform,Performing,9089,"4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(1)/1; ...; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(6)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(6)/1; ...; [2019-05-22 19:19:19,51] [warn] Job definition already exists. Performing describe and retrieving latest revision.; [2019-05-22 19:19:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: job id: 45a77017-89a7-45c0-8b8b-d40ae2420212; [2019-05-22 19:19:21,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from - to Initializing; [2019-05-22 19:19:26,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: job id: 7c2d29c2-f04e-4b3f-8579-915a6fbc9033; [2019-05-22 19:19:26,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:15755,Performance,queue,queue,15755,"E"", ""value"": ""/cromwell_root/HC_GVCF-23-rc.txt""}, ; {""name"": ""AWS_CROMWELL_WORKFLOW_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/""}; ], ; ""vcpus"": 16, ; ""command"": [; ""gzipdata"", ""/bin/bash"", ""-c"", ; ""...""; ], ; ""volumes"": [{""host"": {; ""sourcePath"": ""/cromwell_root/hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ""name"": ""local-disk""}], ; ""memory"": 32000, ""ulimits"": [], ""exitCode"": 0}, ; ""parameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching N",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:208,Testability,log,log,208,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:249,Testability,test,testing,249,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:477,Testability,log,logs,477,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:872,Testability,log,log,872,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:5754,Testability,sandbox,sandbox,5754,"endJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:5840,Testability,sandbox,sandbox,5840,"e from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a7263",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:7131,Testability,sandbox,sandbox,7131,"86bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:7217,Testability,sandbox,sandbox,7217,"ls \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:2600",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:12653,Testability,log,logStreamName,12653,"all-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...; [2019-05-22 19:42:10,31] [info] WorkflowManagerActor WorkflowActor-3997371c-9513-4386-a579-a72639c6e960 is in a terminal state: WorkflowFailedState; [2019-05-22 19:42:59,50] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; ...; Workflow 3997371c-9513-4386-a579-a72639c6e960 transitioned to state Failed; ```. Pulling the actual AWS Batch Job parameters for the ""failed"" job (7c2d29c2-f04e-4b3f-8579-915a6fbc9033) I see the following:; ```; {""jobs"": [{; ""status"": ""SUCCEEDED"", ; ""container"": {; ""mountPoints"": [{""sourceVolume"": ""local-disk"", ""containerPath"": ""/cromwell_root""}], ; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""image"": ""260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-TN-alignandmolvar:1.3.2"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""environment"": [; {""name"": ""AWS_CROMWELL_LOCAL_DISK"", ""value"": ""/cromwell_root""}, ; {""name"": ""AWS_CROMWELL_CALL_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/call-HC_GVCF/shard-23""}, ; {""name"": ""AWS_CROMWELL_OUTPUTS"", ; ""value"": ""Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:14166,Testability,log,log,14166,"c.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/call-HC_GVCF/shard-23""}, ; {""name"": ""AWS_CROMWELL_OUTPUTS"", ; ""value"": ""Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/call-HC_GVCF/shard-23/Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,local-disk /cromwell_root;Run02_Pair003_Lane1_Tumor.hc.gvcf.gz.tbi,s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/call-HC_GVCF/shard-23/Run02_Pair003_Lane1_Tumor.hc.gvcf.gz.tbi,Run02_Pair003_Lane1_Tumor.hc.gvcf.gz.tbi,local-disk /cromwell_root""}, ; {""name"": ""AWS_CROMWELL_INPUTS_GZ"", ; ""value"": ""...""}, ; {""name"": ""AWS_CROMWELL_STDERR_FILE"", ""value"": ""/cromwell_root/HC_GVCF-23-stderr.log""}, ; {""name"": ""AWS_CROMWELL_STDOUT_FILE"", ""value"": ""/cromwell_root/HC_GVCF-23-stdout.log""}, ; {""name"": ""AWS_CROMWELL_PATH"", ""value"": ""hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ; {""name"": ""AWS_CROMWELL_RC_FILE"", ""value"": ""/cromwell_root/HC_GVCF-23-rc.txt""}, ; {""name"": ""AWS_CROMWELL_WORKFLOW_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/""}; ], ; ""vcpus"": 16, ; ""command"": [; ""gzipdata"", ""/bin/bash"", ""-c"", ; ""...""; ], ; ""volumes"": [{""host"": {; ""sourcePath"": ""/cromwell_root/hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ""name"": ""local-disk""}], ; ""memory"": 32000, ""ulimits"": [], ""exitCode"": 0}, ; ""parameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:14255,Testability,log,log,14255,"rd-23""}, ; {""name"": ""AWS_CROMWELL_OUTPUTS"", ; ""value"": ""Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/call-HC_GVCF/shard-23/Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,Run02_Pair003_Lane1_Tumor.hc.gvcf.gz,local-disk /cromwell_root;Run02_Pair003_Lane1_Tumor.hc.gvcf.gz.tbi,s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/call-HC_GVCF/shard-23/Run02_Pair003_Lane1_Tumor.hc.gvcf.gz.tbi,Run02_Pair003_Lane1_Tumor.hc.gvcf.gz.tbi,local-disk /cromwell_root""}, ; {""name"": ""AWS_CROMWELL_INPUTS_GZ"", ; ""value"": ""...""}, ; {""name"": ""AWS_CROMWELL_STDERR_FILE"", ""value"": ""/cromwell_root/HC_GVCF-23-stderr.log""}, ; {""name"": ""AWS_CROMWELL_STDOUT_FILE"", ""value"": ""/cromwell_root/HC_GVCF-23-stdout.log""}, ; {""name"": ""AWS_CROMWELL_PATH"", ""value"": ""hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ; {""name"": ""AWS_CROMWELL_RC_FILE"", ""value"": ""/cromwell_root/HC_GVCF-23-rc.txt""}, ; {""name"": ""AWS_CROMWELL_WORKFLOW_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/""}; ], ; ""vcpus"": 16, ; ""command"": [; ""gzipdata"", ""/bin/bash"", ""-c"", ; ""...""; ], ; ""volumes"": [{""host"": {; ""sourcePath"": ""/cromwell_root/hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ""name"": ""local-disk""}], ; ""memory"": 32000, ""ulimits"": [], ""exitCode"": 0}, ; ""parameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""contain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:15503,Testability,log,logStreamName,15503,"E"", ""value"": ""/cromwell_root/HC_GVCF-23-rc.txt""}, ; {""name"": ""AWS_CROMWELL_WORKFLOW_ROOT"", ; ""value"": ""s3://dev-nphi-cromwell-v8/cromwell-execution/TN_workflow/2b65d83d-7d30-465e-9127-95c6886056e4/call-Haplotypecaller/shard-1/hc.Haplotypecaller/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/""}; ], ; ""vcpus"": 16, ; ""command"": [; ""gzipdata"", ""/bin/bash"", ""-c"", ; ""...""; ], ; ""volumes"": [{""host"": {; ""sourcePath"": ""/cromwell_root/hc.Haplotypecaller/hc.HC_GVCF/cfe96bd8-ee6b-4ba5-8ed8-198e6f5f9589/Some(23)/1""}, ""name"": ""local-disk""}], ; ""memory"": 32000, ""ulimits"": [], ""exitCode"": 0}, ; ""parameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching N",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:16111,Testability,log,log,16111,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:16166,Testability,log,log,16166,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5004:15976,Usability,Clear,Clearly,15976,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004
https://github.com/broadinstitute/cromwell/issues/5006:95,Security,hash,hashing,95,"# Proxy Support in Cromwell. Are proxies supported in Cromwell? . Cromwell fails to use remote hashing behind my corporate proxy. However, I can get this to work outside the proxy. While behind the proxy, Cromwell can still pull Docker images and local hashing works as these rely on the Docker daemon which does utilize my proxies. ## Exceptions. These are the exceptions that Cromwell is raising behind the proxy:; ```; Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/debian%3Apull headers= threw an exception on attempt #4. Giving up.; java.net.ConnectException: Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```; ```; Docker lookup failed; java.lang.Exception: Failed to get docker hash for debian@sha256:75f7d0590b45561bfa443abad0b3e0f86e2811b1fc176f786cd30eb078d1846f Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```. ## Troubleshooting. I set my JVM proxies using the bash command ; ``` ; export _JAVA_OPTIONS='-Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport'; ```. And at the top of my cromwell logs I can see ; ```; Picked up _JAVA_OPTIONS: -Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport; ```. Of note, I have tried with and without the digest tag. Both are unsuccessful. . ## Cromwell Version. I am using cromwell 41.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006
https://github.com/broadinstitute/cromwell/issues/5006:253,Security,hash,hashing,253,"# Proxy Support in Cromwell. Are proxies supported in Cromwell? . Cromwell fails to use remote hashing behind my corporate proxy. However, I can get this to work outside the proxy. While behind the proxy, Cromwell can still pull Docker images and local hashing works as these rely on the Docker daemon which does utilize my proxies. ## Exceptions. These are the exceptions that Cromwell is raising behind the proxy:; ```; Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/debian%3Apull headers= threw an exception on attempt #4. Giving up.; java.net.ConnectException: Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```; ```; Docker lookup failed; java.lang.Exception: Failed to get docker hash for debian@sha256:75f7d0590b45561bfa443abad0b3e0f86e2811b1fc176f786cd30eb078d1846f Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```. ## Troubleshooting. I set my JVM proxies using the bash command ; ``` ; export _JAVA_OPTIONS='-Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport'; ```. And at the top of my cromwell logs I can see ; ```; Picked up _JAVA_OPTIONS: -Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport; ```. Of note, I have tried with and without the digest tag. Both are unsuccessful. . ## Cromwell Version. I am using cromwell 41.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006
https://github.com/broadinstitute/cromwell/issues/5006:771,Security,hash,hash,771,"# Proxy Support in Cromwell. Are proxies supported in Cromwell? . Cromwell fails to use remote hashing behind my corporate proxy. However, I can get this to work outside the proxy. While behind the proxy, Cromwell can still pull Docker images and local hashing works as these rely on the Docker daemon which does utilize my proxies. ## Exceptions. These are the exceptions that Cromwell is raising behind the proxy:; ```; Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/debian%3Apull headers= threw an exception on attempt #4. Giving up.; java.net.ConnectException: Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```; ```; Docker lookup failed; java.lang.Exception: Failed to get docker hash for debian@sha256:75f7d0590b45561bfa443abad0b3e0f86e2811b1fc176f786cd30eb078d1846f Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```. ## Troubleshooting. I set my JVM proxies using the bash command ; ``` ; export _JAVA_OPTIONS='-Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport'; ```. And at the top of my cromwell logs I can see ; ```; Picked up _JAVA_OPTIONS: -Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport; ```. Of note, I have tried with and without the digest tag. Both are unsuccessful. . ## Cromwell Version. I am using cromwell 41.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006
https://github.com/broadinstitute/cromwell/issues/5006:1188,Testability,log,logs,1188,"# Proxy Support in Cromwell. Are proxies supported in Cromwell? . Cromwell fails to use remote hashing behind my corporate proxy. However, I can get this to work outside the proxy. While behind the proxy, Cromwell can still pull Docker images and local hashing works as these rely on the Docker daemon which does utilize my proxies. ## Exceptions. These are the exceptions that Cromwell is raising behind the proxy:; ```; Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/debian%3Apull headers= threw an exception on attempt #4. Giving up.; java.net.ConnectException: Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```; ```; Docker lookup failed; java.lang.Exception: Failed to get docker hash for debian@sha256:75f7d0590b45561bfa443abad0b3e0f86e2811b1fc176f786cd30eb078d1846f Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```. ## Troubleshooting. I set my JVM proxies using the bash command ; ``` ; export _JAVA_OPTIONS='-Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport'; ```. And at the top of my cromwell logs I can see ; ```; Picked up _JAVA_OPTIONS: -Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport; ```. Of note, I have tried with and without the digest tag. Both are unsuccessful. . ## Cromwell Version. I am using cromwell 41.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006
https://github.com/broadinstitute/cromwell/pull/5009:27,Testability,test,testing,27,Clone of #4977 to allow CI testing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5009
https://github.com/broadinstitute/cromwell/pull/5012:295,Testability,test,tests,295,"Backing scripts for the suite of `Cromwell Perf: xyz` jobs in [jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/). Specifically, they allow us to:; * Create an ad-hoc database clone; * Create an ad-hoc instance; * Delete an instance; * Delete a database. But, ""what about the API perf tests?"", I hear you ask... those are defined over in a parallel [McNulty](https://github.com/broadinstitute/mcnulty/pull/55) PR, and are run using the same Jenkins server's [Limit Finder](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-limit-finder/) job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5012
https://github.com/broadinstitute/cromwell/pull/5013:122,Deployability,Pipeline,PipelinesApiFactoryInterface,122,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013
https://github.com/broadinstitute/cromwell/pull/5013:53,Energy Efficiency,Adapt,Adapter,53,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013
https://github.com/broadinstitute/cromwell/pull/5013:108,Energy Efficiency,adapt,adapting,108,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013
https://github.com/broadinstitute/cromwell/pull/5013:53,Integrability,Adapter,Adapter,53,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013
https://github.com/broadinstitute/cromwell/pull/5013:53,Modifiability,Adapt,Adapter,53,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013
https://github.com/broadinstitute/cromwell/pull/5013:108,Modifiability,adapt,adapting,108,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013
https://github.com/broadinstitute/cromwell/pull/5015:15,Deployability,configurat,configuration,15,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015
https://github.com/broadinstitute/cromwell/pull/5015:378,Deployability,configurat,configuration,378,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015
https://github.com/broadinstitute/cromwell/pull/5015:15,Modifiability,config,configuration,15,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015
https://github.com/broadinstitute/cromwell/pull/5015:378,Modifiability,config,configuration,378,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015
https://github.com/broadinstitute/cromwell/pull/5015:124,Security,access,accessible,124,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015
https://github.com/broadinstitute/cromwell/pull/5016:28,Modifiability,config,configured,28,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:54,Testability,log,log-temporary,54,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:167,Testability,log,log,167,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:374,Testability,log,logs,374,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:825,Testability,log,log,825,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:870,Testability,log,log,870,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:918,Testability,log,logs,918,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5016:952,Testability,test,tested,952,"Per #4826, when Cromwell is configured with `workflow-log-temporary: false` and workflow does not specify `final_workflow_log_dir`, Cromwell server does not close the log file resulting in a file handle leak. The lack of a file close also prevents certain FUSE drivers from flushing any file contents to remote storage, making it impossible for clients to actually read the logs until the server is terminated (which may be never). . Looking at the code, there is no call to `workflowLogger.close()` or `workflowLogger.close(andDelete = false)` unless the file is being copied. This line in WorkflowActor.scala:; `case None if WorkflowLogger.isTemporary => workflowLogger.close(andDelete = true)`; should change to:; `case None => workflowLogger.close(andDelete = WorkflowLogger.isTemporary)`. At this point in the code, the log contents have been fully written and the log should be closed, regardless of whether the logs are temporary or not. . I've tested that this fix resolves the issue according to repro instructions in #4826.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5016
https://github.com/broadinstitute/cromwell/pull/5018:125,Testability,test,tests,125,"- Switched from running Travis' mysql/postgresql instances to dockerized versions; - Added a couple of centaur and some unit tests for mariadb, using mysql liquibase/drivers for now; - NOTE: No postgresql tests enabled in this commit; - Changed mysql CI login from travis: to cromwell:test; - DRYed out more of the CI scripts into test.inc.sh; - Switched CI broadinstitute/dsde-toolbox from :latest to :dev; - Don't fail with false negatives when MetadataDatabaseAccessSpec is re-run against persistent RDBMS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5018
https://github.com/broadinstitute/cromwell/pull/5018:205,Testability,test,tests,205,"- Switched from running Travis' mysql/postgresql instances to dockerized versions; - Added a couple of centaur and some unit tests for mariadb, using mysql liquibase/drivers for now; - NOTE: No postgresql tests enabled in this commit; - Changed mysql CI login from travis: to cromwell:test; - DRYed out more of the CI scripts into test.inc.sh; - Switched CI broadinstitute/dsde-toolbox from :latest to :dev; - Don't fail with false negatives when MetadataDatabaseAccessSpec is re-run against persistent RDBMS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5018
https://github.com/broadinstitute/cromwell/pull/5018:254,Testability,log,login,254,"- Switched from running Travis' mysql/postgresql instances to dockerized versions; - Added a couple of centaur and some unit tests for mariadb, using mysql liquibase/drivers for now; - NOTE: No postgresql tests enabled in this commit; - Changed mysql CI login from travis: to cromwell:test; - DRYed out more of the CI scripts into test.inc.sh; - Switched CI broadinstitute/dsde-toolbox from :latest to :dev; - Don't fail with false negatives when MetadataDatabaseAccessSpec is re-run against persistent RDBMS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5018
https://github.com/broadinstitute/cromwell/pull/5018:285,Testability,test,test,285,"- Switched from running Travis' mysql/postgresql instances to dockerized versions; - Added a couple of centaur and some unit tests for mariadb, using mysql liquibase/drivers for now; - NOTE: No postgresql tests enabled in this commit; - Changed mysql CI login from travis: to cromwell:test; - DRYed out more of the CI scripts into test.inc.sh; - Switched CI broadinstitute/dsde-toolbox from :latest to :dev; - Don't fail with false negatives when MetadataDatabaseAccessSpec is re-run against persistent RDBMS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5018
https://github.com/broadinstitute/cromwell/pull/5018:331,Testability,test,test,331,"- Switched from running Travis' mysql/postgresql instances to dockerized versions; - Added a couple of centaur and some unit tests for mariadb, using mysql liquibase/drivers for now; - NOTE: No postgresql tests enabled in this commit; - Changed mysql CI login from travis: to cromwell:test; - DRYed out more of the CI scripts into test.inc.sh; - Switched CI broadinstitute/dsde-toolbox from :latest to :dev; - Don't fail with false negatives when MetadataDatabaseAccessSpec is re-run against persistent RDBMS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5018
https://github.com/broadinstitute/cromwell/pull/5020:0,Deployability,Update,Updated,0,Updated all-purpose-mess-remover.dot and all-purpose-mess-remover.dot.png with submission_print_out.sh.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5020
https://github.com/broadinstitute/cromwell/pull/5025:44,Modifiability,config,configs,44,For those of us who like to use the centaur configs when running Cromwell locally,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5025
https://github.com/broadinstitute/cromwell/pull/5028:875,Availability,failure,failure,875,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:110,Energy Efficiency,Monitor,MonitoringAction,110,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:364,Energy Efficiency,monitor,monitor-bigquery,364,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:534,Energy Efficiency,monitor,monitoring,534,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:649,Energy Efficiency,monitor,monitoring,649,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:970,Energy Efficiency,monitor,monitor-bbq,970,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:95,Modifiability,variab,variables,95,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:153,Modifiability,variab,variables,153,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:274,Modifiability,config,config,274,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:765,Safety,predict,prediction,765,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:860,Safety,detect,detect,860,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/pull/5028:1468,Usability,feedback,feedback,1468,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028
https://github.com/broadinstitute/cromwell/issues/5029:775,Integrability,interface,interface,775,"Hi everyone,. I got an idea (read ""stole it from another tool"") that WDL files can easily be made executable using shebang lines. . Instead of calling:; ```; java -jar ${CROMWELL_JAR} run example/hello.wdl -i example/hello.input.json; ```. By using a `#!/usr/bin/env cromwexe` shebang line in our WDL file, we can call simply:; ```; example/hello.wdl -i example/hello.input.json; ```. See my working prototype:; https://github.com/prihoda/cromwexe. The only thing `cromwexe` is doing at the moment is passing all args to the regular java command:; ```python; cmd = [""java"", ""-jar"", cromwell_jar, ""run""] + sys.argv[1:]; ```; It starts to get interesting once you realize that if we can parse the workflow script inside `cromwexe`, we can automatically generate a command-line interface that parses the args directly from the command line (and even provide a help screen with descriptions of all input args):; ```; example/hello.wdl -i example/hello.defaults.json --sayHello.name World; ```. We can also parse the job metadata output of cromwell to determine the location of our output files and move them over from the execution folder to the user's working folder, if the user provides the output arg as well:; ```; example/hello.wdl -i example/hello.defaults.json --sayHello.name World --output.greeting greeting.txt; ```; Edit: I see that this can be done by the `use_relative_output_paths` or `final_workflow_outputs_dir` args. Then I guess the added functionality is that output args could be used to define each output file path separately. I have two questions :); 1) What do you think? I am excited to implement the full solution, it should not take more than a few hours.; 2) Can I use this parser? https://github.com/TMiguelT/WdlParserPackaging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029
https://github.com/broadinstitute/cromwell/issues/5029:319,Usability,simpl,simply,319,"Hi everyone,. I got an idea (read ""stole it from another tool"") that WDL files can easily be made executable using shebang lines. . Instead of calling:; ```; java -jar ${CROMWELL_JAR} run example/hello.wdl -i example/hello.input.json; ```. By using a `#!/usr/bin/env cromwexe` shebang line in our WDL file, we can call simply:; ```; example/hello.wdl -i example/hello.input.json; ```. See my working prototype:; https://github.com/prihoda/cromwexe. The only thing `cromwexe` is doing at the moment is passing all args to the regular java command:; ```python; cmd = [""java"", ""-jar"", cromwell_jar, ""run""] + sys.argv[1:]; ```; It starts to get interesting once you realize that if we can parse the workflow script inside `cromwexe`, we can automatically generate a command-line interface that parses the args directly from the command line (and even provide a help screen with descriptions of all input args):; ```; example/hello.wdl -i example/hello.defaults.json --sayHello.name World; ```. We can also parse the job metadata output of cromwell to determine the location of our output files and move them over from the execution folder to the user's working folder, if the user provides the output arg as well:; ```; example/hello.wdl -i example/hello.defaults.json --sayHello.name World --output.greeting greeting.txt; ```; Edit: I see that this can be done by the `use_relative_output_paths` or `final_workflow_outputs_dir` args. Then I guess the added functionality is that output args could be used to define each output file path separately. I have two questions :); 1) What do you think? I am excited to implement the full solution, it should not take more than a few hours.; 2) Can I use this parser? https://github.com/TMiguelT/WdlParserPackaging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029
https://github.com/broadinstitute/cromwell/issues/5031:264,Testability,log,logged,264,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. The jira is not visible to me and I'm logged in and on the Broad network. . <img width=""1439"" alt=""Screen Shot 2019-06-13 at 4 18 45 PM"" src=""https://user-images.githubusercontent.com/4700332/59464633-fdbcf080-8df6-11e9-88dc-0e4450a3d3ce.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031
https://github.com/broadinstitute/cromwell/pull/5032:8,Usability,feedback,feedback,8,"~~Early feedback requested on:~~. * ~~Would these metrics be useful?~~; * ~~Is there overhead of getting the metadata count?~~; * ~~... and if so, is it worth it?~~; * ~~... and if not, are the other metrics still worth capturing?~~. Ready for review!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5032
https://github.com/broadinstitute/cromwell/pull/5034:299,Availability,down,down,299,"On Cromwells which are not our own production instance, people seem to be more frustrated than delighted by having the initial read limits be arbitrarily restrictive. - [x] Will probably need to update `firecloud-develop` to set some values which are still relying on the previous low defaults back down to 128k",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034
https://github.com/broadinstitute/cromwell/pull/5034:195,Deployability,update,update,195,"On Cromwells which are not our own production instance, people seem to be more frustrated than delighted by having the initial read limits be arbitrarily restrictive. - [x] Will probably need to update `firecloud-develop` to set some values which are still relying on the previous low defaults back down to 128k",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034
https://github.com/broadinstitute/cromwell/pull/5035:61,Availability,failure,failures,61,Also cherry pick a couple of Khalid's CI fixes for unrelated failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5035
https://github.com/broadinstitute/cromwell/pull/5036:84,Deployability,configurat,configuration,84,This PR enables sending metrics to Stackdriver in Cromwell Perf (and removes StatsD configuration).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5036
https://github.com/broadinstitute/cromwell/pull/5036:84,Modifiability,config,configuration,84,This PR enables sending metrics to Stackdriver in Cromwell Perf (and removes StatsD configuration).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5036
https://github.com/broadinstitute/cromwell/pull/5040:105,Safety,avoid,avoiding,105,"Omits the first of the two CCHE migrations which appears to be made unnecessary by the second, hopefully avoiding Götterdämmerung.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5040
https://github.com/broadinstitute/cromwell/issues/5041:24,Availability,error,errors,24,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041
https://github.com/broadinstitute/cromwell/issues/5041:188,Availability,error,error,188,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041
https://github.com/broadinstitute/cromwell/issues/5041:165,Deployability,pipeline,pipelines,165,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041
https://github.com/broadinstitute/cromwell/issues/5041:595,Deployability,pipeline,pipeline,595,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041
https://github.com/broadinstitute/cromwell/pull/5043:114,Deployability,patch,patch,114,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:845,Integrability,depend,dependent,845,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:1409,Modifiability,config,configurable,1409,"ategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Copies the reference 9 times. Each file is hard-linked 1000 times from the cache before the cache-link is removed. Using 9*3,8 GB = Approx. 36 GB of disk space.; soft-link | does not work with containers and ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:52,Performance,cache,cached-copy,52,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:398,Performance,cache,cached-copy,398,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:653,Performance,cache,cache,653,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:953,Performance,cache,cache,953,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:1116,Performance,cache,cache,1116,"een using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:1181,Performance,cache,cache,1181,"em also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:1726,Performance,cache,cached-copy,1726,"ell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Copies the reference 9 times. Each file is hard-linked 1000 times from the cache before the cache-link is removed. Using 9*3,8 GB = Approx. 36 GB of disk space.; soft-link | does not work with containers and is excluded.; copy | Not used.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:1913,Performance,cache,cached-copy,1913,"ell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Copies the reference 9 times. Each file is hard-linked 1000 times from the cache before the cache-link is removed. Using 9*3,8 GB = Approx. 36 GB of disk space.; soft-link | does not work with containers and is excluded.; copy | Not used.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:2192,Performance,cache,cached-copy,2192,"ell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Copies the reference 9 times. Each file is hard-linked 1000 times from the cache before the cache-link is removed. Using 9*3,8 GB = Approx. 36 GB of disk space.; soft-link | does not work with containers and is excluded.; copy | Not used.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:2281,Performance,cache,cache,2281,"ell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Copies the reference 9 times. Each file is hard-linked 1000 times from the cache before the cache-link is removed. Using 9*3,8 GB = Approx. 36 GB of disk space.; soft-link | does not work with containers and is excluded.; copy | Not used.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5043:2298,Performance,cache,cache-link,2298,"ell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 = Approx. 3 TB of disk space. Afer this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Copies the reference 9 times. Each file is hard-linked 1000 times from the cache before the cache-link is removed. Using 9*3,8 GB = Approx. 36 GB of disk space.; soft-link | does not work with containers and is excluded.; copy | Not used.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043
https://github.com/broadinstitute/cromwell/pull/5049:71,Testability,test,test,71,Separate PR for broken gcloud rsync that flustered the Centaur `space` test https://github.com/GoogleCloudPlatform/gsutil/issues/806,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5049
https://github.com/broadinstitute/cromwell/issues/5051:417,Integrability,message,message,417,"Jira:; https://broadworkbench.atlassian.net/browse/BA-5756?atlOrigin=eyJpIjoiYjk0YjlhYzYyN2Y2NGRkY2FiMGIwNWFmZDk5M2ZiMWEiLCJwIjoiaiJ9. ```; version: cromwell41; backend: SGE; hard disk：Network shared storage （lustre）; ```. When the rc file has been generated, the task status is still running. This state will last a long time.; When we set ""exit-code-timeout-seconds = 18000"", such tasks will fail after 10 hours `(""message"": ""The job was aborted from outside Cromwell"") `. Despite the fact that the task has been completed normally. This situation does not always happen, and only a few tasks will encounter this problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5051
https://github.com/broadinstitute/cromwell/issues/5051:352,Safety,timeout,timeout-seconds,352,"Jira:; https://broadworkbench.atlassian.net/browse/BA-5756?atlOrigin=eyJpIjoiYjk0YjlhYzYyN2Y2NGRkY2FiMGIwNWFmZDk5M2ZiMWEiLCJwIjoiaiJ9. ```; version: cromwell41; backend: SGE; hard disk：Network shared storage （lustre）; ```. When the rc file has been generated, the task status is still running. This state will last a long time.; When we set ""exit-code-timeout-seconds = 18000"", such tasks will fail after 10 hours `(""message"": ""The job was aborted from outside Cromwell"") `. Despite the fact that the task has been completed normally. This situation does not always happen, and only a few tasks will encounter this problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5051
https://github.com/broadinstitute/cromwell/issues/5051:440,Safety,abort,aborted,440,"Jira:; https://broadworkbench.atlassian.net/browse/BA-5756?atlOrigin=eyJpIjoiYjk0YjlhYzYyN2Y2NGRkY2FiMGIwNWFmZDk5M2ZiMWEiLCJwIjoiaiJ9. ```; version: cromwell41; backend: SGE; hard disk：Network shared storage （lustre）; ```. When the rc file has been generated, the task status is still running. This state will last a long time.; When we set ""exit-code-timeout-seconds = 18000"", such tasks will fail after 10 hours `(""message"": ""The job was aborted from outside Cromwell"") `. Despite the fact that the task has been completed normally. This situation does not always happen, and only a few tasks will encounter this problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5051
https://github.com/broadinstitute/cromwell/issues/5052:549,Deployability,pipeline,pipelines,549,"Hi-. The HuBMAP consortium has been implementing some workflows in CWL and running these via `cwltool` -- we're quite interested in storing the provenance information for a workflow run in Research Object format. This would include the inputs and outputs for a certain run, in addition to (a normalized version of) the workflow itself. This is already implemented in `cwltool` and accessible through its `--provenance` flag; is anything like this planned for Cromwell?. Some of the HuBMAP tissue mapping centers are interested in or have been using pipelines written in WDL (e.g. [ENCODE's ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline), and we would like to support these without giving up the ability to store workflow run provenance in a standard format. Is anything like this planned for Cromwell? I didn't see anything in the issue/forum/PR searches I've been doing. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5052
https://github.com/broadinstitute/cromwell/issues/5052:599,Deployability,pipeline,pipeline,599,"Hi-. The HuBMAP consortium has been implementing some workflows in CWL and running these via `cwltool` -- we're quite interested in storing the provenance information for a workflow run in Research Object format. This would include the inputs and outputs for a certain run, in addition to (a normalized version of) the workflow itself. This is already implemented in `cwltool` and accessible through its `--provenance` flag; is anything like this planned for Cromwell?. Some of the HuBMAP tissue mapping centers are interested in or have been using pipelines written in WDL (e.g. [ENCODE's ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline), and we would like to support these without giving up the ability to store workflow run provenance in a standard format. Is anything like this planned for Cromwell? I didn't see anything in the issue/forum/PR searches I've been doing. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5052
https://github.com/broadinstitute/cromwell/issues/5052:648,Deployability,pipeline,pipeline,648,"Hi-. The HuBMAP consortium has been implementing some workflows in CWL and running these via `cwltool` -- we're quite interested in storing the provenance information for a workflow run in Research Object format. This would include the inputs and outputs for a certain run, in addition to (a normalized version of) the workflow itself. This is already implemented in `cwltool` and accessible through its `--provenance` flag; is anything like this planned for Cromwell?. Some of the HuBMAP tissue mapping centers are interested in or have been using pipelines written in WDL (e.g. [ENCODE's ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline), and we would like to support these without giving up the ability to store workflow run provenance in a standard format. Is anything like this planned for Cromwell? I didn't see anything in the issue/forum/PR searches I've been doing. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5052
https://github.com/broadinstitute/cromwell/issues/5052:381,Security,access,accessible,381,"Hi-. The HuBMAP consortium has been implementing some workflows in CWL and running these via `cwltool` -- we're quite interested in storing the provenance information for a workflow run in Research Object format. This would include the inputs and outputs for a certain run, in addition to (a normalized version of) the workflow itself. This is already implemented in `cwltool` and accessible through its `--provenance` flag; is anything like this planned for Cromwell?. Some of the HuBMAP tissue mapping centers are interested in or have been using pipelines written in WDL (e.g. [ENCODE's ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline), and we would like to support these without giving up the ability to store workflow run provenance in a standard format. Is anything like this planned for Cromwell? I didn't see anything in the issue/forum/PR searches I've been doing. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5052
https://github.com/broadinstitute/cromwell/pull/5053:121,Availability,avail,available,121,"Filling in the missing meta and parameter_meta types. This makes Cromwell/WOM support the spec with JSON like structures available for the meta sections. . ```; $parameter_meta = 'parameter_meta' $ws* '{' ($ws* $parameter_meta_kv $ws*)* '}'; $parameter_meta_kv = $identifier $ws* ':' $ws* $meta_value; $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array; $meta_object = '{}' | '{' $parameter_meta_kv (, $parameter_meta_kv)* '}'; $meta_array = '[]' | '[' $meta_value (, $meta_value)* ']'; ```. There is a part I didn't know how to properly code, which is `WorkflowDescription` support ( services/src/main/scala/cromwell/services/womtool/models/MetaValueElementJsonSupport.scala). I could use help there. . Thank you, ; Ohad.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053
https://github.com/broadinstitute/cromwell/issues/5055:356,Deployability,Configurat,Configuration,356,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:393,Deployability,configurat,configuration,393,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:487,Deployability,configurat,configuration,487,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:7,Modifiability,config,configured,7,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:159,Modifiability,config,configured,159,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:356,Modifiability,Config,Configuration,356,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:393,Modifiability,config,configuration,393,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:487,Modifiability,config,configuration,487,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:748,Modifiability,config,configured,748,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:804,Modifiability,config,configured,804,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:898,Modifiability,config,config,898,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5055:594,Usability,simpl,simple,594,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055
https://github.com/broadinstitute/cromwell/issues/5056:1063,Deployability,configurat,configuration,1063,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056
https://github.com/broadinstitute/cromwell/issues/5056:1063,Modifiability,config,configuration,1063,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056
https://github.com/broadinstitute/cromwell/issues/5056:1108,Security,PASSWORD,PASSWORDS,1108,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056
https://github.com/broadinstitute/cromwell/issues/5056:273,Usability,feedback,feedback,273,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056
https://github.com/broadinstitute/cromwell/pull/5057:385,Deployability,integrat,integration,385,"The purpose of this PR is to fix issue https://github.com/broadinstitute/cromwell/issues/4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location, while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory.; Therefore, we have changed the place where Cromwell searches that files. We also added an integration test that reproduces the problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5057
https://github.com/broadinstitute/cromwell/pull/5057:385,Integrability,integrat,integration,385,"The purpose of this PR is to fix issue https://github.com/broadinstitute/cromwell/issues/4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location, while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory.; Therefore, we have changed the place where Cromwell searches that files. We also added an integration test that reproduces the problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5057
https://github.com/broadinstitute/cromwell/pull/5057:397,Testability,test,test,397,"The purpose of this PR is to fix issue https://github.com/broadinstitute/cromwell/issues/4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location, while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory.; Therefore, we have changed the place where Cromwell searches that files. We also added an integration test that reproduces the problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5057
https://github.com/broadinstitute/cromwell/pull/5058:37,Integrability,synchroniz,synchronize,37,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/pull/5058:24,Testability,test,tests,24,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/pull/5058:152,Testability,Test,Test,152,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/pull/5058:210,Testability,Test,Test,210,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/pull/5058:278,Testability,Test,Test,278,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/pull/5058:342,Testability,test,tests,342,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/pull/5058:383,Testability,test,tests,383,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058
https://github.com/broadinstitute/cromwell/issues/5059:107,Availability,echo,echo,107,"Hello， Could someone tell me how to fix this problem? Thanks!!!. **task hello** {; String name. command {; echo 'Hello ${name}!' > /Users/00.wdl/QC_new/QC/hello.out1; }; output {; File out1 = ""/Users/00.wdl/QC_new/QC/hello.out1""; }; }. **task hello2** {; 	File in1. 	command {; 		cat ${in1}; 	}. 	output {; 		File	response2 = stdout(); 	}. }. **workflow test** {; call hello {input name=""World"" }; call hello2 {input in1=hello.out1 }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5059
https://github.com/broadinstitute/cromwell/issues/5059:354,Testability,test,test,354,"Hello， Could someone tell me how to fix this problem? Thanks!!!. **task hello** {; String name. command {; echo 'Hello ${name}!' > /Users/00.wdl/QC_new/QC/hello.out1; }; output {; File out1 = ""/Users/00.wdl/QC_new/QC/hello.out1""; }; }. **task hello2** {; 	File in1. 	command {; 		cat ${in1}; 	}. 	output {; 		File	response2 = stdout(); 	}. }. **workflow test** {; call hello {input name=""World"" }; call hello2 {input in1=hello.out1 }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5059
https://github.com/broadinstitute/cromwell/pull/5061:6,Deployability,configurat,configuration,6,"Add a configuration option to deal with nested scatters in WDL. Currently, the inner scatter is converted into a subworkflow. This behavior is now controlled by the `inner-outer-scatter` option defined in `wom/src/main/resources/reference.conf` configuration file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061
https://github.com/broadinstitute/cromwell/pull/5061:245,Deployability,configurat,configuration,245,"Add a configuration option to deal with nested scatters in WDL. Currently, the inner scatter is converted into a subworkflow. This behavior is now controlled by the `inner-outer-scatter` option defined in `wom/src/main/resources/reference.conf` configuration file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061
https://github.com/broadinstitute/cromwell/pull/5061:6,Modifiability,config,configuration,6,"Add a configuration option to deal with nested scatters in WDL. Currently, the inner scatter is converted into a subworkflow. This behavior is now controlled by the `inner-outer-scatter` option defined in `wom/src/main/resources/reference.conf` configuration file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061
https://github.com/broadinstitute/cromwell/pull/5061:245,Modifiability,config,configuration,245,"Add a configuration option to deal with nested scatters in WDL. Currently, the inner scatter is converted into a subworkflow. This behavior is now controlled by the `inner-outer-scatter` option defined in `wom/src/main/resources/reference.conf` configuration file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061
https://github.com/broadinstitute/cromwell/issues/5063:2781,Availability,down,down,2781,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:2790,Availability,mainten,maintenance,2790,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:1068,Deployability,configurat,configuration,1068,"thinking about a more elegant way to solve the awkwardness of running a scatter while using Singularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; modul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:1149,Deployability,install,installed,1149,"ngularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:2019,Deployability,install,installed,2019,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:1658,Integrability,wrap,wrap,1658,"m but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:2641,Integrability,wrap,wrap,2641,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:1068,Modifiability,config,configuration,1068,"thinking about a more elegant way to solve the awkwardness of running a scatter while using Singularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; modul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:1134,Performance,load,loaded,1134,"ngularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:1179,Performance,load,load,1179,"ngularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:2004,Performance,load,loaded,2004,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:2049,Performance,load,load,2049,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/issues/5063:2744,Testability,test,tested,2744,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063
https://github.com/broadinstitute/cromwell/pull/5064:494,Testability,log,logic,494,"The purpose of this PR is to fix issue #4586.; There's already a [PR](https://github.com/broadinstitute/cromwell/pull/5057) that fixes this issue. In the existing [PR](https://github.com/broadinstitute/cromwell/pull/5057) the issue was solved by changing the location of the file search. ; However, we are not sure if this is the best solution, as we are still waiting for your opinion. In case the existing [PR](https://github.com/broadinstitute/cromwell/pull/5057) is not consistent with the logic of the Cromwell, we have added another possible solution. In this PR the files themselves are moved to the place where the Cromwell expects them to be.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5064
https://github.com/broadinstitute/cromwell/pull/5065:70,Availability,error,error,70,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:20,Deployability,configurat,configuration,20,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:132,Deployability,configurat,configuration,132,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:20,Modifiability,config,configuration,20,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:132,Modifiability,config,configuration,132,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:57,Testability,log,log,57,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:66,Testability,log,log,66,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:83,Testability,log,log,83,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/pull/5065:162,Testability,log,logback,162,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065
https://github.com/broadinstitute/cromwell/issues/5066:616,Availability,down,downloaded,616,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:4200,Availability,Error,Error,4200,"ut_r1} ${input_r2}; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }; ```. ### Input for the workflow is this:; ```; #input WDL. {; ""scMeth.sampleName"": ""sub"",; ""scMeth.input_fastq1"": ""sub_1.fastq.gz"",; ""scMeth.input_fastq2"": ""sub_2.fastq.gz"",; ""scMeth.file_format"": ""fastq"",; ""scMeth.command"": ""moveBarcodeToID.pl"",; ""scMeth.low_quality_cutoff"": 21,; ""scMeth.read_length_cutoff"": 62,; ""scMeth.TAG"": ""'length='"",; ""scMeth.bases"": 6,; ""scMeth.trim_start_R1"": 11,; ""scMeth.trim_end_R1"": -16,; ""scMeth.trim_start_R2"": 25,; ""scMeth.trim_end_R2"": -2,; ""scMeth.trimAdapters.sampleName"": ""sub"",; ""scMeth.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:4825,Availability,heartbeat,heartbeat,4825,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:4889,Availability,heartbeat,heartbeatInterval,4889,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:4948,Availability,failure,failureShutdownDuration,4948,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:9454,Availability,error,error,9454,"cMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/execution/script; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: job id: 39755; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:9536,Availability,Error,Error,9536,"ode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:12689,Availability,robust,robustExecuteOrRecover,12689,obExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:13032,Availability,robust,robustExecuteOrRecover,13032,ckend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:14402,Availability,Error,Error,14402,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:539,Deployability,install,installed,539,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:3983,Deployability,configurat,configuration,3983,"th_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=$TAG -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }; ```. ### Input for the workflow is this:; ```; #input WDL. {; ""scMeth.sampleName"": ""sub"",; ""scMeth.input_fastq1"": ""sub_1.fastq.gz"",; ""scMeth.input_fastq2"": ""sub_2.fastq.gz"",; ""scMeth.file_format"": ""fastq"",; ""scMeth.command"": ""moveBarcodeToID.pl"",; ""scMeth.low_quality_cutoff"": 21,; ""scMeth.read_length_cutoff"": 62,; ""scMeth.TAG"": ""'length='"",; ""scMeth.bases"": 6,; ""scMeth.trim_start_R1"": 11,; ""scMeth.trim_end_R1"": -16,; ""scMeth.trim_start_R2"": 25,; ""scMeth.trim_end_R2"": -2,; ""scMeth.trimAdapters.sampleName"": ""sub"",; ""scMeth.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:4835,Deployability,configurat,configuration,4835,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:1490,Energy Efficiency,adapt,adapters,1490,"0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. task trimCellBarcode {; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName; command {; perl ${command} paired ${input_fastq1} ${input_fastq2} ${bases} ${sampleName}.R1.debarcoded.fq.gz ${sampleName}.R2.debarcoded.fq.gz; }; output {; File fastq_debarcoded_R1 = ""${sampleName}.R1.debarcoded.fq.gz""; File fastq_debarcoded_R2 = ""${sampleName}.R2.debarcoded.fq.gz""; }; }. task trimAdapters {; File file_format",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:1490,Integrability,adapter,adapters,1490,"0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. task trimCellBarcode {; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName; command {; perl ${command} paired ${input_fastq1} ${input_fastq2} ${bases} ${sampleName}.R1.debarcoded.fq.gz ${sampleName}.R2.debarcoded.fq.gz; }; output {; File fastq_debarcoded_R1 = ""${sampleName}.R1.debarcoded.fq.gz""; File fastq_debarcoded_R2 = ""${sampleName}.R2.debarcoded.fq.gz""; }; }. task trimAdapters {; File file_format",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:1490,Modifiability,adapt,adapters,1490,"0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. task trimCellBarcode {; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName; command {; perl ${command} paired ${input_fastq1} ${input_fastq2} ${bases} ${sampleName}.R1.debarcoded.fq.gz ${sampleName}.R2.debarcoded.fq.gz; }; output {; File fastq_debarcoded_R1 = ""${sampleName}.R1.debarcoded.fq.gz""; File fastq_debarcoded_R2 = ""${sampleName}.R2.debarcoded.fq.gz""; }; }. task trimAdapters {; File file_format",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:3983,Modifiability,config,configuration,3983,"th_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=$TAG -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }; ```. ### Input for the workflow is this:; ```; #input WDL. {; ""scMeth.sampleName"": ""sub"",; ""scMeth.input_fastq1"": ""sub_1.fastq.gz"",; ""scMeth.input_fastq2"": ""sub_2.fastq.gz"",; ""scMeth.file_format"": ""fastq"",; ""scMeth.command"": ""moveBarcodeToID.pl"",; ""scMeth.low_quality_cutoff"": 21,; ""scMeth.read_length_cutoff"": 62,; ""scMeth.TAG"": ""'length='"",; ""scMeth.bases"": 6,; ""scMeth.trim_start_R1"": 11,; ""scMeth.trim_end_R1"": -16,; ""scMeth.trim_start_R2"": 25,; ""scMeth.trim_end_R2"": -2,; ""scMeth.trimAdapters.sampleName"": ""sub"",; ""scMeth.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:4835,Modifiability,config,configuration,4835,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:5348,Modifiability,config,configured,5348,"b.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Starting workflow 41d3eecf-c5a9-42e4-8a29",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:5462,Modifiability,config,configured,5462,"00000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Starting workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Successfully started WorkflowActor-41d3eecf-c5a9-42e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:5582,Modifiability,config,configured,5582,"-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Starting workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Successfully started WorkflowActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,52] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-07-10 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:6641,Modifiability,config,configured,6641,"se 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Starting workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,51] [info] WorkflowManagerActor Successfully started WorkflowActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,52] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-07-10 14:32:54,53] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-07-10 14:32:54,60] [info] MaterializeWorkflowDescriptorActor [41d3eecf]: Parsing workflow as WDL draft-2; [2019-07-10 14:32:55,28] [info] MaterializeWorkflowDescriptorActor [41d3eecf]: Call-to-Backend assignments: scMeth.trimAdapters -> Local, scMeth.trimCellBarcode -> Local; [2019-07-10 14:32:57,55] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimCellBarcode; [2019-07-10 14:32:58,20] [info] Assigned new job execution tokens to the following groups: 41d3ee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:9883,Modifiability,config,config,9883,"ns to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$B",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:9960,Modifiability,Config,ConfigAsyncJobExecutionActor,9960,"Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:10031,Modifiability,config,config,10031,"ipts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:10097,Modifiability,Config,ConfigAsyncJobExecutionActor,10097,"Meth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:10409,Modifiability,config,config,10409,"mple_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:10477,Modifiability,Config,ConfigAsyncJobExecutionActor,10477,ncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:10819,Modifiability,config,config,10819,syncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:10946,Modifiability,Config,ConfigAsyncJobExecutionActor,10946,ionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:11262,Modifiability,config,config,11262,mwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:11328,Modifiability,Config,ConfigAsyncJobExecutionActor,11328,tents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:11646,Modifiability,config,config,11646,5); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:11700,Modifiability,Config,ConfigAsyncJobExecutionActor,11700,nActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:12158,Modifiability,config,config,12158,Actor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:12217,Modifiability,Config,ConfigAsyncJobExecutionActor,12217,a:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:12519,Modifiability,config,config,12519,dFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at ak,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:12582,Modifiability,Config,ConfigAsyncJobExecutionActor,12582,bExecutionActor.scala:155); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Act,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:13654,Modifiability,config,config,13654,ckendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:13714,Modifiability,Config,ConfigAsyncJobExecutionActor,13714,ckendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:5256,Performance,throttle,throttle,5256,".url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:8644,Performance,queue,queue,8644,"ripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/inputs/13016223/sub_1.fastq.gz /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/inputs/13016223/sub_2.fastq.gz 6 sub.R1.debarcoded.fq.gz sub.R2.debarcoded.fq.gz; [2019-07-10 14:32:58,39] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: executing: /bin/bash /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/execution/script; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: job id: 39755; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:487,Security,access,accessible,487,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:1191,Security,validat,validated,1191,"rks fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_st",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:1230,Security,validat,validate,1230,"rks fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_st",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:1263,Security,validat,validate,1263," first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:5162,Security,hash,hash-lookup,5162,"t_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:5240,Security,hash,hash-lookup,5240,"] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54,48] [info] Unspecified type (Unspecified version) workflow 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 submitted; [2019-07-10 14:32:54,50] [info] SingleWorkflowRunnerActor: Workflow submitted 41d3eecf-c5a9-42e4-8a29-8be9c252b7f5; [2019-07-10 14:32:54,50] [info] 1 new workflows fetched by cromid-1cf43fa: 41d3eecf-c5a9-42e4-8a29-8b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15083,Security,validat,validation,15083,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15094,Security,Validat,Validation,15094,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15105,Security,Validat,ValidationTry,15105,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15137,Security,Validat,Validation,15137,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15170,Security,validat,validation,15170,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15181,Security,Validat,Validation,15181,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15192,Security,Validat,ValidationTry,15192,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:15224,Security,Validat,Validation,15224,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:8631,Testability,log,log,8631,"ripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/inputs/13016223/sub_1.fastq.gz /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/inputs/13016223/sub_2.fastq.gz 6 sub.R1.debarcoded.fq.gz sub.R2.debarcoded.fq.gz; [2019-07-10 14:32:58,39] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: executing: /bin/bash /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/execution/script; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: job id: 39755; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5066:8668,Testability,log,log,8668,"3016223/sub_1.fastq.gz /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/inputs/13016223/sub_2.fastq.gz 6 sub.R1.debarcoded.fq.gz sub.R2.debarcoded.fq.gz; [2019-07-10 14:32:58,39] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: executing: /bin/bash /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/execution/script; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: job id: 39755; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066
https://github.com/broadinstitute/cromwell/issues/5069:310,Availability,error,error,310,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:558,Availability,error,error,558,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:904,Availability,error,error,904,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:1424,Availability,error,error,1424,"hich a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:2470,Availability,Down,Downloading,2470," the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcompone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:2582,Availability,Down,Downloading,2582,"/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostExcepti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:2851,Availability,Down,Downloaded,2851,"nthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:2957,Availability,Down,Downloaded,2957,s in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3322,Availability,down,download,3322,nthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; Comman,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3331,Availability,error,error,3331,nthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; Comman,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3416,Availability,down,downloading,3416,def.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing outpu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3542,Availability,down,download,3542,s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-46,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3551,Availability,error,error,3551,s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-46,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3638,Availability,down,downloading,3638,sl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3906,Availability,down,download,3906,12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retryi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3915,Availability,error,error,3915,12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retryi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:4000,Availability,down,downloading,4000,repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs:/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:4101,Availability,down,download,4101,rg/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:4110,Availability,error,error,4110,rg/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:4197,Availability,down,downloading,4197,components-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:316,Integrability,message,messages,316,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:564,Integrability,message,messages,564,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:2003,Integrability,depend,dependency,2003,"operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloade",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:3158,Integrability,depend,dependencies,3158,ion.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: o,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:4621,Modifiability,config,config,4621, download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:38 Waiting 5 seconds and retrying; 2019/07/10 18:38:44 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:4955,Modifiability,config,config,4955, download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:38 Waiting 5 seconds and retrying; 2019/07/10 18:38:44 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:5289,Modifiability,config,config,5289, download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:38 Waiting 5 seconds and retrying; 2019/07/10 18:38:44 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:205,Performance,perform,performs,205,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:788,Security,access,access,788,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:329,Testability,log,logs,329,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:418,Testability,Test,Test,418,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:577,Testability,log,log,577,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/issues/5069:1952,Testability,log,logged,1952,"operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloade",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069
https://github.com/broadinstitute/cromwell/pull/5070:5,Modifiability,enhance,enhancement,5,"This enhancement allows EFS or any parallel file system that can be mounted to the computes, to be accessible to the workflow run through cromwell with AWS backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070
https://github.com/broadinstitute/cromwell/pull/5070:99,Security,access,accessible,99,"This enhancement allows EFS or any parallel file system that can be mounted to the computes, to be accessible to the workflow run through cromwell with AWS backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070
https://github.com/broadinstitute/cromwell/pull/5073:46,Integrability,depend,dependencies,46,This PR adds an optional flag `-l` or `--list-dependencies` for command `validate` to list the imported files in the workflow and their subworkflows.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5073
https://github.com/broadinstitute/cromwell/pull/5073:73,Security,validat,validate,73,This PR adds an optional flag `-l` or `--list-dependencies` for command `validate` to list the imported files in the workflow and their subworkflows.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5073
https://github.com/broadinstitute/cromwell/pull/5075:949,Availability,error,error,949,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:512,Deployability,deploy,deploy-and-centaur,512,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:238,Performance,perform,performance,238,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:223,Testability,test,testing,223,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:285,Testability,test,tests,285,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:316,Testability,log,logic,316,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:787,Testability,test,tests,787,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5075:843,Testability,test,tests,843,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075
https://github.com/broadinstitute/cromwell/pull/5077:65,Deployability,Update,Updated,65,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5077:211,Deployability,update,updated,211,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5077:396,Deployability,Update,Update,396,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5077:251,Modifiability,Refactor,Refactored,251,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5077:607,Performance,load,load,607,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5077:560,Testability,test,test,560,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5077:664,Testability,test,tests,664,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077
https://github.com/broadinstitute/cromwell/pull/5082:23,Deployability,configurat,configuration,23,I'm proposing here the configuration I use for [TORQUE](https://www.adaptivecomputing.com/products/torque/).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082
https://github.com/broadinstitute/cromwell/pull/5082:68,Energy Efficiency,adapt,adaptivecomputing,68,I'm proposing here the configuration I use for [TORQUE](https://www.adaptivecomputing.com/products/torque/).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082
https://github.com/broadinstitute/cromwell/pull/5082:23,Modifiability,config,configuration,23,I'm proposing here the configuration I use for [TORQUE](https://www.adaptivecomputing.com/products/torque/).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082
https://github.com/broadinstitute/cromwell/pull/5082:68,Modifiability,adapt,adaptivecomputing,68,I'm proposing here the configuration I use for [TORQUE](https://www.adaptivecomputing.com/products/torque/).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082
https://github.com/broadinstitute/cromwell/issues/5083:65,Availability,error,error,65,"I'm creating a new cromwell server, but running into a migration error with postgres.; ( PostgreSQL v9.2.24 on CentOS7); I can switch to mysql, but just wanted to report the error: . ```; 2019-07-21 23:07:06,634 INFO - Running with database db.url = jdbc:postgresql://localhost:5432/cromwell; 2019-07-21 23:07:13,702 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,734 INFO - CREATE TABLE public.databasechangeloglock (ID INTEGER NOT NULL, LOCKED BOOLEAN NOT NULL, LOCKGRANTED TIMESTAMP WITHOUT TIME ZONE, LOCKEDBY VARCHAR(255), CONSTRAINT DATABASECHANGELOGLOCK_PKEY PRIMARY KEY (ID)); 2019-07-21 23:07:13,751 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,755 INFO - DELETE FROM public.databasechangeloglock; 2019-07-21 23:07:13,762 INFO - INSERT INTO public.databasechangeloglock (ID, LOCKED) VALUES (1, FALSE); 2019-07-21 23:07:13,767 INFO - SELECT LOCKED FROM public.databasechangeloglock WHERE ID=1; 2019-07-21 23:07:13,778 INFO - Successfully acquired change log lock; 2019-07-21 23:07:17,932 INFO - Creating database history table with name: public.databasechangelog; 2019-07-21 23:07:17,934 INFO - CREATE TABLE public.databasechangelog (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED TIMESTAMP WITHOUT TIME ZONE NOT NULL, ORDEREXECUTED INTEGER NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35), DESCRIPTION VARCHAR(255), COMMENTS VARCHAR(255), TAG VARCHAR(255), LIQUIBASE VARCHAR(20), CONTEXTS VARCHAR(255), LABELS VARCHAR(255), DEPLOYMENT_ID VARCHAR(10)); 2019-07-21 23:07:17,983 INFO - SELECT COUNT(*) FROM public.databasechangelog; 2019-07-21 23:07:17,985 INFO - Reading from public.databasechangelog; 2019-07-21 23:07:17,986 INFO - SELECT * FROM public.databasechangelog ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-07-21 23:07:17,987 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:18,146 INFO - CREATE TABLE ""public"".""CALL_CACHIN",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:174,Availability,error,error,174,"I'm creating a new cromwell server, but running into a migration error with postgres.; ( PostgreSQL v9.2.24 on CentOS7); I can switch to mysql, but just wanted to report the error: . ```; 2019-07-21 23:07:06,634 INFO - Running with database db.url = jdbc:postgresql://localhost:5432/cromwell; 2019-07-21 23:07:13,702 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,734 INFO - CREATE TABLE public.databasechangeloglock (ID INTEGER NOT NULL, LOCKED BOOLEAN NOT NULL, LOCKGRANTED TIMESTAMP WITHOUT TIME ZONE, LOCKEDBY VARCHAR(255), CONSTRAINT DATABASECHANGELOGLOCK_PKEY PRIMARY KEY (ID)); 2019-07-21 23:07:13,751 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,755 INFO - DELETE FROM public.databasechangeloglock; 2019-07-21 23:07:13,762 INFO - INSERT INTO public.databasechangeloglock (ID, LOCKED) VALUES (1, FALSE); 2019-07-21 23:07:13,767 INFO - SELECT LOCKED FROM public.databasechangeloglock WHERE ID=1; 2019-07-21 23:07:13,778 INFO - Successfully acquired change log lock; 2019-07-21 23:07:17,932 INFO - Creating database history table with name: public.databasechangelog; 2019-07-21 23:07:17,934 INFO - CREATE TABLE public.databasechangelog (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED TIMESTAMP WITHOUT TIME ZONE NOT NULL, ORDEREXECUTED INTEGER NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35), DESCRIPTION VARCHAR(255), COMMENTS VARCHAR(255), TAG VARCHAR(255), LIQUIBASE VARCHAR(20), CONTEXTS VARCHAR(255), LABELS VARCHAR(255), DEPLOYMENT_ID VARCHAR(10)); 2019-07-21 23:07:17,983 INFO - SELECT COUNT(*) FROM public.databasechangelog; 2019-07-21 23:07:17,985 INFO - Reading from public.databasechangelog; 2019-07-21 23:07:17,986 INFO - SELECT * FROM public.databasechangelog ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-07-21 23:07:17,987 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:18,146 INFO - CREATE TABLE ""public"".""CALL_CACHIN",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34499,Availability,ERROR,ERROR,34499,"D, referencedTableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,321 INFO - ALTER TABLE ""public"".""WORKFLOW_STORE_ENTRY"" ADD ""HOG_GROUP"" VARCHAR(100); 2019-07-21 23:07:19,322 INFO - Columns HOG_GROUP(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2019-07-21 23:07:19,331 INFO - ChangeSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.vis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34633,Availability,Error,Error,34633,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34640,Availability,ERROR,ERROR,34640,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34654,Availability,error,error,34654,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34881,Availability,ERROR,ERROR,34881,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34937,Availability,down,down,34937,"T INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35187,Availability,ERROR,ERROR,35187,"7e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35201,Availability,error,error,35201,"7e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36726,Availability,ERROR,ERROR,36726,"seUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36740,Availability,error,error,36740,"seUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:37439,Availability,ERROR,ERROR,37439,"; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307); 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266); 	at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:95); 	at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java); 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:352); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:37453,Availability,error,error,37453,"; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307); 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266); 	at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:95); 	at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java); 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:352); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34831,Deployability,release,released,34831,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35429,Deployability,Update,UpdateVisitor,35429,"_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35449,Deployability,Update,UpdateVisitor,35449,"RY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35571,Deployability,update,update,35571,"all_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35623,Deployability,update,update,35623,". Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35709,Deployability,update,updateSchema,35709,"_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35806,Deployability,update,updateSchema,35806,"ccessfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36030,Energy Efficiency,adapt,adapted,36030,"t changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35881,Modifiability,Enhance,EnhancedSqlDatabase,35881,"OR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:35986,Modifiability,Enhance,EnhancedSqlDatabase,35986,"xception: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36030,Modifiability,adapt,adapted,36030,"t changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36477,Performance,concurren,concurrent,36477,"base.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Positio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36562,Performance,concurren,concurrent,36562,"ase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecuto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:1026,Testability,log,log,1026,"ell server, but running into a migration error with postgres.; ( PostgreSQL v9.2.24 on CentOS7); I can switch to mysql, but just wanted to report the error: . ```; 2019-07-21 23:07:06,634 INFO - Running with database db.url = jdbc:postgresql://localhost:5432/cromwell; 2019-07-21 23:07:13,702 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,734 INFO - CREATE TABLE public.databasechangeloglock (ID INTEGER NOT NULL, LOCKED BOOLEAN NOT NULL, LOCKGRANTED TIMESTAMP WITHOUT TIME ZONE, LOCKEDBY VARCHAR(255), CONSTRAINT DATABASECHANGELOGLOCK_PKEY PRIMARY KEY (ID)); 2019-07-21 23:07:13,751 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,755 INFO - DELETE FROM public.databasechangeloglock; 2019-07-21 23:07:13,762 INFO - INSERT INTO public.databasechangeloglock (ID, LOCKED) VALUES (1, FALSE); 2019-07-21 23:07:13,767 INFO - SELECT LOCKED FROM public.databasechangeloglock WHERE ID=1; 2019-07-21 23:07:13,778 INFO - Successfully acquired change log lock; 2019-07-21 23:07:17,932 INFO - Creating database history table with name: public.databasechangelog; 2019-07-21 23:07:17,934 INFO - CREATE TABLE public.databasechangelog (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED TIMESTAMP WITHOUT TIME ZONE NOT NULL, ORDEREXECUTED INTEGER NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35), DESCRIPTION VARCHAR(255), COMMENTS VARCHAR(255), TAG VARCHAR(255), LIQUIBASE VARCHAR(20), CONTEXTS VARCHAR(255), LABELS VARCHAR(255), DEPLOYMENT_ID VARCHAR(10)); 2019-07-21 23:07:17,983 INFO - SELECT COUNT(*) FROM public.databasechangelog; 2019-07-21 23:07:17,985 INFO - Reading from public.databasechangelog; 2019-07-21 23:07:17,986 INFO - SELECT * FROM public.databasechangelog ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-07-21 23:07:17,987 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:18,146 INFO - CREATE TABLE ""public"".""CALL_CACHING_AGGREGATION_ENTRY"" (""C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:34847,Testability,log,log,34847,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36172,Usability,Simpl,SimpleJdbcAction,36172,"ption: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5083:36242,Usability,Simpl,SimpleJdbcAction,36242,"alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083
https://github.com/broadinstitute/cromwell/issues/5084:169,Availability,error,error,169,"Starting up a new cromwell-44 server on CentOS7 with MySQL v5.7 (mysql Ver 14.14 Distrib 5.7.27, for Linux (x86_64) using EditLine wrapper). migrations complete without error:; ```; 2019-07-21 23:34:35,925 INFO - DROP INDEX METADATA_JOB_AND_KEY_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,935 INFO - Index METADATA_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1623,Availability,heartbeat,heartbeat,1623,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1687,Availability,heartbeat,heartbeatInterval,1687,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1746,Availability,failure,failureShutdownDuration,1746,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:3588,Availability,error,errors,3588,"red to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(Hikari",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:3675,Availability,ERROR,ERROR,3675,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:3683,Availability,Error,Error,3683,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:3765,Availability,error,error,3765,"well-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:6748,Availability,ERROR,ERROR,6748,"StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 2019-07-21 23:34:40,417 cromwell-system-akka.dispatchers.service-dispatcher-14 ERROR - Failed to summarize metadata; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where ""SUMMARY_NAME"" = 'WORKFLOW_METADATA_SUMMARY_ENTRY_I' at line 1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:6832,Availability,error,error,6832,"StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 2019-07-21 23:34:40,417 cromwell-system-akka.dispatchers.service-dispatcher-14 ERROR - Failed to summarize metadata; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where ""SUMMARY_NAME"" = 'WORKFLOW_METADATA_SUMMARY_ENTRY_I' at line 1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1298,Deployability,release,released,1298,"_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1368,Deployability,configurat,configuration,1368,"_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1633,Deployability,configurat,configuration,1633,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:5480,Energy Efficiency,adapt,adapted,5480,.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:46); 	at slick.jdbc.StatementInvoker.foreach(StatementInvoker.scala:15); 	at slick.jdbc.StreamingInvokerAction.run(StreamingInvokerAction.scala:22); 	at slick.jdbc.StreamingInvokerAction.run$(StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.T,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:131,Integrability,wrap,wrapper,131,"Starting up a new cromwell-44 server on CentOS7 with MySQL v5.7 (mysql Ver 14.14 Distrib 5.7.27, for Linux (x86_64) using EditLine wrapper). migrations complete without error:; ```; 2019-07-21 23:34:35,925 INFO - DROP INDEX METADATA_JOB_AND_KEY_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,935 INFO - Index METADATA_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1368,Modifiability,config,configuration,1368,"_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1633,Modifiability,config,configuration,1633,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1935,Modifiability,config,configured,1935,"als', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Dist",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:2413,Modifiability,config,configured,2413," name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:2587,Modifiability,config,configured,2587,"spatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:2761,Modifiability,config,configured,2761," : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:5480,Modifiability,adapt,adapted,5480,.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:46); 	at slick.jdbc.StatementInvoker.foreach(StatementInvoker.scala:15); 	at slick.jdbc.StreamingInvokerAction.run(StreamingInvokerAction.scala:22); 	at slick.jdbc.StreamingInvokerAction.run$(StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.T,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:2268,Performance,throttle,throttle,2268," Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-sys",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:6470,Performance,concurren,concurrent,6470,"StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 2019-07-21 23:34:40,417 cromwell-system-akka.dispatchers.service-dispatcher-14 ERROR - Failed to summarize metadata; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where ""SUMMARY_NAME"" = 'WORKFLOW_METADATA_SUMMARY_ENTRY_I' at line 1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:6555,Performance,concurren,concurrent,6555,"StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 2019-07-21 23:34:40,417 cromwell-system-akka.dispatchers.service-dispatcher-14 ERROR - Failed to summarize metadata; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where ""SUMMARY_NAME"" = 'WORKFLOW_METADATA_SUMMARY_ENTRY_I' at line 1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:3703,Safety,abort,abort,3703,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:3943,Safety,Abort,Aborting,3943,"well-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:2174,Security,hash,hash-lookup,2174,"Name=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:2252,Security,hash,hash-lookup,2252,"-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5084:1314,Testability,log,log,1314,"_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084
https://github.com/broadinstitute/cromwell/issues/5085:1274,Availability,down,download,1274,"omwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1360,Availability,down,download,1360,"roadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1627,Availability,echo,echo-job,1627,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1670,Availability,echo,echo-job,1670,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1738,Availability,echo,echo-job,1738,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1806,Availability,echo,echo-job,1806,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1885,Availability,echo,echo-job,1885,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1984,Availability,echo,echo-job,1984,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:2050,Availability,echo,echo-job,2050,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:2127,Availability,echo,echo-job,2127,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:837,Deployability,configurat,configuration,837,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1265,Deployability,release,releases,1265,"omwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1351,Deployability,release,releases,1351,"roadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:837,Modifiability,config,configuration,837,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:882,Security,PASSWORD,PASSWORDS,882,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:1137,Testability,test,test,1137,"m https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/issues/5085:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085
https://github.com/broadinstitute/cromwell/pull/5086:506,Testability,log,logs,506,"This code takes a json blob and can do things like keep keys or exclude them. ~Soon it will be able to~ It can add labels where they didn't exist and edit them in-place. ~Note Well: Will be in ""draft"" state until ready to fulfill the requirements stated in the ticket. So far it can:~. - [x] I have a full metadata JSON, but a user is requesting it rendered with includeKey or excludeKey options, return the desired result; - [x] I have a full metadata JSON, but the user is just requesting the outputs or logs (ie the /logs and /outputs endpoints). - [x] I have a full metadata JSON including subworkflows, but wish to exclude subworkflows. - [x] I have a full metadata JSON, but wish to edit the labels field. - [x] I have a full metadata JSON without labels, and wish to attach a labels field",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086
https://github.com/broadinstitute/cromwell/pull/5086:520,Testability,log,logs,520,"This code takes a json blob and can do things like keep keys or exclude them. ~Soon it will be able to~ It can add labels where they didn't exist and edit them in-place. ~Note Well: Will be in ""draft"" state until ready to fulfill the requirements stated in the ticket. So far it can:~. - [x] I have a full metadata JSON, but a user is requesting it rendered with includeKey or excludeKey options, return the desired result; - [x] I have a full metadata JSON, but the user is just requesting the outputs or logs (ie the /logs and /outputs endpoints). - [x] I have a full metadata JSON including subworkflows, but wish to exclude subworkflows. - [x] I have a full metadata JSON, but wish to edit the labels field. - [x] I have a full metadata JSON without labels, and wish to attach a labels field",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086
https://github.com/broadinstitute/cromwell/pull/5087:15,Availability,error,error,15,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087
https://github.com/broadinstitute/cromwell/pull/5087:46,Availability,error,error,46,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087
https://github.com/broadinstitute/cromwell/pull/5087:56,Integrability,message,message,56,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087
https://github.com/broadinstitute/cromwell/pull/5087:94,Safety,timeout,timeout,94,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087
https://github.com/broadinstitute/cromwell/pull/5087:161,Safety,timeout,timeout,161,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087
https://github.com/broadinstitute/cromwell/pull/5087:259,Testability,log,logged,259,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087
https://github.com/broadinstitute/cromwell/pull/5089:513,Performance,race condition,race conditions,513,"The purpose of this PR is to make `/metadata` more informative, as described in #4856.; It appears that Cromwell already stores the necessary info in a `stderr.log` file. Therefore, reading the content of that file and printing to the user should be enough.; However, one thing bothers us. In the method `handleExecutionResult` of the trait `StandardAsyncExecutionActor`, the comment says that ; ``` ; Only check stderr size if we need to, otherwise this results in a lot of unnecessary I/O that; may fail due to race conditions on quickly-executing jobs.; ```; But in order to give additional info to the user, we have to read the content of that file. Also, we do not understand, why this is such a big deal. The method `handleExecutionResult` must be called when the execution is finished and that file is written. What race conditions may arise and is it important in this case?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5089
https://github.com/broadinstitute/cromwell/pull/5089:823,Performance,race condition,race conditions,823,"The purpose of this PR is to make `/metadata` more informative, as described in #4856.; It appears that Cromwell already stores the necessary info in a `stderr.log` file. Therefore, reading the content of that file and printing to the user should be enough.; However, one thing bothers us. In the method `handleExecutionResult` of the trait `StandardAsyncExecutionActor`, the comment says that ; ``` ; Only check stderr size if we need to, otherwise this results in a lot of unnecessary I/O that; may fail due to race conditions on quickly-executing jobs.; ```; But in order to give additional info to the user, we have to read the content of that file. Also, we do not understand, why this is such a big deal. The method `handleExecutionResult` must be called when the execution is finished and that file is written. What race conditions may arise and is it important in this case?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5089
https://github.com/broadinstitute/cromwell/pull/5089:160,Testability,log,log,160,"The purpose of this PR is to make `/metadata` more informative, as described in #4856.; It appears that Cromwell already stores the necessary info in a `stderr.log` file. Therefore, reading the content of that file and printing to the user should be enough.; However, one thing bothers us. In the method `handleExecutionResult` of the trait `StandardAsyncExecutionActor`, the comment says that ; ``` ; Only check stderr size if we need to, otherwise this results in a lot of unnecessary I/O that; may fail due to race conditions on quickly-executing jobs.; ```; But in order to give additional info to the user, we have to read the content of that file. Also, we do not understand, why this is such a big deal. The method `handleExecutionResult` must be called when the execution is finished and that file is written. What race conditions may arise and is it important in this case?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5089
https://github.com/broadinstitute/cromwell/pull/5091:389,Performance,optimiz,optimizations,389,"Consolidates GCS localizations and delocalizations into one Action each. Speeds up the `lots_of_inputs` Centaur test from more than 70 minutes to about 20 minutes. This PR creates a localization script that groups localizations by source bucket, stages the localization script to GCS, then localizes the localization script to the node and runs it. This PR does not:. * Attempt any gsutil optimizations in copying.; * Consolidate localization or delocalization on other PAPI v2 supported input types: HTTP, DRS or SRA. These other input types may require separate localization script executions since at least some of them (DRS and SRA) will likely require different Docker images to actually run their localizations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5091
https://github.com/broadinstitute/cromwell/pull/5091:112,Testability,test,test,112,"Consolidates GCS localizations and delocalizations into one Action each. Speeds up the `lots_of_inputs` Centaur test from more than 70 minutes to about 20 minutes. This PR creates a localization script that groups localizations by source bucket, stages the localization script to GCS, then localizes the localization script to the node and runs it. This PR does not:. * Attempt any gsutil optimizations in copying.; * Consolidate localization or delocalization on other PAPI v2 supported input types: HTTP, DRS or SRA. These other input types may require separate localization script executions since at least some of them (DRS and SRA) will likely require different Docker images to actually run their localizations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5091
https://github.com/broadinstitute/cromwell/issues/5092:877,Availability,error,error,877,"```WDL; task SplitNCigarReads {; input {; File inputBam; File inputBamIndex; File referenceFasta; File referenceFastaDict; File referenceFastaFai; String outputBam; Array[File] intervals = []. Int memory = 4; Float memoryMultiplier = 4; String dockerImage = ""quay.io/biocontainers/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:1274,Availability,recover,recoverWith,1274,"/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:5454,Availability,robust,robustExecuteOrRecover,5454,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:5962,Availability,Error,Error,5962,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:5850,Energy Efficiency,Schedul,Scheduler,5850,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:5872,Energy Efficiency,Schedul,Scheduler,5872,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:2670,Modifiability,config,config,2670,ply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$Backgrou,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:2747,Modifiability,Config,ConfigAsyncJobExecutionActor,2747,ckContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecut,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:2817,Modifiability,config,config,2817,bleBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionAct,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:2883,Modifiability,Config,ConfigAsyncJobExecutionActor,2883,ation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:3192,Modifiability,config,config,3192,dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobEx,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:3260,Modifiability,Config,ConfigAsyncJobExecutionActor,3260, akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:15,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:3599,Modifiability,config,config,3599,tandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(Standar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:3726,Modifiability,Config,ConfigAsyncJobExecutionActor,3726,bExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.sta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:4039,Modifiability,config,config,4039, at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.back,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:4105,Modifiability,Config,ConfigAsyncJobExecutionActor,4105,riptContents$(StandardAsyncExecutionActor.scala:318); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:4420,Modifiability,config,config,4420,scala:175); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBack,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:4474,Modifiability,Config,ConfigAsyncJobExecutionActor,4474,xecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:4927,Modifiability,config,config,4927,ExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.excepti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:4986,Modifiability,Config,ConfigAsyncJobExecutionActor,4986,ctor.scala:11); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:158); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Ar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:5285,Modifiability,config,config,5285,"sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:5348,Modifiability,Config,ConfigAsyncJobExecutionActor,5348,"emAsyncJobExecutionActor.scala:155); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:1247,Performance,concurren,concurrent,1247,"age = ""quay.io/biocontainers/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:1316,Performance,concurren,concurrent,1316," mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:1393,Performance,concurren,concurrent,1393,"tNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instanti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:1714,Performance,concurren,concurrent,1714,"ns as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.inst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:1274,Safety,recover,recoverWith,1274,"/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6106,Security,validat,validation,6106,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6117,Security,Validat,Validation,6117,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6128,Security,Validat,ValidationTry,6128,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6160,Security,Validat,Validation,6160,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6192,Security,validat,validation,6192,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6203,Security,Validat,Validation,6203,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6214,Security,Validat,ValidationTry,6214,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/issues/5092:6246,Security,Validat,Validation,6246,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092
https://github.com/broadinstitute/cromwell/pull/5093:19,Testability,test,test,19,Created a new unit test demonstrating negative results in key exlusion.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093
https://github.com/broadinstitute/cromwell/pull/5096:511,Performance,race condition,race conditions,511,"The purpose of this PR is to make `/metadata` more informative, as described in #4856.; It appears that Cromwell already stores the necessary info in a `stderr.log` file. Therefore, reading the content of that file and printing to the user should be enough.; However, one thing bothers us. In the method `handleExecutionResult` of the trait `StandardAsyncExecutionActor`, the comment says that; ```; Only check stderr size if we need to, otherwise this results in a lot of unnecessary I/O that; may fail due to race conditions on quickly-executing jobs.; ```; But in order to give additional info to the user, we have to read the content of that file. Also, we do not understand, why this is such a big deal. The method `handleExecutionResult` must be called when the execution is finished and that file is written. What race conditions may arise and is it important in this case?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096
https://github.com/broadinstitute/cromwell/pull/5096:821,Performance,race condition,race conditions,821,"The purpose of this PR is to make `/metadata` more informative, as described in #4856.; It appears that Cromwell already stores the necessary info in a `stderr.log` file. Therefore, reading the content of that file and printing to the user should be enough.; However, one thing bothers us. In the method `handleExecutionResult` of the trait `StandardAsyncExecutionActor`, the comment says that; ```; Only check stderr size if we need to, otherwise this results in a lot of unnecessary I/O that; may fail due to race conditions on quickly-executing jobs.; ```; But in order to give additional info to the user, we have to read the content of that file. Also, we do not understand, why this is such a big deal. The method `handleExecutionResult` must be called when the execution is finished and that file is written. What race conditions may arise and is it important in this case?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096
https://github.com/broadinstitute/cromwell/pull/5096:160,Testability,log,log,160,"The purpose of this PR is to make `/metadata` more informative, as described in #4856.; It appears that Cromwell already stores the necessary info in a `stderr.log` file. Therefore, reading the content of that file and printing to the user should be enough.; However, one thing bothers us. In the method `handleExecutionResult` of the trait `StandardAsyncExecutionActor`, the comment says that; ```; Only check stderr size if we need to, otherwise this results in a lot of unnecessary I/O that; may fail due to race conditions on quickly-executing jobs.; ```; But in order to give additional info to the user, we have to read the content of that file. Also, we do not understand, why this is such a big deal. The method `handleExecutionResult` must be called when the execution is finished and that file is written. What race conditions may arise and is it important in this case?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096
https://github.com/broadinstitute/cromwell/pull/5097:600,Energy Efficiency,reduce,reduce,600,"The purpose of this PR is to fix issue #4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory. There were already two PRs from us ([1](https://github.com/broadinstitute/cromwell/pull/5064), [2](https://github.com/broadinstitute/cromwell/pull/5057)) aimed to solve this issue, but these were not the appropriate solutions.; This time we found what part of GCP backend handles these ad hoc files and implemented the same logic on AWS. Also, in order to reduce the amount of duplicate code, we made a small refactoring.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097
https://github.com/broadinstitute/cromwell/pull/5097:653,Modifiability,refactor,refactoring,653,"The purpose of this PR is to fix issue #4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory. There were already two PRs from us ([1](https://github.com/broadinstitute/cromwell/pull/5064), [2](https://github.com/broadinstitute/cromwell/pull/5057)) aimed to solve this issue, but these were not the appropriate solutions.; This time we found what part of GCP backend handles these ad hoc files and implemented the same logic on AWS. Also, in order to reduce the amount of duplicate code, we made a small refactoring.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097
https://github.com/broadinstitute/cromwell/pull/5097:568,Testability,log,logic,568,"The purpose of this PR is to fix issue #4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory. There were already two PRs from us ([1](https://github.com/broadinstitute/cromwell/pull/5064), [2](https://github.com/broadinstitute/cromwell/pull/5057)) aimed to solve this issue, but these were not the appropriate solutions.; This time we found what part of GCP backend handles these ad hoc files and implemented the same logic on AWS. Also, in order to reduce the amount of duplicate code, we made a small refactoring.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097
https://github.com/broadinstitute/cromwell/pull/5098:46,Integrability,depend,dependencies,46,This PR adds an optional flag `-l` or` --list-dependencies` for command `validate` to list the imported files in the workflow and their subworkflows. JIRA ticket: [here](https://broadworkbench.atlassian.net/browse/BA-3501),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5098
https://github.com/broadinstitute/cromwell/pull/5098:73,Security,validat,validate,73,This PR adds an optional flag `-l` or` --list-dependencies` for command `validate` to list the imported files in the workflow and their subworkflows. JIRA ticket: [here](https://broadworkbench.atlassian.net/browse/BA-3501),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5098
https://github.com/broadinstitute/cromwell/pull/5100:22,Availability,Down,Download,22,"Adds capability to. * Download metadata from GCS using the IoActor; * Fetch workflow labels from the database; * Combine the two into a single response. Currently tested using mock IoActor and ServiceRegistry, and not ""created"" by the main Cromwell process",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5100
https://github.com/broadinstitute/cromwell/pull/5100:163,Testability,test,tested,163,"Adds capability to. * Download metadata from GCS using the IoActor; * Fetch workflow labels from the database; * Combine the two into a single response. Currently tested using mock IoActor and ServiceRegistry, and not ""created"" by the main Cromwell process",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5100
https://github.com/broadinstitute/cromwell/pull/5100:176,Testability,mock,mock,176,"Adds capability to. * Download metadata from GCS using the IoActor; * Fetch workflow labels from the database; * Combine the two into a single response. Currently tested using mock IoActor and ServiceRegistry, and not ""created"" by the main Cromwell process",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5100
https://github.com/broadinstitute/cromwell/pull/5101:215,Availability,robust,robust,215,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101
https://github.com/broadinstitute/cromwell/pull/5101:105,Testability,test,test,105,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101
https://github.com/broadinstitute/cromwell/pull/5101:128,Testability,test,test,128,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101
https://github.com/broadinstitute/cromwell/pull/5101:231,Testability,assert,assertions,231,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101
https://github.com/broadinstitute/cromwell/pull/5101:262,Testability,assert,assertion,262,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101
https://github.com/broadinstitute/cromwell/pull/5101:386,Testability,assert,assert,386,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101
https://github.com/broadinstitute/cromwell/pull/5102:122,Availability,failure,failure,122,...until such time as 100KB can be read from OSS in under 60 seconds. Multiple PRs are backed up behind this chronic test failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5102
https://github.com/broadinstitute/cromwell/pull/5102:117,Testability,test,test,117,...until such time as 100KB can be read from OSS in under 60 seconds. Multiple PRs are backed up behind this chronic test failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5102
https://github.com/broadinstitute/cromwell/pull/5104:923,Security,validat,validateSubmitArguments,923,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:1110,Security,validat,validateRunArguments,1110,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:1270,Security,validat,validateSubmitArguments,1270,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:1469,Security,validat,validateRunArguments,1469,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:1524,Security,validat,validateSubmitArguments,1524,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:120,Testability,test,test,120,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:312,Testability,test,test,312,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/pull/5104:1507,Testability,log,logic,1507,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104
https://github.com/broadinstitute/cromwell/issues/5105:1554,Security,access,accessible,1554,"a workflow with one File output named csvFile:; **workflowOptions file (I have tried this with and without ""file://"" with same results)**:; ```json; {; ""final_workflow_outputs_dir"": ""file:///data/external/workflow_1/4fd344c8-a228-421b-b561-9ed516e2316c"",; ""use_relative_output_paths"": true; }; ```; **Final outputs**; ```json; {; ""cwl_temp_file_ad3d3e78-d6a6-421a-9111-86fdefe14b80.cwl.csvFile"": ""\""/cromwell-executions/cwl_temp_file_ad3d3e78-d6a6-421a-9111-86fdefe14b80.cwl/ad3d3e78-d6a6-421a-9111-86fdefe14b80/call-getdataframe/execution/glob-aae5e4d226234858387812bc5d30218c/217.csv\""""; }; ```. After the workflow successfully executes, the specified output directory remains empty. **Environment notes**; I'm using Cromwell in server mode in a Docker container as a service to be consumed by other Docker applications on the same host. The client applications communicate with the Cromwell container using the python requests library. The specified final_workflow_outputs_dir is located in a bind mount accessible from both containers at the same location (e.g. /data/external is the default directory ""external"" to the containers which is mounted on all containers at that location). I have a workaround with a workflow step that makes a request back to the client service, but this is not ideal because it requires the users to modify the workflows. The client software includes an Angular application for editing workflows using Rabix's cwl-svg. **Full workflow**; ```yaml; $namespaces: {sbg: https://www.sevenbridges.com}; class: Workflow; cwlVersion: v1.0; doc: A test workflow to demonstrate the editor.; id: workflow1; inputs:; - {id: omics_url, sbg:x: -158.51063537597656, sbg:y: 29.940061569213867, type: string}; - {id: omics_auth_token, sbg:x: -214.89361572265625, sbg:y: 170.31314086914062, type: string}; - {id: collection_id, sbg:x: -152.0425567626953, sbg:y: 306.3538818359375, type: int}; label: Test Workflow; outputs:; - id: csvFile; outputSource: [getdataframe/csvFile]; sbg:x: ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105
https://github.com/broadinstitute/cromwell/issues/5105:2120,Testability,test,test,2120,"; }; ```. After the workflow successfully executes, the specified output directory remains empty. **Environment notes**; I'm using Cromwell in server mode in a Docker container as a service to be consumed by other Docker applications on the same host. The client applications communicate with the Cromwell container using the python requests library. The specified final_workflow_outputs_dir is located in a bind mount accessible from both containers at the same location (e.g. /data/external is the default directory ""external"" to the containers which is mounted on all containers at that location). I have a workaround with a workflow step that makes a request back to the client service, but this is not ideal because it requires the users to modify the workflows. The client software includes an Angular application for editing workflows using Rabix's cwl-svg. **Full workflow**; ```yaml; $namespaces: {sbg: https://www.sevenbridges.com}; class: Workflow; cwlVersion: v1.0; doc: A test workflow to demonstrate the editor.; id: workflow1; inputs:; - {id: omics_url, sbg:x: -158.51063537597656, sbg:y: 29.940061569213867, type: string}; - {id: omics_auth_token, sbg:x: -214.89361572265625, sbg:y: 170.31314086914062, type: string}; - {id: collection_id, sbg:x: -152.0425567626953, sbg:y: 306.3538818359375, type: int}; label: Test Workflow; outputs:; - id: csvFile; outputSource: [getdataframe/csvFile]; sbg:x: 523.4833374023438; sbg:y: 191.5; type: File; requirements:; - {class: MultipleInputFeatureRequirement}; steps:; - id: getcollection; in:; - {id: collection_id, source: collection_id}; - id: omics_url; source: [omics_url, omics_url, omics_url, omics_url]; - id: omics_auth_token; source: [omics_auth_token, omics_auth_token, omics_auth_token, omics_auth_token]; label: Get Collection; out:; - {id: collection_file}; run:; baseCommand: [getcollection.py]; class: CommandLineTool; cwlVersion: v1.0; doc: Get a collection as an HDF5 file.; id: getcollection; inputs:; - id: collection_id; in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105
https://github.com/broadinstitute/cromwell/issues/5105:2463,Testability,Test,Test,2463,"specified final_workflow_outputs_dir is located in a bind mount accessible from both containers at the same location (e.g. /data/external is the default directory ""external"" to the containers which is mounted on all containers at that location). I have a workaround with a workflow step that makes a request back to the client service, but this is not ideal because it requires the users to modify the workflows. The client software includes an Angular application for editing workflows using Rabix's cwl-svg. **Full workflow**; ```yaml; $namespaces: {sbg: https://www.sevenbridges.com}; class: Workflow; cwlVersion: v1.0; doc: A test workflow to demonstrate the editor.; id: workflow1; inputs:; - {id: omics_url, sbg:x: -158.51063537597656, sbg:y: 29.940061569213867, type: string}; - {id: omics_auth_token, sbg:x: -214.89361572265625, sbg:y: 170.31314086914062, type: string}; - {id: collection_id, sbg:x: -152.0425567626953, sbg:y: 306.3538818359375, type: int}; label: Test Workflow; outputs:; - id: csvFile; outputSource: [getdataframe/csvFile]; sbg:x: 523.4833374023438; sbg:y: 191.5; type: File; requirements:; - {class: MultipleInputFeatureRequirement}; steps:; - id: getcollection; in:; - {id: collection_id, source: collection_id}; - id: omics_url; source: [omics_url, omics_url, omics_url, omics_url]; - id: omics_auth_token; source: [omics_auth_token, omics_auth_token, omics_auth_token, omics_auth_token]; label: Get Collection; out:; - {id: collection_file}; run:; baseCommand: [getcollection.py]; class: CommandLineTool; cwlVersion: v1.0; doc: Get a collection as an HDF5 file.; id: getcollection; inputs:; - id: collection_id; inputBinding: {position: 0}; type: int; - id: omics_url; inputBinding: {position: 1}; type: string; - id: omics_auth_token; inputBinding: {position: 2}; type: string; label: Get Collection; outputs:; - id: collection_file; outputBinding: {glob: '*.h5'}; type: File; sbg:x: 32.978721618652344; sbg:y: 166.41757202148438; - id: getdataframe; in:; - {id: inputF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105
https://github.com/broadinstitute/cromwell/pull/5106:171,Availability,down,down,171,"There is a claim of 5–10% faster compiler performance since 2.12.8, which seems worthwhile. Not super scientific, but a build test before and after (with warmed JVM) went down from about 106 to 97 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5106
https://github.com/broadinstitute/cromwell/pull/5106:42,Performance,perform,performance,42,"There is a claim of 5–10% faster compiler performance since 2.12.8, which seems worthwhile. Not super scientific, but a build test before and after (with warmed JVM) went down from about 106 to 97 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5106
https://github.com/broadinstitute/cromwell/pull/5106:126,Testability,test,test,126,"There is a claim of 5–10% faster compiler performance since 2.12.8, which seems worthwhile. Not super scientific, but a build test before and after (with warmed JVM) went down from about 106 to 97 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5106
https://github.com/broadinstitute/cromwell/pull/5107:151,Availability,error,error,151,"With your current version of Cromwell, the workflow does not terminate even if the underlying task is killed by the HPC scheduler due to out of memory error. This should be generalized to batch schedulers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107
https://github.com/broadinstitute/cromwell/pull/5107:120,Energy Efficiency,schedul,scheduler,120,"With your current version of Cromwell, the workflow does not terminate even if the underlying task is killed by the HPC scheduler due to out of memory error. This should be generalized to batch schedulers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107
https://github.com/broadinstitute/cromwell/pull/5107:194,Energy Efficiency,schedul,schedulers,194,"With your current version of Cromwell, the workflow does not terminate even if the underlying task is killed by the HPC scheduler due to out of memory error. This should be generalized to batch schedulers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107
https://github.com/broadinstitute/cromwell/pull/5109:430,Testability,log,logic,430,"An interim step in parallelizing (de)localization during which a bug in a delocalization edge case was found and fixed. * In determining requester pays status on optional outputs: don't definitively determine status unless a file or directory was actually transferred, regardless of the output being optional or not.; * Pull a library of transfer-related bash functions out into its own file, while fully dynamic (de)localization logic lives in separate files `source`ing this library.; * Creates a structure supporting parallel transfers which can `source` the transfer library.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5109
https://github.com/broadinstitute/cromwell/pull/5110:497,Deployability,integrat,integration,497,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:497,Integrability,integrat,integration,497,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:461,Modifiability,Refactor,Refactored,461,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:153,Testability,assert,assertions,153,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:300,Testability,log,logs,300,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:331,Testability,test,tested,331,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:509,Testability,test,tests,509,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5110:560,Testability,log,logs,560,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110
https://github.com/broadinstitute/cromwell/pull/5113:175,Availability,Error,Error,175,"The description of the problem can be found here; https://broadworkbench.atlassian.net/browse/BA-5881; Unfortunately, there is no direct way to reproduce `500 Internal Server Error Backend Error`. Therefore, it is unclear how to show that this PR solves the problem.; However, since this error causes IOException, we think that reproducing some other IOException with this file should reproduce the problem close enough. If you are interested, we have an [experimental branch](https://github.com/EpamLifeSciencesTeam/cromwell/pull/9) for reproducing IOException. In that branch, Cromwell stops during execution, giving us time to delete the `rc` file if we want to. It allows us to see that this fix actually works and Cromwell indeed tries to read the file multiple times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113
https://github.com/broadinstitute/cromwell/pull/5113:189,Availability,Error,Error,189,"The description of the problem can be found here; https://broadworkbench.atlassian.net/browse/BA-5881; Unfortunately, there is no direct way to reproduce `500 Internal Server Error Backend Error`. Therefore, it is unclear how to show that this PR solves the problem.; However, since this error causes IOException, we think that reproducing some other IOException with this file should reproduce the problem close enough. If you are interested, we have an [experimental branch](https://github.com/EpamLifeSciencesTeam/cromwell/pull/9) for reproducing IOException. In that branch, Cromwell stops during execution, giving us time to delete the `rc` file if we want to. It allows us to see that this fix actually works and Cromwell indeed tries to read the file multiple times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113
https://github.com/broadinstitute/cromwell/pull/5113:288,Availability,error,error,288,"The description of the problem can be found here; https://broadworkbench.atlassian.net/browse/BA-5881; Unfortunately, there is no direct way to reproduce `500 Internal Server Error Backend Error`. Therefore, it is unclear how to show that this PR solves the problem.; However, since this error causes IOException, we think that reproducing some other IOException with this file should reproduce the problem close enough. If you are interested, we have an [experimental branch](https://github.com/EpamLifeSciencesTeam/cromwell/pull/9) for reproducing IOException. In that branch, Cromwell stops during execution, giving us time to delete the `rc` file if we want to. It allows us to see that this fix actually works and Cromwell indeed tries to read the file multiple times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113
https://github.com/broadinstitute/cromwell/pull/5119:189,Performance,cache,cache,189,"Specifically to prevent extremely long cycles of the states `CheckingCallCache`, `FetchingCachedOutputsFromDatabase`, `BackendIsCopyingCachedOutputs`. This has been observed in cases where cache copying has failed, the system goes back to the call cache to try another entry, it fails again, and so on and so on, indefinitely... The upshot in one case was 49,000 ""cache cycles"", a 30MB pile of metadata, and a non-responsive Job Manager page. This PR collapses these three similar states (and a few others) into a single transition event (now called `CallCacheReading`) to cover the entire duration of the call cache read process. EDIT: Note that this is not a panacea. The workflow identified in another ticket (BA-5830) had over 250,000 ""execution events"", but this was just because there were almost 20,000 scattered shards - and only one ""call cache cycle"" per shard. In that case this PR will only have a marginal impact.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119
https://github.com/broadinstitute/cromwell/pull/5119:248,Performance,cache,cache,248,"Specifically to prevent extremely long cycles of the states `CheckingCallCache`, `FetchingCachedOutputsFromDatabase`, `BackendIsCopyingCachedOutputs`. This has been observed in cases where cache copying has failed, the system goes back to the call cache to try another entry, it fails again, and so on and so on, indefinitely... The upshot in one case was 49,000 ""cache cycles"", a 30MB pile of metadata, and a non-responsive Job Manager page. This PR collapses these three similar states (and a few others) into a single transition event (now called `CallCacheReading`) to cover the entire duration of the call cache read process. EDIT: Note that this is not a panacea. The workflow identified in another ticket (BA-5830) had over 250,000 ""execution events"", but this was just because there were almost 20,000 scattered shards - and only one ""call cache cycle"" per shard. In that case this PR will only have a marginal impact.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119
https://github.com/broadinstitute/cromwell/pull/5119:364,Performance,cache,cache,364,"Specifically to prevent extremely long cycles of the states `CheckingCallCache`, `FetchingCachedOutputsFromDatabase`, `BackendIsCopyingCachedOutputs`. This has been observed in cases where cache copying has failed, the system goes back to the call cache to try another entry, it fails again, and so on and so on, indefinitely... The upshot in one case was 49,000 ""cache cycles"", a 30MB pile of metadata, and a non-responsive Job Manager page. This PR collapses these three similar states (and a few others) into a single transition event (now called `CallCacheReading`) to cover the entire duration of the call cache read process. EDIT: Note that this is not a panacea. The workflow identified in another ticket (BA-5830) had over 250,000 ""execution events"", but this was just because there were almost 20,000 scattered shards - and only one ""call cache cycle"" per shard. In that case this PR will only have a marginal impact.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119
https://github.com/broadinstitute/cromwell/pull/5119:611,Performance,cache,cache,611,"Specifically to prevent extremely long cycles of the states `CheckingCallCache`, `FetchingCachedOutputsFromDatabase`, `BackendIsCopyingCachedOutputs`. This has been observed in cases where cache copying has failed, the system goes back to the call cache to try another entry, it fails again, and so on and so on, indefinitely... The upshot in one case was 49,000 ""cache cycles"", a 30MB pile of metadata, and a non-responsive Job Manager page. This PR collapses these three similar states (and a few others) into a single transition event (now called `CallCacheReading`) to cover the entire duration of the call cache read process. EDIT: Note that this is not a panacea. The workflow identified in another ticket (BA-5830) had over 250,000 ""execution events"", but this was just because there were almost 20,000 scattered shards - and only one ""call cache cycle"" per shard. In that case this PR will only have a marginal impact.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119
https://github.com/broadinstitute/cromwell/pull/5119:848,Performance,cache,cache,848,"Specifically to prevent extremely long cycles of the states `CheckingCallCache`, `FetchingCachedOutputsFromDatabase`, `BackendIsCopyingCachedOutputs`. This has been observed in cases where cache copying has failed, the system goes back to the call cache to try another entry, it fails again, and so on and so on, indefinitely... The upshot in one case was 49,000 ""cache cycles"", a 30MB pile of metadata, and a non-responsive Job Manager page. This PR collapses these three similar states (and a few others) into a single transition event (now called `CallCacheReading`) to cover the entire duration of the call cache read process. EDIT: Note that this is not a panacea. The workflow identified in another ticket (BA-5830) had over 250,000 ""execution events"", but this was just because there were almost 20,000 scattered shards - and only one ""call cache cycle"" per shard. In that case this PR will only have a marginal impact.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119
https://github.com/broadinstitute/cromwell/pull/5119:414,Usability,responsiv,responsive,414,"Specifically to prevent extremely long cycles of the states `CheckingCallCache`, `FetchingCachedOutputsFromDatabase`, `BackendIsCopyingCachedOutputs`. This has been observed in cases where cache copying has failed, the system goes back to the call cache to try another entry, it fails again, and so on and so on, indefinitely... The upshot in one case was 49,000 ""cache cycles"", a 30MB pile of metadata, and a non-responsive Job Manager page. This PR collapses these three similar states (and a few others) into a single transition event (now called `CallCacheReading`) to cover the entire duration of the call cache read process. EDIT: Note that this is not a panacea. The workflow identified in another ticket (BA-5830) had over 250,000 ""execution events"", but this was just because there were almost 20,000 scattered shards - and only one ""call cache cycle"" per shard. In that case this PR will only have a marginal impact.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5119
https://github.com/broadinstitute/cromwell/pull/5120:142,Availability,down,downloaded,142,DRS ammonite localizer script is gone!! This PR replaces it with a executable jar in docker where the dependencies are no longer complied and downloaded each time you want to localizer DRS input!. JIRA [ticket](https://broadworkbench.atlassian.net/browse/BA-5821),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5120
https://github.com/broadinstitute/cromwell/pull/5120:102,Integrability,depend,dependencies,102,DRS ammonite localizer script is gone!! This PR replaces it with a executable jar in docker where the dependencies are no longer complied and downloaded each time you want to localizer DRS input!. JIRA [ticket](https://broadworkbench.atlassian.net/browse/BA-5821),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5120
https://github.com/broadinstitute/cromwell/pull/5125:122,Availability,failure,failures,122,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. 🤷‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125
https://github.com/broadinstitute/cromwell/pull/5125:211,Energy Efficiency,monitor,monitoring,211,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. 🤷‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125
https://github.com/broadinstitute/cromwell/pull/5125:222,Testability,log,log,222,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. 🤷‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125
https://github.com/broadinstitute/cromwell/pull/5125:226,Testability,assert,assertion,226,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. 🤷‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125
https://github.com/broadinstitute/cromwell/issues/5126:93,Availability,error,error,93,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126
https://github.com/broadinstitute/cromwell/issues/5126:163,Availability,Error,Error,163,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126
https://github.com/broadinstitute/cromwell/issues/5126:1034,Deployability,configurat,configuration,1034,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126
https://github.com/broadinstitute/cromwell/issues/5126:1034,Modifiability,config,configuration,1034,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126
https://github.com/broadinstitute/cromwell/issues/5126:1079,Security,PASSWORD,PASSWORDS,1079,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126
https://github.com/broadinstitute/cromwell/issues/5126:244,Usability,feedback,feedback,244,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126
https://github.com/broadinstitute/cromwell/pull/5128:0,Testability,Test,Tests,0,"Tests pass with this new value, where they didn't before",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5128
https://github.com/broadinstitute/cromwell/pull/5129:56,Availability,Error,ErrorReporter,56,The PAPIv2 version of `interpretOperationStatus` calls `ErrorReporter#toUnsuccessfulRunStatus` instead of `RunStatus.UnsuccessfulRunStatus#apply`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5129
https://github.com/broadinstitute/cromwell/pull/5130:94,Testability,test,tests,94,This is a duplicate of PR [5104](https://github.com/broadinstitute/cromwell/pull/5104) to run tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5130
https://github.com/broadinstitute/cromwell/issues/5131:98,Availability,error,error,98,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:388,Deployability,pipeline,pipeline,388,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:825,Deployability,configurat,configuration,825,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:739,Energy Efficiency,allocate,allocated,739,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:825,Modifiability,config,configuration,825,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:862,Safety,avoid,avoid,862,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:114,Testability,log,log,114,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:324,Testability,test,tested,324,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/issues/5131:169,Usability,guid,guidance,169,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131
https://github.com/broadinstitute/cromwell/pull/5132:51,Energy Efficiency,monitor,monitoring,51,"Since the `monitoring_log` test checks whether the monitoring script is working normally, which it is, for the time being fix the memory expectation to unblock other PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5132
https://github.com/broadinstitute/cromwell/pull/5132:27,Testability,test,test,27,"Since the `monitoring_log` test checks whether the monitoring script is working normally, which it is, for the time being fix the memory expectation to unblock other PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5132
https://github.com/broadinstitute/cromwell/pull/5133:36,Energy Efficiency,monitor,monitoring,36,Includes:. * Saloni's fixes for the monitoring test breakage; * Dan's fixes for GPU test breakages; * A revert of external contribution #5113 that was never run through full CI before merge to develop and causes PAPI v2 builds to hang,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5133
https://github.com/broadinstitute/cromwell/pull/5133:47,Testability,test,test,47,Includes:. * Saloni's fixes for the monitoring test breakage; * Dan's fixes for GPU test breakages; * A revert of external contribution #5113 that was never run through full CI before merge to develop and causes PAPI v2 builds to hang,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5133
https://github.com/broadinstitute/cromwell/pull/5133:84,Testability,test,test,84,Includes:. * Saloni's fixes for the monitoring test breakage; * Dan's fixes for GPU test breakages; * A revert of external contribution #5113 that was never run through full CI before merge to develop and causes PAPI v2 builds to hang,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5133
https://github.com/broadinstitute/cromwell/pull/5135:787,Availability,echo,echo,787,"In an issue [BA-5929](https://broadworkbench.atlassian.net/browse/BA-5929) a problem with big numbers, exceeding Int capacity, was introduced. Therefore, I decided to add BigDecimal type support in WDL language.; Changes:; - BigDecimal type introduced in WDL; - Basic binary and unary operations for number types are implemented for BigDecimal. ---; I referred to this list during operations implementation:; https://software.broadinstitute.org/wdl/documentation/spec#types. Manually tested on:; ### inputs.json:; ```json; {; ""my_wf.largenumber"": 7000000000; }; ```. ### workflow; ```; workflow my_wf {; BigDecimal largenumber. call print_number {; input: largenumber=largenumber; }; }; task print_number {; BigDecimal largenumber; BigDecimal largeNumIncr = largenumber + 42; command {; echo SOME_LARGE_NUMBER ${largeNumIncr}; }; output {; String lnum=read_string(stdout()); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135
https://github.com/broadinstitute/cromwell/pull/5135:484,Testability,test,tested,484,"In an issue [BA-5929](https://broadworkbench.atlassian.net/browse/BA-5929) a problem with big numbers, exceeding Int capacity, was introduced. Therefore, I decided to add BigDecimal type support in WDL language.; Changes:; - BigDecimal type introduced in WDL; - Basic binary and unary operations for number types are implemented for BigDecimal. ---; I referred to this list during operations implementation:; https://software.broadinstitute.org/wdl/documentation/spec#types. Manually tested on:; ### inputs.json:; ```json; {; ""my_wf.largenumber"": 7000000000; }; ```. ### workflow; ```; workflow my_wf {; BigDecimal largenumber. call print_number {; input: largenumber=largenumber; }; }; task print_number {; BigDecimal largenumber; BigDecimal largeNumIncr = largenumber + 42; command {; echo SOME_LARGE_NUMBER ${largeNumIncr}; }; output {; String lnum=read_string(stdout()); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135
https://github.com/broadinstitute/cromwell/issues/5136:191,Availability,failure,failures,191,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/issues/5136:330,Availability,error,error,330,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/issues/5136:741,Deployability,pipeline,pipelines-worker-,741,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/issues/5136:226,Integrability,message,message,226,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/issues/5136:1404,Safety,avoid,avoid,1404,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/issues/5136:1498,Safety,timeout,timeouts,1498,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/issues/5136:561,Testability,log,log,561,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136
https://github.com/broadinstitute/cromwell/pull/5141:125,Testability,test,test,125,"Partition same-named file inputs from other kinds of inputs for special `gsutil -m cp` treatment. . `lots_of_inputs` Centaur test 400-input timings on the (misleadingly named) `do_nothing` call:. * Cromwell 44: 4765.68 seconds / 400 inputs = 11.91 seconds / input; * Cromwell 45: 1181.64 seconds / 400 inputs = 2.95 seconds / input; * Cromwell 46: 200.82 seconds / 400 inputs = 0.50 seconds / input. But 400 inputs isn't really ""lots"", so trying this test again on 46 with 20,000 inputs to drown out the lower order terms:. * Cromwell 46: 2663.71 seconds / 20,000 inputs = 0.13 seconds / input",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141
https://github.com/broadinstitute/cromwell/pull/5141:451,Testability,test,test,451,"Partition same-named file inputs from other kinds of inputs for special `gsutil -m cp` treatment. . `lots_of_inputs` Centaur test 400-input timings on the (misleadingly named) `do_nothing` call:. * Cromwell 44: 4765.68 seconds / 400 inputs = 11.91 seconds / input; * Cromwell 45: 1181.64 seconds / 400 inputs = 2.95 seconds / input; * Cromwell 46: 200.82 seconds / 400 inputs = 0.50 seconds / input. But 400 inputs isn't really ""lots"", so trying this test again on 46 with 20,000 inputs to drown out the lower order terms:. * Cromwell 46: 2663.71 seconds / 20,000 inputs = 0.13 seconds / input",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141
https://github.com/broadinstitute/cromwell/pull/5143:324,Availability,error,error,324,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143
https://github.com/broadinstitute/cromwell/pull/5143:429,Availability,echo,echo,429,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143
https://github.com/broadinstitute/cromwell/pull/5143:732,Availability,error,error,732,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143
https://github.com/broadinstitute/cromwell/pull/5143:992,Availability,echo,echo,992,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143
https://github.com/broadinstitute/cromwell/pull/5143:5,Testability,test,testing,5,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143
https://github.com/broadinstitute/cromwell/pull/5143:289,Usability,simpl,simple,289,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143
https://github.com/broadinstitute/cromwell/pull/5144:57,Testability,test,tests,57,This application.conf is preventing me from running unit tests locally. It appears to be unnecessary to the remaining test cases so I'm trying out removing it. Let's see what travis says... 🤞,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5144
https://github.com/broadinstitute/cromwell/pull/5144:118,Testability,test,test,118,This application.conf is preventing me from running unit tests locally. It appears to be unnecessary to the remaining test cases so I'm trying out removing it. Let's see what travis says... 🤞,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5144
https://github.com/broadinstitute/cromwell/pull/5147:56,Testability,test,testing,56,We now invoke run mode from the command line instead of testing the actor in a special (and flakey) test harness.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5147
https://github.com/broadinstitute/cromwell/pull/5147:100,Testability,test,test,100,We now invoke run mode from the command line instead of testing the actor in a special (and flakey) test harness.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5147
https://github.com/broadinstitute/cromwell/pull/5150:446,Integrability,interface,interface,446,"### Motivation:. For motivation see the [metadata design doc](https://docs.google.com/document/d/1VYnzk97yTtllozO9ivZpZQTwrsY5T0wGqxlvAbrEQgg/edit?ts=5d5d601c#heading=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1163,Modifiability,refactor,refactor,1163,"### Motivation:. For motivation see the [metadata design doc](https://docs.google.com/document/d/1VYnzk97yTtllozO9ivZpZQTwrsY5T0wGqxlvAbrEQgg/edit?ts=5d5d601c#heading=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1901,Modifiability,refactor,refactor,1901,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1985,Modifiability,refactor,refactor,1985,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1871,Safety,risk,risky,1871,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1286,Testability,test,tests,1286,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1358,Testability,test,tests,1358,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:1939,Testability,test,test,1939,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5150:519,Usability,Guid,Guidance,519,"### Motivation:. For motivation see the [metadata design doc](https://docs.google.com/document/d/1VYnzk97yTtllozO9ivZpZQTwrsY5T0wGqxlvAbrEQgg/edit?ts=5d5d601c#heading=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150
https://github.com/broadinstitute/cromwell/pull/5151:61,Availability,error,error,61,"--mem-per-cpu=${requested_memory_mb_per_core} causes WomLong error on Cromwell 45.1, as reported here: . https://github.com/ENCODE-DCC/chip-seq-pipeline2/issues/59. Editting syntax to --mem-per-cpu ${requested_memory_mb_per_core} fixes this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5151
https://github.com/broadinstitute/cromwell/pull/5161:39,Deployability,upgrade,upgrade,39,"There is more than ""new shiny"" to this upgrade, it's supposed to be faster and better at IO and those are things we could very much use in our tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5161
https://github.com/broadinstitute/cromwell/pull/5161:143,Testability,test,tests,143,"There is more than ""new shiny"" to this upgrade, it's supposed to be faster and better at IO and those are things we could very much use in our tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5161
https://github.com/broadinstitute/cromwell/issues/5162:570,Availability,error,error,570,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:70,Deployability,pipeline,pipeline,70,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:576,Integrability,message,message,576,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1317,Integrability,interface,interface,1317,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:737,Modifiability,variab,variable,737,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:910,Modifiability,config,config,910,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1157,Modifiability,config,configure,1157,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1217,Modifiability,Config,Config,1217,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1732,Modifiability,config,config,1732,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1862,Performance,queue,queueArn,1862,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:2084,Performance,cache,cache-results,2084,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:519,Security,secur,security,519,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:841,Security,secur,security,841,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:938,Security,authenticat,authentication,938,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1101,Security,secur,security,1101,"Formation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-resu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:584,Testability,log,logged,584,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1966,Testability,log,log-dir,1966,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:1995,Testability,log,logs,1995,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5162:2011,Testability,log,log-temporary,2011,"rovided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162
https://github.com/broadinstitute/cromwell/issues/5166:221,Security,validat,validates,221,"When I try to have something like; ```wdl; runtime {; docker: ""quay.io/biocontainers/star@sha256:f9b0406354ff2e5ccfadaef6fde6367c7bcb4bdc7e67920f0f827a6ff6bf4fb5"" #2.7.2b--0; cpu: ""4""; memory_mb: ""51200""; }; ```; WOMTool validates it as valid WDL, but cromwell Server fails the workflow with:; ```; Workflow failed. WorkflowFailure(java.lang.IllegalArgumentException: No coercion defined from '4' of type 'java.lang.Integer' to 'String?'.,List(WorkflowFailure(No coercion defined from '4' of type 'java.lang.Integer' to 'String?'.,List()))); ```; I enclose ZIP with my WDL, input and application.json.; I run cromwell with Ubuntu Server 18.04.LTS; [cpu_coercion_bug.zip](https://github.com/broadinstitute/cromwell/files/3591979/cpu_coercion_bug.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5166
https://github.com/broadinstitute/cromwell/pull/5170:424,Performance,queue,queue,424,"The purpose of this PR is to provide ARN validation from [this](https://github.com/broadinstitute/cromwell/blob/88ba33918fd762599d8fd2e2c3d142a04ad5d2fb/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchRuntimeAttributes.scala#L159) TODO.; I added the validation of any ARN in a general form as stated in the TODO.; However, since the final regex turned out to be too vague, I also added validation for queue ARNs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5170
https://github.com/broadinstitute/cromwell/pull/5170:41,Security,validat,validation,41,"The purpose of this PR is to provide ARN validation from [this](https://github.com/broadinstitute/cromwell/blob/88ba33918fd762599d8fd2e2c3d142a04ad5d2fb/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchRuntimeAttributes.scala#L159) TODO.; I added the validation of any ARN in a general form as stated in the TODO.; However, since the final regex turned out to be too vague, I also added validation for queue ARNs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5170
https://github.com/broadinstitute/cromwell/pull/5170:273,Security,validat,validation,273,"The purpose of this PR is to provide ARN validation from [this](https://github.com/broadinstitute/cromwell/blob/88ba33918fd762599d8fd2e2c3d142a04ad5d2fb/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchRuntimeAttributes.scala#L159) TODO.; I added the validation of any ARN in a general form as stated in the TODO.; However, since the final regex turned out to be too vague, I also added validation for queue ARNs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5170
https://github.com/broadinstitute/cromwell/pull/5170:409,Security,validat,validation,409,"The purpose of this PR is to provide ARN validation from [this](https://github.com/broadinstitute/cromwell/blob/88ba33918fd762599d8fd2e2c3d142a04ad5d2fb/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchRuntimeAttributes.scala#L159) TODO.; I added the validation of any ARN in a general form as stated in the TODO.; However, since the final regex turned out to be too vague, I also added validation for queue ARNs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5170
https://github.com/broadinstitute/cromwell/pull/5172:787,Availability,avail,available,787,The performance of method `processRunnableTaskCallInputExpression` first appeared on our radar in https://github.com/broadinstitute/cromwell/pull/5048. It came up again yesterday when we had a high CPU event on all three runners. I took a thread dump during the event and there were 44 threads busy with; ```; at wom.graph.CommandCallNode.toString(CallNode.scala:68); at java.lang.String.valueOf(String.java:2994); at java.lang.StringBuilder.append(StringBuilder.java:131); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:549); ```. I think we are making the mistake of calculating a potentially very large `toString` by using the default case class implementation. I do have the full stack dump available in case anyone is curious. ![Screen Shot 2019-09-11 at 5 00 45 PM](https://user-images.githubusercontent.com/1087943/64735012-b60d3a00-d4b5-11e9-8f60-e9a55fd20f07.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5172
https://github.com/broadinstitute/cromwell/pull/5172:4,Performance,perform,performance,4,The performance of method `processRunnableTaskCallInputExpression` first appeared on our radar in https://github.com/broadinstitute/cromwell/pull/5048. It came up again yesterday when we had a high CPU event on all three runners. I took a thread dump during the event and there were 44 threads busy with; ```; at wom.graph.CommandCallNode.toString(CallNode.scala:68); at java.lang.String.valueOf(String.java:2994); at java.lang.StringBuilder.append(StringBuilder.java:131); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:549); ```. I think we are making the mistake of calculating a potentially very large `toString` by using the default case class implementation. I do have the full stack dump available in case anyone is curious. ![Screen Shot 2019-09-11 at 5 00 45 PM](https://user-images.githubusercontent.com/1087943/64735012-b60d3a00-d4b5-11e9-8f60-e9a55fd20f07.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5172
https://github.com/broadinstitute/cromwell/issues/5174:33,Performance,cache,cache,33,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174
https://github.com/broadinstitute/cromwell/issues/5174:153,Performance,cache,cache,153,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174
https://github.com/broadinstitute/cromwell/issues/5174:44,Safety,timeout,timeout,44,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174
https://github.com/broadinstitute/cromwell/issues/5174:308,Security,access,access,308,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174
https://github.com/broadinstitute/cromwell/pull/5175:264,Integrability,message,messages,264,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:242,Modifiability,config,configs,242,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:14,Testability,test,tests,14,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:45,Testability,test,tests,45,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:158,Testability,test,tests,158,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:214,Testability,test,tests,214,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:237,Testability,test,test,237,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:314,Testability,test,tests,314,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:419,Testability,test,tested,419,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/pull/5175:434,Testability,test,test,434,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175
https://github.com/broadinstitute/cromwell/issues/5178:1316,Availability,error,error,1316,"g `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1371,Availability,Error,Error,1371,"local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1421,Availability,Error,Error,1421,"local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3403,Energy Efficiency,adapt,adapted,3403,unLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3916,Energy Efficiency,adapt,adapted,3916,ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; </details>. A workaround is setting up a registry to host the images (so we can,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:225,Modifiability,config,config,225,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3403,Modifiability,adapt,adapted,3403,unLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3916,Modifiability,adapt,adapted,3916,ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; </details>. A workaround is setting up a registry to host the images (so we can,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:57,Performance,load,loaded,57,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:77,Performance,load,load,77,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1001,Performance,queue,queue,1001,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:2879,Performance,concurren,concurrent,2879,ker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.inter,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3700,Safety,unsafe,unsafeRunAsync,3700,t.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:240,Security,hash,hash-lookup,240,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1454,Security,access,access,1454,"local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1555,Security,access,access,1555,"local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:774,Testability,log,log,774,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:988,Testability,log,log,988,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1025,Testability,log,log,1025,"es that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:1529,Testability,log,login,1529,"local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3837,Usability,simpl,simple,3837,(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; </det,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/issues/5178:3907,Usability,simpl,simple,3907,ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; </details>. A workaround is setting up a registry to host the images (so we can,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178
https://github.com/broadinstitute/cromwell/pull/5180:15,Modifiability,config,config,15,"This PR adds a config option for user defined retries. With `memory-retry` the user can specify an array of strings which when encountered in the `stderr` file by Cromwell, allows the task to be retried with a factor also mentioned in the config. JIRA issue [link](https://broadworkbench.atlassian.net/browse/BA-5933).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180
https://github.com/broadinstitute/cromwell/pull/5180:239,Modifiability,config,config,239,"This PR adds a config option for user defined retries. With `memory-retry` the user can specify an array of strings which when encountered in the `stderr` file by Cromwell, allows the task to be retried with a factor also mentioned in the config. JIRA issue [link](https://broadworkbench.atlassian.net/browse/BA-5933).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180
https://github.com/broadinstitute/cromwell/issues/5182:276,Availability,echo,echo,276,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:317,Availability,echo,echo,317,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:403,Availability,echo,echo,403,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:466,Availability,echo,echo,466,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:534,Availability,echo,echo,534,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:684,Availability,echo,echo,684,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:698,Availability,echo,echo,698,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:706,Availability,echo,echo,706,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:818,Availability,echo,echo,818,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:832,Availability,echo,echo,832,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:840,Availability,echo,echo,840,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:888,Availability,failure,failure,888,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/issues/5182:57,Testability,log,logic,57,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182
https://github.com/broadinstitute/cromwell/pull/5184:91,Energy Efficiency,schedul,scheduling,91,"Volcano is a batch system running high performance workloads on Kubernetes, which provides scheduling, job management, and other mechanisms k8s missed. This is an example configure for cromwell to run on volcano clusters.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184
https://github.com/broadinstitute/cromwell/pull/5184:171,Modifiability,config,configure,171,"Volcano is a batch system running high performance workloads on Kubernetes, which provides scheduling, job management, and other mechanisms k8s missed. This is an example configure for cromwell to run on volcano clusters.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184
https://github.com/broadinstitute/cromwell/pull/5184:39,Performance,perform,performance,39,"Volcano is a batch system running high performance workloads on Kubernetes, which provides scheduling, job management, and other mechanisms k8s missed. This is an example configure for cromwell to run on volcano clusters.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184
https://github.com/broadinstitute/cromwell/pull/5194:18,Availability,avail,available,18,Makes the IoActor available to the carbonite worker and readers by allowing the service registry to receive and store its reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5194
https://github.com/broadinstitute/cromwell/pull/5196:43,Deployability,canary,canary,43,"Not terribly convinced of the value of the canary test when compared to the interesting five dollar genome case, but put here as the OKR asked for it. Also note that the original canary was a submission of 30k workflows, not a scatter. I modified it to fit in a single workflow run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196
https://github.com/broadinstitute/cromwell/pull/5196:179,Deployability,canary,canary,179,"Not terribly convinced of the value of the canary test when compared to the interesting five dollar genome case, but put here as the OKR asked for it. Also note that the original canary was a submission of 30k workflows, not a scatter. I modified it to fit in a single workflow run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196
https://github.com/broadinstitute/cromwell/pull/5196:50,Testability,test,test,50,"Not terribly convinced of the value of the canary test when compared to the interesting five dollar genome case, but put here as the OKR asked for it. Also note that the original canary was a submission of 30k workflows, not a scatter. I modified it to fit in a single workflow run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196
https://github.com/broadinstitute/cromwell/pull/5202:82,Deployability,hotfix,hotfixes,82,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:2,Modifiability,Refactor,Refactor,2,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:27,Modifiability,variab,variables,27,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:119,Modifiability,variab,variables,119,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:75,Safety,Detect,Detect,75,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:201,Testability,test,tests,201,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:286,Testability,log,log,286,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:549,Testability,test,test,549,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5202:593,Testability,test,tests,593,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202
https://github.com/broadinstitute/cromwell/pull/5203:208,Security,validat,validation,208,Pull request for #BA-5929 issue:; https://broadworkbench.atlassian.net/browse/BA-5929. Changes:; Implemented store larger values in WomFloat type via changing Double implementation to BigDecimal.; Also added validation logic of type compatibility.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203
https://github.com/broadinstitute/cromwell/pull/5203:219,Testability,log,logic,219,Pull request for #BA-5929 issue:; https://broadworkbench.atlassian.net/browse/BA-5929. Changes:; Implemented store larger values in WomFloat type via changing Double implementation to BigDecimal.; Also added validation logic of type compatibility.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203
https://github.com/broadinstitute/cromwell/issues/5204:154,Availability,ERROR,ERROR,154,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:422,Availability,ERROR,ERROR,422,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:473,Availability,error,error,473,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:1779,Deployability,configurat,configuration,1779,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:1779,Modifiability,config,configuration,1779,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:682,Performance,cache,cache-results,682,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:172,Security,hash,hash,172,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:468,Security,Hash,Hash,468,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:1824,Security,PASSWORD,PASSWORDS,1824,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/issues/5204:989,Usability,feedback,feedback,989,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204
https://github.com/broadinstitute/cromwell/pull/5205:214,Integrability,Depend,Depending,214,"Implements HybridMetadataReadDecider logic from https://docs.google.com/document/d/1VYnzk97yTtllozO9ivZpZQTwrsY5T0wGqxlvAbrEQgg/edit#bookmark=id.mwkzc5gtae81. * Asks classic metadata service for archive status ; * Depending on the response, forwards the original query to either the classic or carbonite metadata service",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5205
https://github.com/broadinstitute/cromwell/pull/5205:37,Testability,log,logic,37,"Implements HybridMetadataReadDecider logic from https://docs.google.com/document/d/1VYnzk97yTtllozO9ivZpZQTwrsY5T0wGqxlvAbrEQgg/edit#bookmark=id.mwkzc5gtae81. * Asks classic metadata service for archive status ; * Depending on the response, forwards the original query to either the classic or carbonite metadata service",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5205
https://github.com/broadinstitute/cromwell/pull/5208:22,Testability,test,test,22,- [x] Still TODO: FSM test cases,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5208
https://github.com/broadinstitute/cromwell/issues/5213:304,Deployability,update,updated,304,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/issues/5213:448,Deployability,update,updated,448,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/issues/5213:30,Modifiability,config,config,30,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/issues/5213:124,Performance,load,loaded,124,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/issues/5213:155,Performance,cache,cache,155,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/issues/5213:414,Performance,cache,cache,414,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/issues/5213:404,Usability,clear,clear,404,"This question may be a single config option but I have yet to find any information on it. Currently, when a CWL workflow is loaded cromwell seems to use a cache of previous cwl files used by the workflow when they are requested from http. Is there a way to prevent this behavior if the workflow has been updated but has the same name and requirements? We have had cases where we need to stop cromwell to clear the cache and force it redownload the updated workflow files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5213
https://github.com/broadinstitute/cromwell/pull/5216:51,Availability,recover,recoverAsync,51,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216
https://github.com/broadinstitute/cromwell/pull/5216:51,Safety,recover,recoverAsync,51,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216
https://github.com/broadinstitute/cromwell/pull/5216:113,Testability,test,tested,113,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216
https://github.com/broadinstitute/cromwell/pull/5217:85,Availability,error,errors,85,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217
https://github.com/broadinstitute/cromwell/pull/5217:11,Modifiability,config,config,11,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217
https://github.com/broadinstitute/cromwell/pull/5217:121,Modifiability,config,config,121,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217
https://github.com/broadinstitute/cromwell/pull/5217:161,Modifiability,variab,variable,161,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217
https://github.com/broadinstitute/cromwell/pull/5217:0,Security,Validat,Validation,0,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217
https://github.com/broadinstitute/cromwell/pull/5217:18,Security,validat,validated,18,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217
https://github.com/broadinstitute/cromwell/issues/5218:1132,Modifiability,config,config,1132,"We are running joint-discovery-gatk4 in slurm with Cromwell 46. . Only a small fraction of the shards are being processed concurrently. What adjustments can be made to process more or all shards concurrently?. Running joint-discovery-gatk4 with 50 GVCFs completed successfully in 12 hours. . Running joint-discovery-gatk4 with 100 GVCFs has been running for over 3 days. Given that we plan to scale this to thousands of samples, this will lead to unacceptable runtimes. . This is the WDL that is being used: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl). Intervals are the same as those found here: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json). Test WGS GVCF files (average size = 9G) were generated via HaplotypeCaller 3.5-0 with the parameter emitRefConfidence=GVCF. . In the cromwell config file, concurrent-job-limit is set to 1000. To verify that it is functioning, it was set to 2 and 2 concurrent cromwell jobs were observed. . Call caching on or off as well as adjusting batch_size did not impact this issue. . All other throttling settings are set to default settings. . Thank you for your help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218
https://github.com/broadinstitute/cromwell/issues/5218:122,Performance,concurren,concurrently,122,"We are running joint-discovery-gatk4 in slurm with Cromwell 46. . Only a small fraction of the shards are being processed concurrently. What adjustments can be made to process more or all shards concurrently?. Running joint-discovery-gatk4 with 50 GVCFs completed successfully in 12 hours. . Running joint-discovery-gatk4 with 100 GVCFs has been running for over 3 days. Given that we plan to scale this to thousands of samples, this will lead to unacceptable runtimes. . This is the WDL that is being used: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl). Intervals are the same as those found here: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json). Test WGS GVCF files (average size = 9G) were generated via HaplotypeCaller 3.5-0 with the parameter emitRefConfidence=GVCF. . In the cromwell config file, concurrent-job-limit is set to 1000. To verify that it is functioning, it was set to 2 and 2 concurrent cromwell jobs were observed. . Call caching on or off as well as adjusting batch_size did not impact this issue. . All other throttling settings are set to default settings. . Thank you for your help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218
https://github.com/broadinstitute/cromwell/issues/5218:195,Performance,concurren,concurrently,195,"We are running joint-discovery-gatk4 in slurm with Cromwell 46. . Only a small fraction of the shards are being processed concurrently. What adjustments can be made to process more or all shards concurrently?. Running joint-discovery-gatk4 with 50 GVCFs completed successfully in 12 hours. . Running joint-discovery-gatk4 with 100 GVCFs has been running for over 3 days. Given that we plan to scale this to thousands of samples, this will lead to unacceptable runtimes. . This is the WDL that is being used: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl). Intervals are the same as those found here: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json). Test WGS GVCF files (average size = 9G) were generated via HaplotypeCaller 3.5-0 with the parameter emitRefConfidence=GVCF. . In the cromwell config file, concurrent-job-limit is set to 1000. To verify that it is functioning, it was set to 2 and 2 concurrent cromwell jobs were observed. . Call caching on or off as well as adjusting batch_size did not impact this issue. . All other throttling settings are set to default settings. . Thank you for your help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218
https://github.com/broadinstitute/cromwell/issues/5218:1145,Performance,concurren,concurrent-job-limit,1145,"We are running joint-discovery-gatk4 in slurm with Cromwell 46. . Only a small fraction of the shards are being processed concurrently. What adjustments can be made to process more or all shards concurrently?. Running joint-discovery-gatk4 with 50 GVCFs completed successfully in 12 hours. . Running joint-discovery-gatk4 with 100 GVCFs has been running for over 3 days. Given that we plan to scale this to thousands of samples, this will lead to unacceptable runtimes. . This is the WDL that is being used: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl). Intervals are the same as those found here: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json). Test WGS GVCF files (average size = 9G) were generated via HaplotypeCaller 3.5-0 with the parameter emitRefConfidence=GVCF. . In the cromwell config file, concurrent-job-limit is set to 1000. To verify that it is functioning, it was set to 2 and 2 concurrent cromwell jobs were observed. . Call caching on or off as well as adjusting batch_size did not impact this issue. . All other throttling settings are set to default settings. . Thank you for your help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218
https://github.com/broadinstitute/cromwell/issues/5218:1238,Performance,concurren,concurrent,1238,"We are running joint-discovery-gatk4 in slurm with Cromwell 46. . Only a small fraction of the shards are being processed concurrently. What adjustments can be made to process more or all shards concurrently?. Running joint-discovery-gatk4 with 50 GVCFs completed successfully in 12 hours. . Running joint-discovery-gatk4 with 100 GVCFs has been running for over 3 days. Given that we plan to scale this to thousands of samples, this will lead to unacceptable runtimes. . This is the WDL that is being used: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl). Intervals are the same as those found here: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json). Test WGS GVCF files (average size = 9G) were generated via HaplotypeCaller 3.5-0 with the parameter emitRefConfidence=GVCF. . In the cromwell config file, concurrent-job-limit is set to 1000. To verify that it is functioning, it was set to 2 and 2 concurrent cromwell jobs were observed. . Call caching on or off as well as adjusting batch_size did not impact this issue. . All other throttling settings are set to default settings. . Thank you for your help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218
https://github.com/broadinstitute/cromwell/issues/5218:990,Testability,Test,Test,990,"We are running joint-discovery-gatk4 in slurm with Cromwell 46. . Only a small fraction of the shards are being processed concurrently. What adjustments can be made to process more or all shards concurrently?. Running joint-discovery-gatk4 with 50 GVCFs completed successfully in 12 hours. . Running joint-discovery-gatk4 with 100 GVCFs has been running for over 3 days. Given that we plan to scale this to thousands of samples, this will lead to unacceptable runtimes. . This is the WDL that is being used: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl). Intervals are the same as those found here: [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json). Test WGS GVCF files (average size = 9G) were generated via HaplotypeCaller 3.5-0 with the parameter emitRefConfidence=GVCF. . In the cromwell config file, concurrent-job-limit is set to 1000. To verify that it is functioning, it was set to 2 and 2 concurrent cromwell jobs were observed. . Call caching on or off as well as adjusting batch_size did not impact this issue. . All other throttling settings are set to default settings. . Thank you for your help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218
https://github.com/broadinstitute/cromwell/pull/5220:37,Deployability,release,released,37,"Rather than having very long ""Worker released"" events which don't mean very much, re-introduce ""Cromwell Polling Interval"" to fill the gap between ""GCS is done"" and ""Cromwell notices it"". ![Screen Shot 2019-10-09 at 5 02 36 PM](https://user-images.githubusercontent.com/13006282/66520194-b6015980-eab6-11e9-887c-7ee27f288d3b.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220
https://github.com/broadinstitute/cromwell/issues/5221:201,Deployability,pipeline,pipeline,201,"Apologies if this has been addressed elsewhere but I'm not entirely sure where my problem is originating so no amount of Googling i've done today has given me any clue how to fix this. I'm running the pipeline ""processing-for-variant-discovery-gatk4.wdl"" on a local machine. I've left everything in the wdl file as it came from github, and the only thing i've changed in the input file is references to a local unmapped bam, and local reference files. edit: I'm using cromwell-39.jar. The problem is (i think) Cromwell keeps nesting the execution folder and copying over all inputs and references over and over again until the drive ultimately gets filled and the process crashes. An example is here in a screenshot:; ![repeated_cromwell_nesting](https://user-images.githubusercontent.com/1724546/66525112-4afd5600-eaa9-11e9-8995-f8b8005710f1.png). I'm really at a loss to what's happening and it's driving me insane -- any help is greatly appreciated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221
https://github.com/broadinstitute/cromwell/pull/5222:25,Testability,test,testing,25,Suggestions for a better testing approach welcome.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5222
https://github.com/broadinstitute/cromwell/pull/5223:95,Testability,test,testing,95,"Please ignore the giant metadata blob, that was generated from the nested workflows purely for testing purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5223
https://github.com/broadinstitute/cromwell/pull/5225:51,Testability,test,tests,51,While poking at #5216 I found that several Centaur tests that had been excluded for AWS are now passing. 🎉,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5225
https://github.com/broadinstitute/cromwell/issues/5226:368,Deployability,integrat,integration,368,"Most of the example aws.conf file for AWSBatch does not actually provide a working example of a s3 ARN, they all seems to expect the user to specify <provide-your-s3-arn>. https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/. It would be helpful to provide actual working example as AWS support is new and most of us are just learning how to setup for AWS integration for Cromwell. Cheers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5226
https://github.com/broadinstitute/cromwell/issues/5226:368,Integrability,integrat,integration,368,"Most of the example aws.conf file for AWSBatch does not actually provide a working example of a s3 ARN, they all seems to expect the user to specify <provide-your-s3-arn>. https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/. It would be helpful to provide actual working example as AWS support is new and most of us are just learning how to setup for AWS integration for Cromwell. Cheers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5226
https://github.com/broadinstitute/cromwell/issues/5226:338,Usability,learn,learning,338,"Most of the example aws.conf file for AWSBatch does not actually provide a working example of a s3 ARN, they all seems to expect the user to specify <provide-your-s3-arn>. https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/. It would be helpful to provide actual working example as AWS support is new and most of us are just learning how to setup for AWS integration for Cromwell. Cheers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5226
https://github.com/broadinstitute/cromwell/pull/5228:324,Availability,echo,echo,324,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228
https://github.com/broadinstitute/cromwell/pull/5228:389,Availability,echo,echo,389,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228
https://github.com/broadinstitute/cromwell/pull/5228:633,Availability,echo,echo,633,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228
https://github.com/broadinstitute/cromwell/pull/5228:847,Availability,echo,echo,847,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228
https://github.com/broadinstitute/cromwell/pull/5228:989,Availability,echo,echo,989,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228
https://github.com/broadinstitute/cromwell/pull/5228:142,Energy Efficiency,monitor,monitoring,142,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228
https://github.com/broadinstitute/cromwell/pull/5229:20,Modifiability,config,config,20,"For any AWS backend config using temporary credentials (fargate, assume-roles, ec2 instances?) - obtaining the actual credentials needs to be delayed as long as possible so that long running tasks will use the providers ability to refresh. This PR just converts AwsAuthModes to hand out AwsCredentialProviders rather than AwsCredentials. . All the AWS Java SDK builders take credential providers as the basis for their client anyhow (builder.credentialsProvider(xxx)) . Unresolved in this PR is the use of 'options' as a parameter to the auth modes credentials() call - I could not work out what they were intended for. In general they were just passed in as (_ => """") but there may be some loss of functionality?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5229
https://github.com/broadinstitute/cromwell/pull/5230:124,Availability,failure,failure,124,"Immutable Docker hash request keys to guard against credential mutations breaking lookups, more graceful handling of actual failure cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5230
https://github.com/broadinstitute/cromwell/pull/5230:17,Security,hash,hash,17,"Immutable Docker hash request keys to guard against credential mutations breaking lookups, more graceful handling of actual failure cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5230
https://github.com/broadinstitute/cromwell/issues/5235:492,Deployability,integrat,integrating,492,"I'm specifically thinking of an environment variable - I don't see anything that looks like it might meet these criteria. The goal would be to alter the program behavior slightly depending on whether it is run in a workflow (in which case additional output may be helpful) or on its own (in which case the user probably doesn't want random machine-readable output files appearing in the run dir). (And yes, I am volunteering to add this if it does not exist, because it's a major obstacle to integrating Cromwell with other systems.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235
https://github.com/broadinstitute/cromwell/issues/5235:179,Integrability,depend,depending,179,"I'm specifically thinking of an environment variable - I don't see anything that looks like it might meet these criteria. The goal would be to alter the program behavior slightly depending on whether it is run in a workflow (in which case additional output may be helpful) or on its own (in which case the user probably doesn't want random machine-readable output files appearing in the run dir). (And yes, I am volunteering to add this if it does not exist, because it's a major obstacle to integrating Cromwell with other systems.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235
https://github.com/broadinstitute/cromwell/issues/5235:492,Integrability,integrat,integrating,492,"I'm specifically thinking of an environment variable - I don't see anything that looks like it might meet these criteria. The goal would be to alter the program behavior slightly depending on whether it is run in a workflow (in which case additional output may be helpful) or on its own (in which case the user probably doesn't want random machine-readable output files appearing in the run dir). (And yes, I am volunteering to add this if it does not exist, because it's a major obstacle to integrating Cromwell with other systems.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235
https://github.com/broadinstitute/cromwell/issues/5235:44,Modifiability,variab,variable,44,"I'm specifically thinking of an environment variable - I don't see anything that looks like it might meet these criteria. The goal would be to alter the program behavior slightly depending on whether it is run in a workflow (in which case additional output may be helpful) or on its own (in which case the user probably doesn't want random machine-readable output files appearing in the run dir). (And yes, I am volunteering to add this if it does not exist, because it's a major obstacle to integrating Cromwell with other systems.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235
https://github.com/broadinstitute/cromwell/pull/5236:17,Security,validat,validation,17,…n occurs during validation of command line arguments [BA-4061]. [BA-4061]: https://broadworkbench.atlassian.net/browse/BA-4061,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236
https://github.com/broadinstitute/cromwell/pull/5237:34,Availability,failure,failure,34,For workflow-success and workflow-failure type tests:. * Wait for the query results for the workflow to indicate carbonite complete; * Ensure that the metadata is as valid afterwards as it was beforehand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237
https://github.com/broadinstitute/cromwell/pull/5237:47,Testability,test,tests,47,For workflow-success and workflow-failure type tests:. * Wait for the query results for the workflow to indicate carbonite complete; * Ensure that the metadata is as valid afterwards as it was beforehand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237
https://github.com/broadinstitute/cromwell/pull/5241:45,Testability,test,test,45,Proposing this change to the docker deadlock test since it progressed past the database connection issues 8/8 times in https://travis-ci.com/broadinstitute/cromwell/builds/132908257,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5241
https://github.com/broadinstitute/cromwell/issues/5246:370,Modifiability,config,config,370,"Hi, we allow users to run jobs with different workflow options and our users would like to know what options were used for given workflow run. I was searching a bit but I cannot find how to retrieve workflow options for given run. I can retrieve files that were uploaded (that's fine) from metadata but I would need also options listed if there were defaults set in the config file. Eg. user can change `read_from_cache=true` but if she does not supply any file the default option would not be listed in the metadata (nor in the `WORKFLOW_STORE_ENTRY` as far as I could check.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246
https://github.com/broadinstitute/cromwell/pull/5247:150,Testability,test,tests,150,Completely punted on unexpanding subworkflows and call-level queries. The latter shortcoming surprisingly appears to at least one of the causes of 19 tests still failing in Chris' #5237 PR. But the basic metadata endpoint with include / exclude does appear to work on a Carboniting Cromwell.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5247
https://github.com/broadinstitute/cromwell/pull/5250:114,Deployability,pipeline,pipeline,114,"I ran into the same issue as [#3876](https://github.com/broadinstitute/cromwell/issues/3876) running the chip-seq pipeline on beegfs, the workflow would stall looking for outputs after any individual task. I was able to resolve this by changing globLinkCommand to use soft instead of hard links, though not sure if soft links will cause problems in other contexts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250
https://github.com/broadinstitute/cromwell/pull/5252:36,Testability,test,tests,36,Fixes a couple of Carbonite Centaur tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5252
https://github.com/broadinstitute/cromwell/issues/5256:562,Availability,error,error,562,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256
https://github.com/broadinstitute/cromwell/issues/5256:628,Availability,ERROR,ERROR,628,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256
https://github.com/broadinstitute/cromwell/issues/5256:19,Deployability,pipeline,pipeline,19,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256
https://github.com/broadinstitute/cromwell/issues/5256:218,Deployability,pipeline,pipeline,218,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256
https://github.com/broadinstitute/cromwell/issues/5256:40,Security,validat,validation,40,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256
https://github.com/broadinstitute/cromwell/issues/5256:608,Security,validat,validate,608,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256
https://github.com/broadinstitute/cromwell/pull/5260:34,Availability,error,error,34,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:169,Availability,error,error,169,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:371,Availability,error,errors,371,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:689,Availability,error,error,689,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:761,Availability,error,errors,761,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:22,Deployability,Update,Updates,22,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:1411,Deployability,UPDATE,UPDATE,1411,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:1432,Deployability,rolling,rolling,1432,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:1484,Deployability,rolling,rolling,1484,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:40,Integrability,message,message,40,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:179,Integrability,message,message,179,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:699,Integrability,message,message,699,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5260:77,Performance,cache,cache,77,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260
https://github.com/broadinstitute/cromwell/pull/5262:314,Availability,error,errors,314,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262
https://github.com/broadinstitute/cromwell/pull/5262:2,Deployability,Upgrade,Upgrade,2,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262
https://github.com/broadinstitute/cromwell/pull/5262:250,Deployability,configurat,configuration,250,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262
https://github.com/broadinstitute/cromwell/pull/5262:250,Modifiability,config,configuration,250,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262
https://github.com/broadinstitute/cromwell/pull/5270:519,Deployability,update,updates,519,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:464,Integrability,message,message,464,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:664,Integrability,message,message,664,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1101,Integrability,message,message,1101,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:220,Modifiability,config,config,220,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:289,Modifiability,config,config,289,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:829,Modifiability,config,configured,829,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:894,Modifiability,config,config,894,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1117,Modifiability,config,config,1117,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1190,Modifiability,config,config,1190,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:241,Testability,test,tested,241,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:608,Testability,test,test,608,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:980,Testability,test,test,980,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:998,Testability,test,test,998,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1204,Testability,test,test,1204,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1276,Testability,test,test,1276,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1293,Testability,test,test,1293,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/pull/5270:1364,Testability,test,test,1364,"The TES backend previously hardcoded the value of preemptible to false [[BA-6004](https://broadworkbench.atlassian.net/browse/BA-6004)]. This change reads the value in from either the backend runtime or workflow runtime config. . I manually tested that the setting gets picked up from the config (workflow runtime setting of preemptible successfully overrides backend runtime setting; absence of preemptible setting uses default of false) and gets put in the task message sent to the TES API. . I also made a few minor updates to the TES docs to describe preemptible and fix a couple broken links. . A quick test was run as follows to view the TES task submission message sent by Cromwell:. - In one window, listen on port 5000 and display anything sent to it. `ncat -l 5000 -k`. - In another window, submit workflow to Cromwell configured to talk to TES on port 5000 . `java -Dconfig.file=app.config -jar /c/git/cromwell/server/target/scala-2.12/cromwell-48-366e431-SNAP.jar run test.wdl --inputs test.json; `; - Check that proper `""resources"":{""preemptible"":<value>}}` field is contained in the TES message. . [app.config.txt](https://github.com/broadinstitute/cromwell/files/3822302/app.config.txt); [test.json.txt](https://github.com/broadinstitute/cromwell/files/3822303/test.json.txt); [test.wdl.txt](https://github.com/broadinstitute/cromwell/files/3822304/test.wdl.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270
https://github.com/broadinstitute/cromwell/issues/5271:338,Availability,avail,available,338,"Hi,. Is it possible to use an SQLite database instead of a MySQL server to allow for call-caching?. This post comes close to saying there isn't a way without a lot of work (https://github.com/broadinstitute/cromwell/issues/3786). Is this still the position?. I know you can use a MySQL database, but sometimes the extra resources are not available or difficult to get. An option for a local persistence database would be very useful. Thanks for your help,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271
https://github.com/broadinstitute/cromwell/pull/5272:24,Availability,failure,failures,24,Should address the cron failures by setting the maximum freeze scan interval to be less than the Carbonite-flavored Centaur's limit of patience.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5272
https://github.com/broadinstitute/cromwell/pull/5273:222,Deployability,Pipeline,Pipeline,222,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5273:254,Deployability,pipeline,pipeline,254,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5273:288,Deployability,pipeline,pipelines,288,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5273:371,Modifiability,config,configured,371,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5273:5,Safety,timeout,timeout,5,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5273:57,Safety,timeout,timeout,57,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5273:268,Safety,abort,abort,268,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273
https://github.com/broadinstitute/cromwell/pull/5274:476,Energy Efficiency,monitor,monitoring,476,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274
https://github.com/broadinstitute/cromwell/pull/5274:949,Integrability,depend,depend,949,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274
https://github.com/broadinstitute/cromwell/pull/5274:28,Testability,log,logic,28,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274
https://github.com/broadinstitute/cromwell/pull/5274:487,Testability,log,log,487,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274
https://github.com/broadinstitute/cromwell/pull/5276:34,Energy Efficiency,monitor,monitoringTerminationAction,34,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276
https://github.com/broadinstitute/cromwell/pull/5276:141,Integrability,depend,dependency,141,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276
https://github.com/broadinstitute/cromwell/pull/5277:3,Safety,predict,predicted,3,"As predicted in the ticket, simply removing the swagger `default` allows this value to be exercised correctly from the UI.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5277
https://github.com/broadinstitute/cromwell/pull/5277:28,Usability,simpl,simply,28,"As predicted in the ticket, simply removing the swagger `default` allows this value to be exercised correctly from the UI.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5277
https://github.com/broadinstitute/cromwell/issues/5280:423,Deployability,configurat,configuration,423,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:2849,Deployability,configurat,configuration,2849," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:1107,Integrability,wrap,wraps,1107,"currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:1136,Integrability,wrap,wraps,1136,"currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:423,Modifiability,config,configuration,423,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:779,Modifiability,config,configure,779,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:2849,Modifiability,config,configuration,2849," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:492,Performance,cache,cache-results,492,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:663,Performance,cache,cache,663,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:1604,Performance,Cache,Cache,1604,"ults = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:2894,Security,PASSWORD,PASSWORDS,2894," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:622,Usability,clear,clear,622,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/issues/5280:2059,Usability,feedback,feedback,2059," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280
https://github.com/broadinstitute/cromwell/pull/5282:34,Energy Efficiency,monitor,monitoringTerminationAction,34,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5276",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5282
https://github.com/broadinstitute/cromwell/pull/5282:141,Integrability,depend,dependency,141,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5276",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5282
https://github.com/broadinstitute/cromwell/pull/5283:476,Energy Efficiency,monitor,monitoring,476,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5283
https://github.com/broadinstitute/cromwell/pull/5283:949,Integrability,depend,depend,949,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5283
https://github.com/broadinstitute/cromwell/pull/5283:28,Testability,log,logic,28,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5283
https://github.com/broadinstitute/cromwell/pull/5283:487,Testability,log,log,487,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5283
https://github.com/broadinstitute/cromwell/issues/5285:2490,Availability,error,error,2490,"1.splicegraph"": ""<absolute_path_to_file2>""; }. ```; I tried with and without the ""runtime"" spec block (local run with voila on system path) with the same result. . In the cromwell log as it runs I see it says it runs this command:; ```; voila tsv \ ; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/test.psi.voila \; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/splicegraph.sql \; -f something.tsv; ```. However my program (voila) seems to somehow be receiving an argument for a different file (?) which it can not find:. ```; usage: voila tsv [-h] -f FILE_NAME [--threshold THRESHOLD]; [--non-changing-threshold NON_CHANGING_THRESHOLD]; [--probability-threshold PROBABILITY_THRESHOLD] [--show-all]; [--lsv-types-file LSV_TYPES]; [--lsv-types [LSV_TYPES [LSV_TYPES ...]]]; [--lsv-ids-file LSV_IDS] [--lsv-ids [LSV_IDS [LSV_IDS ...]]]; [--gene-names-file GENE_NAMES]; [--gene-names [GENE_NAMES [GENE_NAMES ...]]]; [--gene-ids-file GENE_IDS]; [--gene-ids [GENE_IDS [GENE_IDS ...]]] [-j NPROC] [--debug]; [-l LOGGER] [--silent]; files [files ...]; ```; voila tsv: error: argument files: cannot find ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/ "". The input files exist correctly at the path /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs . However, it appears that somehow a third path is being specified as an argument with the path ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/"" ? I don't understand where this is coming from but I am new at the language so most likely it is just a noob mistake. . Can anyone let me know why my program might be receiving the extra argument that causes it to crash?. Os: ubuntu 18.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285
https://github.com/broadinstitute/cromwell/issues/5285:1448,Testability,log,log,1448,"un format:. `$ docker run majiqvoila voila -f output_file.tsv inputfile1.voila inputfile2.sql`; or; `$ docker run majiqvoila voila inputfile1.voila inputfile2.sql -f outputfile.tsv`. I define a WDL as follows:; ```; workflow myWorkflow1 {. ; File voila_file; File splicegraph. call task_voila_tsv{; input: voila_file=voila_file, splicegraph=splicegraph; }; }. task task_voila_tsv{. File voila_file; File splicegraph; ; command {; voila tsv \ ; ${splicegraph} \; ${voila_file} \; -f something.tsv ; }; ; output {; File out = ""something.tsv""; }; ; runtime {; docker: ""pjewellbiociphers/majiq:latest""; }. }; ```. ; ; and run it with some input file like:; ```; {; ""myWorkflow1.voila_file"": ""<absolute_path_to_file1>"",; ""myWorkflow1.splicegraph"": ""<absolute_path_to_file2>""; }. ```; I tried with and without the ""runtime"" spec block (local run with voila on system path) with the same result. . In the cromwell log as it runs I see it says it runs this command:; ```; voila tsv \ ; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/test.psi.voila \; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/splicegraph.sql \; -f something.tsv; ```. However my program (voila) seems to somehow be receiving an argument for a different file (?) which it can not find:. ```; usage: voila tsv [-h] -f FILE_NAME [--threshold THRESHOLD]; [--non-changing-threshold NON_CHANGING_THRESHOLD]; [--probability-threshold PROBABILITY_THRESHOLD] [--show-all]; [--lsv-types-file LSV_TYPES]; [--lsv-types [LSV_TYPES [LSV_TYPES ...]]]; [--lsv-ids-file LSV_IDS] [--lsv-ids [LSV_IDS [LSV_IDS ...]]]; [--gene-names-file GENE_NAMES]; [--gene-names [GENE_NAMES [GENE_NAMES ...]]]; [--gene-ids-file GENE_IDS]; [--gene-ids [GENE_IDS [GENE_IDS ...]]] [-j NPROC] [--debug]; [-l LOGGER] [--silent]; files [files ...]; ```; voila tsv: error: argument files: cannot find ""/home/pjewell/w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285
https://github.com/broadinstitute/cromwell/issues/5285:1646,Testability,test,test,1646,"un format:. `$ docker run majiqvoila voila -f output_file.tsv inputfile1.voila inputfile2.sql`; or; `$ docker run majiqvoila voila inputfile1.voila inputfile2.sql -f outputfile.tsv`. I define a WDL as follows:; ```; workflow myWorkflow1 {. ; File voila_file; File splicegraph. call task_voila_tsv{; input: voila_file=voila_file, splicegraph=splicegraph; }; }. task task_voila_tsv{. File voila_file; File splicegraph; ; command {; voila tsv \ ; ${splicegraph} \; ${voila_file} \; -f something.tsv ; }; ; output {; File out = ""something.tsv""; }; ; runtime {; docker: ""pjewellbiociphers/majiq:latest""; }. }; ```. ; ; and run it with some input file like:; ```; {; ""myWorkflow1.voila_file"": ""<absolute_path_to_file1>"",; ""myWorkflow1.splicegraph"": ""<absolute_path_to_file2>""; }. ```; I tried with and without the ""runtime"" spec block (local run with voila on system path) with the same result. . In the cromwell log as it runs I see it says it runs this command:; ```; voila tsv \ ; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/test.psi.voila \; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/splicegraph.sql \; -f something.tsv; ```. However my program (voila) seems to somehow be receiving an argument for a different file (?) which it can not find:. ```; usage: voila tsv [-h] -f FILE_NAME [--threshold THRESHOLD]; [--non-changing-threshold NON_CHANGING_THRESHOLD]; [--probability-threshold PROBABILITY_THRESHOLD] [--show-all]; [--lsv-types-file LSV_TYPES]; [--lsv-types [LSV_TYPES [LSV_TYPES ...]]]; [--lsv-ids-file LSV_IDS] [--lsv-ids [LSV_IDS [LSV_IDS ...]]]; [--gene-names-file GENE_NAMES]; [--gene-names [GENE_NAMES [GENE_NAMES ...]]]; [--gene-ids-file GENE_IDS]; [--gene-ids [GENE_IDS [GENE_IDS ...]]] [-j NPROC] [--debug]; [-l LOGGER] [--silent]; files [files ...]; ```; voila tsv: error: argument files: cannot find ""/home/pjewell/w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285
https://github.com/broadinstitute/cromwell/issues/5285:2435,Testability,LOG,LOGGER,2435,"1.splicegraph"": ""<absolute_path_to_file2>""; }. ```; I tried with and without the ""runtime"" spec block (local run with voila on system path) with the same result. . In the cromwell log as it runs I see it says it runs this command:; ```; voila tsv \ ; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/test.psi.voila \; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/splicegraph.sql \; -f something.tsv; ```. However my program (voila) seems to somehow be receiving an argument for a different file (?) which it can not find:. ```; usage: voila tsv [-h] -f FILE_NAME [--threshold THRESHOLD]; [--non-changing-threshold NON_CHANGING_THRESHOLD]; [--probability-threshold PROBABILITY_THRESHOLD] [--show-all]; [--lsv-types-file LSV_TYPES]; [--lsv-types [LSV_TYPES [LSV_TYPES ...]]]; [--lsv-ids-file LSV_IDS] [--lsv-ids [LSV_IDS [LSV_IDS ...]]]; [--gene-names-file GENE_NAMES]; [--gene-names [GENE_NAMES [GENE_NAMES ...]]]; [--gene-ids-file GENE_IDS]; [--gene-ids [GENE_IDS [GENE_IDS ...]]] [-j NPROC] [--debug]; [-l LOGGER] [--silent]; files [files ...]; ```; voila tsv: error: argument files: cannot find ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/ "". The input files exist correctly at the path /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs . However, it appears that somehow a third path is being specified as an argument with the path ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/"" ? I don't understand where this is coming from but I am new at the language so most likely it is just a noob mistake. . Can anyone let me know why my program might be receiving the extra argument that causes it to crash?. Os: ubuntu 18.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285
https://github.com/broadinstitute/cromwell/pull/5286:34,Energy Efficiency,monitor,monitoringTerminationAction,34,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5282 and #5276",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5286
https://github.com/broadinstitute/cromwell/pull/5286:141,Integrability,depend,dependency,141,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5282 and #5276",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5286
https://github.com/broadinstitute/cromwell/pull/5287:34,Energy Efficiency,monitor,monitoringTerminationAction,34,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5276, #5282 and #5286, to fix Travis flakiness.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287
https://github.com/broadinstitute/cromwell/pull/5287:141,Integrability,depend,dependency,141,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5276, #5282 and #5286, to fix Travis flakiness.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287
https://github.com/broadinstitute/cromwell/pull/5288:476,Energy Efficiency,monitor,monitoring,476,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274 and #5283 (to overcome Travis hurdles..)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288
https://github.com/broadinstitute/cromwell/pull/5288:949,Integrability,depend,depend,949,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274 and #5283 (to overcome Travis hurdles..)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288
https://github.com/broadinstitute/cromwell/pull/5288:28,Testability,log,logic,28,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274 and #5283 (to overcome Travis hurdles..)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288
https://github.com/broadinstitute/cromwell/pull/5288:487,Testability,log,log,487,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274 and #5283 (to overcome Travis hurdles..)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288
https://github.com/broadinstitute/cromwell/pull/5293:16,Testability,test,test,16,This passes the test from Chris' branch with some limited (though perhaps alarming) changes. However this PR and #5292 have shed light on nearly complete lack of semantic understanding of Cromwell metadata in the Carbonite JSON editing code. i.e. I can imagine valid workflows that would completely break the Carbonited metadata endpoints and will likely require a significant rethink of how the JSON editor works with metadata.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5293
https://github.com/broadinstitute/cromwell/pull/5295:14,Security,expose,exposed,14,Adding a test exposed some previously unknown bugs 😬 which should be fixed now.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5295
https://github.com/broadinstitute/cromwell/pull/5295:9,Testability,test,test,9,Adding a test exposed some previously unknown bugs 😬 which should be fixed now.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5295
https://github.com/broadinstitute/cromwell/pull/5302:247,Performance,perform,performance,247,The approach here is to accumulate root and subworkflow outputs as they are produced. This approach addresses the below problems from [first approach](https://github.com/broadinstitute/cromwell/pull/5294) which talked to the database:. - possible performance issue arising due to running filter query on `METADATA_KEY` and `METADATA_VALUE` columns to find subworkflow ids; - the scenario where the `DeleteWorkflowFilesActor` is gathering outputs from metadata table but the metadata events containing output details has not been published to the table yet ; - output is a string containing a file path: This gets eliminated during filtering because such a value is stored as `WomString` instead of `WomSingleFile`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5302
https://github.com/broadinstitute/cromwell/pull/5303:198,Testability,log,logic,198,"Kind of related to BA-6087 (though maybe not as much as I originally thought when I started on this), only unexpand subworkflows in the root workflow. i.e. don't piggyback on the depth-first search logic used for label upserting as that may end up doing a lot of unnecessary unexpansions in lower-level subworkflows whose parents are later unexpanded.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5303
https://github.com/broadinstitute/cromwell/pull/5304:26,Testability,log,logic,26,Rendered file: [job retry logic doc](https://github.com/broadinstitute/cromwell/blob/a5926453aaf94ee73167f99c54ebe21e064f6e92/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5304
https://github.com/broadinstitute/cromwell/issues/5305:283,Availability,error,error,283,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:134,Deployability,configurat,configuration,134,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:477,Deployability,configurat,configuration,477,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:112,Energy Efficiency,schedul,scheduler,112,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:134,Modifiability,config,configuration,134,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:477,Modifiability,config,configuration,477,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:380,Performance,load,load,380,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/issues/5305:442,Performance,concurren,concurrent-job-limit,442,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305
https://github.com/broadinstitute/cromwell/pull/5306:6,Modifiability,variab,variables,6,Unset variables and -o nounset do not mix.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5306
https://github.com/broadinstitute/cromwell/pull/5309:228,Availability,Error,ErrorOr,228,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. 🙂 . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309
https://github.com/broadinstitute/cromwell/pull/5309:237,Security,validat,validation,237,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. 🙂 . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309
https://github.com/broadinstitute/cromwell/pull/5309:314,Testability,test,test,314,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. 🙂 . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309
https://github.com/broadinstitute/cromwell/pull/5309:478,Testability,test,tests,478,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. 🙂 . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309
https://github.com/broadinstitute/cromwell/issues/5311:71,Availability,error,error,71,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311
https://github.com/broadinstitute/cromwell/issues/5311:399,Availability,error,error,399,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311
https://github.com/broadinstitute/cromwell/issues/5311:28,Deployability,deploy,deploy,28,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311
https://github.com/broadinstitute/cromwell/issues/5311:366,Deployability,deploy,deploy,366,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311
https://github.com/broadinstitute/cromwell/issues/5311:570,Deployability,deploy,deploy,570,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311
https://github.com/broadinstitute/cromwell/pull/5312:288,Deployability,release,release,288,"Hello all, . This fixes BA-6144; EDIT: also fixes #4969. At LUMC we are currently creating [wdl-packager](https://github.com/biowdl/wdl-packager/tree/init). This program uses miniwdl to determine any file based imports in a WDL file and package them into a zip. This was done to make the release and deployment of BioWDL pipelines less unwieldy. Also this provides ready-made zip files for people who want to run BioWDL on a cromwell server. When testing I discovered that [Cromwell does not properly unpack zip files](https://broadworkbench.atlassian.net/projects/BA/issues/BA-6144). Zips with nested directories can not be unpacked. This is due to a bug in better-files 2.17.1 that was fixed in 3.0.0 and higher versions. In this PR the version of better-files is updated to 3.8.0 and any incompatibilities should be fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312
https://github.com/broadinstitute/cromwell/pull/5312:300,Deployability,deploy,deployment,300,"Hello all, . This fixes BA-6144; EDIT: also fixes #4969. At LUMC we are currently creating [wdl-packager](https://github.com/biowdl/wdl-packager/tree/init). This program uses miniwdl to determine any file based imports in a WDL file and package them into a zip. This was done to make the release and deployment of BioWDL pipelines less unwieldy. Also this provides ready-made zip files for people who want to run BioWDL on a cromwell server. When testing I discovered that [Cromwell does not properly unpack zip files](https://broadworkbench.atlassian.net/projects/BA/issues/BA-6144). Zips with nested directories can not be unpacked. This is due to a bug in better-files 2.17.1 that was fixed in 3.0.0 and higher versions. In this PR the version of better-files is updated to 3.8.0 and any incompatibilities should be fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312
https://github.com/broadinstitute/cromwell/pull/5312:321,Deployability,pipeline,pipelines,321,"Hello all, . This fixes BA-6144; EDIT: also fixes #4969. At LUMC we are currently creating [wdl-packager](https://github.com/biowdl/wdl-packager/tree/init). This program uses miniwdl to determine any file based imports in a WDL file and package them into a zip. This was done to make the release and deployment of BioWDL pipelines less unwieldy. Also this provides ready-made zip files for people who want to run BioWDL on a cromwell server. When testing I discovered that [Cromwell does not properly unpack zip files](https://broadworkbench.atlassian.net/projects/BA/issues/BA-6144). Zips with nested directories can not be unpacked. This is due to a bug in better-files 2.17.1 that was fixed in 3.0.0 and higher versions. In this PR the version of better-files is updated to 3.8.0 and any incompatibilities should be fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312
https://github.com/broadinstitute/cromwell/pull/5312:766,Deployability,update,updated,766,"Hello all, . This fixes BA-6144; EDIT: also fixes #4969. At LUMC we are currently creating [wdl-packager](https://github.com/biowdl/wdl-packager/tree/init). This program uses miniwdl to determine any file based imports in a WDL file and package them into a zip. This was done to make the release and deployment of BioWDL pipelines less unwieldy. Also this provides ready-made zip files for people who want to run BioWDL on a cromwell server. When testing I discovered that [Cromwell does not properly unpack zip files](https://broadworkbench.atlassian.net/projects/BA/issues/BA-6144). Zips with nested directories can not be unpacked. This is due to a bug in better-files 2.17.1 that was fixed in 3.0.0 and higher versions. In this PR the version of better-files is updated to 3.8.0 and any incompatibilities should be fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312
https://github.com/broadinstitute/cromwell/pull/5312:447,Testability,test,testing,447,"Hello all, . This fixes BA-6144; EDIT: also fixes #4969. At LUMC we are currently creating [wdl-packager](https://github.com/biowdl/wdl-packager/tree/init). This program uses miniwdl to determine any file based imports in a WDL file and package them into a zip. This was done to make the release and deployment of BioWDL pipelines less unwieldy. Also this provides ready-made zip files for people who want to run BioWDL on a cromwell server. When testing I discovered that [Cromwell does not properly unpack zip files](https://broadworkbench.atlassian.net/projects/BA/issues/BA-6144). Zips with nested directories can not be unpacked. This is due to a bug in better-files 2.17.1 that was fixed in 3.0.0 and higher versions. In this PR the version of better-files is updated to 3.8.0 and any incompatibilities should be fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312
https://github.com/broadinstitute/cromwell/pull/5317:908,Availability,avail,available,908,"Support for namespaced inputs was dropped from 48 as mentioned in BA-6149. . The code was reworked in #5214 which disallowed namespaced inputs. There was one line of code that actively prohibited namespaced inputs. Removing this line allows namespaced inputs again. The namespaces are a bit different from the ones in 47, so I documented it in the changelog. Also I wrote some documentation on the inputs as I couldn't find any on the [latest development documentation](https://cromwell.readthedocs.io/develop/). . I like the newer namespaces much better than the old ones. A great job! This PR makes sure the fruit of this effort can be plucked by the pipeline developers. If womtool inputs needs to give a cleaner output, then maybe we can change a few other things. Changing it in the inputs parsing is not desirable IMO because that also affects cromwell. In an ideal world all the namespaced inputs are available for advanced pipeline users in cromwell, while not cluttering the womtool inputs output (unless a flag is set).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317
https://github.com/broadinstitute/cromwell/pull/5317:653,Deployability,pipeline,pipeline,653,"Support for namespaced inputs was dropped from 48 as mentioned in BA-6149. . The code was reworked in #5214 which disallowed namespaced inputs. There was one line of code that actively prohibited namespaced inputs. Removing this line allows namespaced inputs again. The namespaces are a bit different from the ones in 47, so I documented it in the changelog. Also I wrote some documentation on the inputs as I couldn't find any on the [latest development documentation](https://cromwell.readthedocs.io/develop/). . I like the newer namespaces much better than the old ones. A great job! This PR makes sure the fruit of this effort can be plucked by the pipeline developers. If womtool inputs needs to give a cleaner output, then maybe we can change a few other things. Changing it in the inputs parsing is not desirable IMO because that also affects cromwell. In an ideal world all the namespaced inputs are available for advanced pipeline users in cromwell, while not cluttering the womtool inputs output (unless a flag is set).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317
https://github.com/broadinstitute/cromwell/pull/5317:931,Deployability,pipeline,pipeline,931,"Support for namespaced inputs was dropped from 48 as mentioned in BA-6149. . The code was reworked in #5214 which disallowed namespaced inputs. There was one line of code that actively prohibited namespaced inputs. Removing this line allows namespaced inputs again. The namespaces are a bit different from the ones in 47, so I documented it in the changelog. Also I wrote some documentation on the inputs as I couldn't find any on the [latest development documentation](https://cromwell.readthedocs.io/develop/). . I like the newer namespaces much better than the old ones. A great job! This PR makes sure the fruit of this effort can be plucked by the pipeline developers. If womtool inputs needs to give a cleaner output, then maybe we can change a few other things. Changing it in the inputs parsing is not desirable IMO because that also affects cromwell. In an ideal world all the namespaced inputs are available for advanced pipeline users in cromwell, while not cluttering the womtool inputs output (unless a flag is set).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317
https://github.com/broadinstitute/cromwell/pull/5318:43,Deployability,update,updates,43,"Carrying forwards only the ""womtool graph"" updates from #4522, allowing (in theory) WDL 1.0 and CWL graphs. Thorough review would be extremely welcome - this was originally part of a hackathon project so the quality of the code might be... somewhat wanting.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5318
https://github.com/broadinstitute/cromwell/pull/5320:202,Energy Efficiency,efficient,efficient,202,"Sorry for the PR bombardment. I came across issue #5271 and noticed this was an issue we also had, and have partially solved already. This is the documentation of our solution. It is not as elegant and efficient as SQLite, but it gets the job done. I have heard of a center that has a separate cromwell server (with database) running **for each user** to get around filesystem permissions and privacy/access concerns. This is a bit unwieldy to say the least. . A file-based database solves these problems by allowing each user to run `cromwell `on the command line and automatically creating a file-based database that is tied to the filesystem permissions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5320
https://github.com/broadinstitute/cromwell/pull/5320:401,Security,access,access,401,"Sorry for the PR bombardment. I came across issue #5271 and noticed this was an issue we also had, and have partially solved already. This is the documentation of our solution. It is not as elegant and efficient as SQLite, but it gets the job done. I have heard of a center that has a separate cromwell server (with database) running **for each user** to get around filesystem permissions and privacy/access concerns. This is a bit unwieldy to say the least. . A file-based database solves these problems by allowing each user to run `cromwell `on the command line and automatically creating a file-based database that is tied to the filesystem permissions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5320
https://github.com/broadinstitute/cromwell/pull/5323:104,Modifiability,Config,Configuring,104,Apparantly mkdocs does not like the `+` sign for list items. https://cromwell.readthedocs.io/en/develop/Configuring/#database. So this fixes that.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5323
https://github.com/broadinstitute/cromwell/pull/5324:5,Testability,test,test,5,more test ideas welcome!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5324
https://github.com/broadinstitute/cromwell/issues/5329:131,Availability,error,error,131,"The docker attribute specified in the ""default_runtime_attributes"" in cromwell.conf does not get picked up. My workflow fails with error: . ```; 2019-12-19 03:04:56,497 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowManagerActor Workflow 759c0000-c343-4466-a26b-aa627785; 89b0 failed (during InitializingWorkflowState): Task gcpp has an invalid runtime attribute docker = !! NOT FOUND !!; ```; The relevant portion of cromwell.conf; ```; computecfg_00 {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""xxxxxx""; auth = ""default""; default-runtime-attributes {; queueArn = ""xxxxxx""; docker = ""ubuntu:latest""; }; ...; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5329
https://github.com/broadinstitute/cromwell/issues/5329:559,Modifiability,config,config,559,"The docker attribute specified in the ""default_runtime_attributes"" in cromwell.conf does not get picked up. My workflow fails with error: . ```; 2019-12-19 03:04:56,497 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowManagerActor Workflow 759c0000-c343-4466-a26b-aa627785; 89b0 failed (during InitializingWorkflowState): Task gcpp has an invalid runtime attribute docker = !! NOT FOUND !!; ```; The relevant portion of cromwell.conf; ```; computecfg_00 {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""xxxxxx""; auth = ""default""; default-runtime-attributes {; queueArn = ""xxxxxx""; docker = ""ubuntu:latest""; }; ...; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5329
https://github.com/broadinstitute/cromwell/issues/5329:692,Performance,queue,queueArn,692,"The docker attribute specified in the ""default_runtime_attributes"" in cromwell.conf does not get picked up. My workflow fails with error: . ```; 2019-12-19 03:04:56,497 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowManagerActor Workflow 759c0000-c343-4466-a26b-aa627785; 89b0 failed (during InitializingWorkflowState): Task gcpp has an invalid runtime attribute docker = !! NOT FOUND !!; ```; The relevant portion of cromwell.conf; ```; computecfg_00 {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""xxxxxx""; auth = ""default""; default-runtime-attributes {; queueArn = ""xxxxxx""; docker = ""ubuntu:latest""; }; ...; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5329
https://github.com/broadinstitute/cromwell/pull/5332:90,Deployability,release,release,90,"See [BA-3823](https://broadworkbench.atlassian.net/browse/BA-6065) for bug details.; See [release notes](https://github.com/slick/slick/releases) for slick 3.3.0. The proof of the ""does it fix it"" pudding will be in the eating, but unless tests break, I don't see why we shouldn't update the version regardless. UPDATE: Tests did break. The ""inspired by a github comment"" upsert code was having issues, and wasn't obviously fixable. I replaced it with what seems like a far more canonical way of doing the same upserts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332
https://github.com/broadinstitute/cromwell/pull/5332:136,Deployability,release,releases,136,"See [BA-3823](https://broadworkbench.atlassian.net/browse/BA-6065) for bug details.; See [release notes](https://github.com/slick/slick/releases) for slick 3.3.0. The proof of the ""does it fix it"" pudding will be in the eating, but unless tests break, I don't see why we shouldn't update the version regardless. UPDATE: Tests did break. The ""inspired by a github comment"" upsert code was having issues, and wasn't obviously fixable. I replaced it with what seems like a far more canonical way of doing the same upserts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332
https://github.com/broadinstitute/cromwell/pull/5332:281,Deployability,update,update,281,"See [BA-3823](https://broadworkbench.atlassian.net/browse/BA-6065) for bug details.; See [release notes](https://github.com/slick/slick/releases) for slick 3.3.0. The proof of the ""does it fix it"" pudding will be in the eating, but unless tests break, I don't see why we shouldn't update the version regardless. UPDATE: Tests did break. The ""inspired by a github comment"" upsert code was having issues, and wasn't obviously fixable. I replaced it with what seems like a far more canonical way of doing the same upserts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332
https://github.com/broadinstitute/cromwell/pull/5332:312,Deployability,UPDATE,UPDATE,312,"See [BA-3823](https://broadworkbench.atlassian.net/browse/BA-6065) for bug details.; See [release notes](https://github.com/slick/slick/releases) for slick 3.3.0. The proof of the ""does it fix it"" pudding will be in the eating, but unless tests break, I don't see why we shouldn't update the version regardless. UPDATE: Tests did break. The ""inspired by a github comment"" upsert code was having issues, and wasn't obviously fixable. I replaced it with what seems like a far more canonical way of doing the same upserts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332
https://github.com/broadinstitute/cromwell/pull/5332:239,Testability,test,tests,239,"See [BA-3823](https://broadworkbench.atlassian.net/browse/BA-6065) for bug details.; See [release notes](https://github.com/slick/slick/releases) for slick 3.3.0. The proof of the ""does it fix it"" pudding will be in the eating, but unless tests break, I don't see why we shouldn't update the version regardless. UPDATE: Tests did break. The ""inspired by a github comment"" upsert code was having issues, and wasn't obviously fixable. I replaced it with what seems like a far more canonical way of doing the same upserts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332
https://github.com/broadinstitute/cromwell/pull/5332:320,Testability,Test,Tests,320,"See [BA-3823](https://broadworkbench.atlassian.net/browse/BA-6065) for bug details.; See [release notes](https://github.com/slick/slick/releases) for slick 3.3.0. The proof of the ""does it fix it"" pudding will be in the eating, but unless tests break, I don't see why we shouldn't update the version regardless. UPDATE: Tests did break. The ""inspired by a github comment"" upsert code was having issues, and wasn't obviously fixable. I replaced it with what seems like a far more canonical way of doing the same upserts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332
https://github.com/broadinstitute/cromwell/pull/5333:64,Security,hash,hashes,64,"Travis doesn't seem to work quite right with PRs with identical hashes matching the external contributors. Scriptify the experience of creating or updating a PR branch, and salting it so that the hash is different (and so that Travis will pick it up correctly).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5333
https://github.com/broadinstitute/cromwell/pull/5333:196,Security,hash,hash,196,"Travis doesn't seem to work quite right with PRs with identical hashes matching the external contributors. Scriptify the experience of creating or updating a PR branch, and salting it so that the hash is different (and so that Travis will pick it up correctly).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5333
https://github.com/broadinstitute/cromwell/issues/5334:228,Modifiability,Config,Configure,228,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:272,Modifiability,config,config,272,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:339,Modifiability,config,config,339,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:346,Modifiability,Config,ConfigBackendLifecycleActorFactory,346,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:383,Modifiability,config,config,383,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:597,Performance,concurren,concurrent,597,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:615,Performance,concurren,concurrent-job-limit,615,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:1488,Performance,cache,cache,1488,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/issues/5334:1508,Performance,cache,cache,1508,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334
https://github.com/broadinstitute/cromwell/pull/5340:59,Testability,Log,Logging,59,This almost certainly does not fix the underlying problem. Logging lines are added at the beginning and end of the localization script so we can determine if the hanging occurs in the final `gsutil` of the script or after that.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340
https://github.com/broadinstitute/cromwell/pull/5342:74,Energy Efficiency,monitor,monitoring,74,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5342:113,Energy Efficiency,monitor,monitor,113,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5342:206,Energy Efficiency,monitor,monitoring,206,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5342:329,Energy Efficiency,monitor,monitoring,329,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5342:371,Energy Efficiency,monitor,monitoring,371,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5342:923,Energy Efficiency,monitor,monitoring,923,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5342:587,Testability,log,logic,587,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342
https://github.com/broadinstitute/cromwell/pull/5343:407,Deployability,pipeline,pipelines,407,By default all cromwell task containers submitted by Google Cloud backend doesn't allow user to mount any filesystems within a container. It happens because containers are launched without specific linux capabilities being enabled. . Nevertheless filesystem mounts can be of help in some workflows because it doesn't require all the task resources to be localized or to be embedded in docker images. Google pipelines api allows to set `ENABLE_FUSE` flag for all submitted action. Once specified it forces Google pipelines engine to launch action containers with additional linux capabilities such as `CAP_SYS_ADMIN` being enabled. The pull request adds support for launching cromwell task containers with an enabled support for fuses in Google Cloud. Fuses support can be enabled via a workflow option `enable_fuse` or via a Google Cloud backend configuration attribute `backend.providers.Papiv2.config.genomics.enable-fuse`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343
https://github.com/broadinstitute/cromwell/pull/5343:512,Deployability,pipeline,pipelines,512,By default all cromwell task containers submitted by Google Cloud backend doesn't allow user to mount any filesystems within a container. It happens because containers are launched without specific linux capabilities being enabled. . Nevertheless filesystem mounts can be of help in some workflows because it doesn't require all the task resources to be localized or to be embedded in docker images. Google pipelines api allows to set `ENABLE_FUSE` flag for all submitted action. Once specified it forces Google pipelines engine to launch action containers with additional linux capabilities such as `CAP_SYS_ADMIN` being enabled. The pull request adds support for launching cromwell task containers with an enabled support for fuses in Google Cloud. Fuses support can be enabled via a workflow option `enable_fuse` or via a Google Cloud backend configuration attribute `backend.providers.Papiv2.config.genomics.enable-fuse`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343
https://github.com/broadinstitute/cromwell/pull/5343:846,Deployability,configurat,configuration,846,By default all cromwell task containers submitted by Google Cloud backend doesn't allow user to mount any filesystems within a container. It happens because containers are launched without specific linux capabilities being enabled. . Nevertheless filesystem mounts can be of help in some workflows because it doesn't require all the task resources to be localized or to be embedded in docker images. Google pipelines api allows to set `ENABLE_FUSE` flag for all submitted action. Once specified it forces Google pipelines engine to launch action containers with additional linux capabilities such as `CAP_SYS_ADMIN` being enabled. The pull request adds support for launching cromwell task containers with an enabled support for fuses in Google Cloud. Fuses support can be enabled via a workflow option `enable_fuse` or via a Google Cloud backend configuration attribute `backend.providers.Papiv2.config.genomics.enable-fuse`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343
https://github.com/broadinstitute/cromwell/pull/5343:846,Modifiability,config,configuration,846,By default all cromwell task containers submitted by Google Cloud backend doesn't allow user to mount any filesystems within a container. It happens because containers are launched without specific linux capabilities being enabled. . Nevertheless filesystem mounts can be of help in some workflows because it doesn't require all the task resources to be localized or to be embedded in docker images. Google pipelines api allows to set `ENABLE_FUSE` flag for all submitted action. Once specified it forces Google pipelines engine to launch action containers with additional linux capabilities such as `CAP_SYS_ADMIN` being enabled. The pull request adds support for launching cromwell task containers with an enabled support for fuses in Google Cloud. Fuses support can be enabled via a workflow option `enable_fuse` or via a Google Cloud backend configuration attribute `backend.providers.Papiv2.config.genomics.enable-fuse`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343
https://github.com/broadinstitute/cromwell/pull/5343:896,Modifiability,config,config,896,By default all cromwell task containers submitted by Google Cloud backend doesn't allow user to mount any filesystems within a container. It happens because containers are launched without specific linux capabilities being enabled. . Nevertheless filesystem mounts can be of help in some workflows because it doesn't require all the task resources to be localized or to be embedded in docker images. Google pipelines api allows to set `ENABLE_FUSE` flag for all submitted action. Once specified it forces Google pipelines engine to launch action containers with additional linux capabilities such as `CAP_SYS_ADMIN` being enabled. The pull request adds support for launching cromwell task containers with an enabled support for fuses in Google Cloud. Fuses support can be enabled via a workflow option `enable_fuse` or via a Google Cloud backend configuration attribute `backend.providers.Papiv2.config.genomics.enable-fuse`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343
https://github.com/broadinstitute/cromwell/pull/5344:131,Availability,error,error,131,"Following up on https://github.com/broadinstitute/cromwell/pull/5321, this is another case when GCS IoActor fails with a retryable error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344
https://github.com/broadinstitute/cromwell/issues/5345:528,Availability,error,error,528,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. I'm trying to run the workflow below. There is a working version for a single sample (https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl), but I want to run the workflow on many samples concurrently. My attempt at creating a workflow to do so failed with the following error:. ```; Failed to process scatter block (reason 1 of 1): No conversion defined for Ast with name Outputs to WorkflowGraphElement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:2019,Availability,toler,tolerate,2019,"dinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ## max_retries: how many times to retry failed tasks -- very important on the cloud when there are transient errors; ## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file to be used instead of the GATK 4 jar; ## in the docker image. This must be supplied when running in an environment that does not support docker; ## (e.g. SGE cluster on a Broad on-prem VM); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over intervals; ## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional); ## split_intervals_extra_args: additional arguments for splitting intervals before scattering (optional); ## run_orientation_bias_mixture_model_filter: (optional) if true, filter orientation bias sites with the read orientation artifact mixture model.; ##; ## ** Primary inputs **; ## ref",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:2196,Availability,error,errors,2196,"dinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ## max_retries: how many times to retry failed tasks -- very important on the cloud when there are transient errors; ## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file to be used instead of the GATK 4 jar; ## in the docker image. This must be supplied when running in an environment that does not support docker; ## (e.g. SGE cluster on a Broad on-prem VM); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over intervals; ## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional); ## split_intervals_extra_args: additional arguments for splitting intervals before scattering (optional); ## run_orientation_bias_mixture_model_filter: (optional) if true, filter orientation bias sites with the read orientation artifact mixture model.; ##; ## ** Primary inputs **; ## ref",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:3583,Availability,down,downloads,3583,"filtering (optional); ## split_intervals_extra_args: additional arguments for splitting intervals before scattering (optional); ## run_orientation_bias_mixture_model_filter: (optional) if true, filter orientation bias sites with the read orientation artifact mixture model.; ##; ## ** Primary inputs **; ## ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index: BAM and index for the tumor sample; ## normal_bam, normal_bam_index: BAM and index for the normal sample; ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_idx: optional panel of normals (and its index) in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_idx: optional database of known germline variants (and its index) (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_idx: VCF of common variants (and its index)with allele frequencies for calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## realignment_index_bundle: resource for FilterAlignmentArtifacts, which runs if and only if it is specified. Generated by BwaMemIndexImageCreator.; ##; ## Funcotator parameters (see Funcotator help for more details).; ## funco_reference_version: ""hg19"" for hg19 or b37. ""hg38"" for hg38. Default: ""hg19""; ## funco_output_format: ""MAF"" to produce a MAF file, ""VCF"" to procude a VCF file. Default: ""MAF""; ## funco_compress: (Only valid if funco_output_format == ""VCF"" ) If true, will compress the output of Funcotator. If false, produces an uncompressed output file. Default: false; ## funco_use_gnomad_AF: If true, will include gnomAD allele frequency annotations in output by connecting to the internet to query gnomAD (this impacts performance). If false, will not annotate with gnomAD. Default: false; ## funco_transcript_selection_mode: How to select transcripts in Funcotator. ALL, CANONICAL, or BEST_EFFECT; ## funco_transcript_selectio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:9983,Availability,down,downloads,9983,"; Int max_retries_or_default = select_first([max_retries, 2]). Boolean compress = select_first([compress_vcfs, false]); Boolean run_ob_filter = select_first([run_orientation_bias_mixture_model_filter, false]); Boolean make_bamout_or_default = select_first([make_bamout, false]); Boolean run_funcotator_or_default = select_first([run_funcotator, false]); Boolean filter_funcotations_or_default = select_first([filter_funcotations, true]). # Disk sizes used for dynamic sizing; Int ref_size = ceil(size(ref_fasta, ""GB"") + size(ref_dict, ""GB"") + size(ref_fai, ""GB"")); Int tumor_reads_size = ceil(size(tumor_reads, ""GB"") + size(tumor_reads_index, ""GB"")); Int gnomad_vcf_size = if defined(gnomad) then ceil(size(gnomad, ""GB"")) else 0; Int normal_reads_size = if defined(normal_reads) then ceil(size(normal_reads, ""GB"") + size(normal_reads_index, ""GB"")) else 0. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = if defined(funco_data_sources_tar_gz) then ceil(size(funco_data_sources_tar_gz, ""GB"") * 3) else 100; Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, ""GB"")) else 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size + select_first([emergency_extra_disk,0]). # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = basename(basename(tumor_reads, "".bam""),"".cram"") #hacky way to strip either .bam or .cram; String unfiltered_name = output_basename + ""-unfiltered""; String filtered_name = output_basename + ""-filtered""; String funcotated_name = output_basename + ""-funcotated"". String output_vcf_name = output_basename + "".vcf"". Int tumor_cram_to_bam_disk = ceil(tumor_reads_size * cram_to_bam_multiplier); Int normal_cram_to_bam_disk = ceil(normal_reads_size * cram_to_bam_multiplier). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:19569,Availability,failure,failure,19569," File filtered_vcf_idx = select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx]); File filtering_stats = Filter.filtering_stats; File mutect_stats = MergeStats.merged_stats; File? contamination_table = CalculateContamination.contamination_table. File? funcotated_file = Funcotate.funcotated_output_file; File? funcotated_file_index = Funcotate.funcotated_output_file_index; File? bamout = MergeBamOuts.merged_bam_out; File? bamout_index = MergeBamOuts.merged_bam_out_index; File? maf_segments = CalculateContamination.maf_segments; File? read_orientation_model_params = LearnReadOrientationModel.artifact_prior_table; }. }; }. task CramToBam {; input {; File ref_fasta; File ref_fai; File ref_dict; #cram and crai must be optional since Normal cram is optional; File? cram; File? crai; String name; Int disk_size; Int? mem; }. Int machine_mem = if defined(mem) then mem * 1000 else 6000. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools view -h -T ~{ref_fasta} ~{cram} |; samtools view -b -o ~{name}.bam -; samtools index -b ~{name}.bam; mv ~{name}.bam.bai ~{name}.bai; }. runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai""; }; }. task SplitIntervals {; input {; File? intervals; File ref_fasta; File ref_fai; File ref_dict; Int scatter_count; String? split_intervals_extra_args. # runtime; Runtime runtime_params; }. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. mkdir interval-files; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" SplitIntervals \; -R ~{ref_fasta} \; ~{""-L "" + intervals} \; -scatter ~{scatter_count} \; -O interval-files \; ~{split_intervals_extra_args}; cp interval-files/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:22738,Availability,echo,echo,22738,"if defined(mem) then mem * 1000 else 3500; Int command_mem = machine_mem - 500. parameter_meta{; intervals: {localization_optional: true}; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; tumor_bam: {localization_optional: true}; tumor_bai: {localization_optional: true}; normal_bam: {localization_optional: true}; normal_bai: {localization_optional: true}; pon: {localization_optional: true}; pon_idx: {localization_optional: true}; gnomad: {localization_optional: true}; gnomad_idx: {localization_optional: true}; gga_vcf: {localization_optional: true}; gga_vcf_idx: {localization_optional: true}; variants_for_contamination: {localization_optional: true}; variants_for_contamination_idx: {localization_optional: true}; }. command <<<; set -e. export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" gatk_override}. # We need to create these files regardless, even if they stay empty; touch bamout.bam; touch f1r2.tar.gz; echo """" > normal_name.txt. gatk --java-options ""-Xmx~{command_mem}m"" GetSampleName -R ~{ref_fasta} -I ~{tumor_bam} -O tumor_name.txt -encode; tumor_command_line=""-I ~{tumor_bam} -tumor `cat tumor_name.txt`"". if [[ ! -z ""~{normal_bam}"" ]]; then; gatk --java-options ""-Xmx~{command_mem}m"" GetSampleName -R ~{ref_fasta} -I ~{normal_bam} -O normal_name.txt -encode; normal_command_line=""-I ~{normal_bam} -normal `cat normal_name.txt`""; fi. gatk --java-options ""-Xmx~{command_mem}m"" Mutect2 \; -R ~{ref_fasta} \; $tumor_command_line \; $normal_command_line \; ~{""--germline-resource "" + gnomad} \; ~{""-pon "" + pon} \; ~{""-L "" + intervals} \; ~{""--alleles "" + gga_vcf} \; -O ""~{output_vcf}"" \; ~{true='--bam-output bamout.bam' false='' make_bamout} \; ~{true='--f1r2-tar-gz f1r2.tar.gz' false='' run_ob_filter} \; ~{m2_extra_args}. ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:36449,Availability,echo,echo,36449,"n_list) then "" --transcript-list "" else """"; String annotation_def_arg = if defined(annotation_defaults) then "" --annotation-default "" else """"; String annotation_over_arg = if defined(annotation_overrides) then "" --annotation-override "" else """"; String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then "" --remove-filtered-variants "" else """"; String excluded_fields_args = if defined(funcotator_excluded_fields) then "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:36676,Availability,echo,echo,36676,"--annotation-override "" else """"; String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then "" --remove-filtered-variants "" else """"; String excluded_fields_args = if defined(funcotator_excluded_fields) then "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:36948,Availability,echo,echo,36948," "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:36954,Availability,ERROR,ERROR,36954," "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:6020,Deployability,release,released,6020,"ults: Default values for annotations, when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funco_annotation_overrides: Values for annotations, even when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true, will only annotate variants that have passed filtering (. or PASS value in the FILTER column). If false, will annotate all variants in the input file. Default: true; ## funcotator_extra_args: Any additional arguments to pass to Funcotator. Default: """"; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested; a bamout.bam; ## file of reassembled reads if requested; ##; ## Cromwell version support; ## - Successfully tested on v34; ##; ## LICENSING :; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information; ## pertaining to the included programs. struct Runtime {; String gatk_docker; File? gatk_override; Int max_retries; Int preemptible; Int cpu; Int machine_mem; Int command_mem; Int disk; Int boot_disk_size; }. workflow Mutect2 {; input {; # Mutect2 inputs; File? intervals; File ref_fasta; File ref_fai; File ref_dict; File file_tumor_reads; Array[File] all_tumor_reads = read_lines(file_tumor_reads); File file_tumor_reads_indexes; Array[File] all_tumor_reads_indexes = read_lines(file_tumor_reads_indexes); Array[Pair[File,File]] tumor_reads_and_in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:33908,Deployability,update,updated,33908,"ptional: true}; input_vcf_idx: {localization_optional: true}; bam: {localization_optional: true}; bai: {localization_optional: true}; }. command {; set -e. export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{command_mem}m"" FilterAlignmentArtifacts \; -V ~{input_vcf} \; -I ~{bam} \; --bwa-mem-index-image ~{realignment_index_bundle} \; ~{realignment_extra_args} \; -O ~{output_vcf}; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File filtered_vcf = ""~{output_vcf}""; File filtered_vcf_idx = ""~{output_vcf_idx}""; }; }. task Funcotate {; input {; File ref_fasta; File ref_fai; File ref_dict; File input_vcf; File input_vcf_idx; String reference_version; String output_file_base_name; String output_format; Boolean compress; Boolean use_gnomad; # This should be updated when a new version of the data sources is released; # TODO: Make this dynamically chosen in the command.; File? data_sources_tar_gz = ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz""; String? control_id; String? case_id; String? sequencing_center; String? sequence_source; String? transcript_selection_mode; File? transcript_selection_list; Array[String]? annotation_defaults; Array[String]? annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? filter_funcotations; File? interval_list. String? extra_args. # ==============; Runtime runtime_params; Int? disk_space #override to request more disk than default small task params. # You may have to change the following two parameter values depending on the task requirements; Int default_ram_mb = 3000; # WARNING: In the workflow, you should calculate the disk space as an input to this task (disk_space_gb). Please see [T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:33958,Deployability,release,released,33958,"ptional: true}; input_vcf_idx: {localization_optional: true}; bam: {localization_optional: true}; bai: {localization_optional: true}; }. command {; set -e. export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{command_mem}m"" FilterAlignmentArtifacts \; -V ~{input_vcf} \; -I ~{bam} \; --bwa-mem-index-image ~{realignment_index_bundle} \; ~{realignment_extra_args} \; -O ~{output_vcf}; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File filtered_vcf = ""~{output_vcf}""; File filtered_vcf_idx = ""~{output_vcf_idx}""; }; }. task Funcotate {; input {; File ref_fasta; File ref_fai; File ref_dict; File input_vcf; File input_vcf_idx; String reference_version; String output_file_base_name; String output_format; Boolean compress; Boolean use_gnomad; # This should be updated when a new version of the data sources is released; # TODO: Make this dynamically chosen in the command.; File? data_sources_tar_gz = ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz""; String? control_id; String? case_id; String? sequencing_center; String? sequence_source; String? transcript_selection_mode; File? transcript_selection_list; Array[String]? annotation_defaults; Array[String]? annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? filter_funcotations; File? interval_list. String? extra_args. # ==============; Runtime runtime_params; Int? disk_space #override to request more disk than default small task params. # You may have to change the following two parameter values depending on the task requirements; Int default_ram_mb = 3000; # WARNING: In the workflow, you should calculate the disk space as an input to this task (disk_space_gb). Please see [T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:38766,Deployability,configurat,configuration,38766,"~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation_over_arg}~{default="""" sep="" --annotation-override "" annotation_overrides} \; ~{excluded_fields_args}~{default="""" sep="" --exclude-field "" funcotator_excluded_fields} \; ~{filter_funcotations_args} \; ~{extra_args_arg}; # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:; if [[ ""~{output_format}"" == ""MAF"" ]] ; then; touch ~{output_maf_index}; fi; >>>. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + select_first([disk_space, runtime_params.disk]) + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File funcotated_output_file = ""~{output_file}""; File funcotated_output_file_index = ""~{output_file_index}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:34661,Integrability,depend,depending,34661," ref_fai; File ref_dict; File input_vcf; File input_vcf_idx; String reference_version; String output_file_base_name; String output_format; Boolean compress; Boolean use_gnomad; # This should be updated when a new version of the data sources is released; # TODO: Make this dynamically chosen in the command.; File? data_sources_tar_gz = ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz""; String? control_id; String? case_id; String? sequencing_center; String? sequence_source; String? transcript_selection_mode; File? transcript_selection_list; Array[String]? annotation_defaults; Array[String]? annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? filter_funcotations; File? interval_list. String? extra_args. # ==============; Runtime runtime_params; Int? disk_space #override to request more disk than default small task params. # You may have to change the following two parameter values depending on the task requirements; Int default_ram_mb = 3000; # WARNING: In the workflow, you should calculate the disk space as an input to this task (disk_space_gb). Please see [TODO: Link from Jose] for examples.; Int default_disk_space_gb = 100; }. # ==============; # Process input args:; String output_maf = output_file_base_name + "".maf""; String output_maf_index = output_maf + "".idx""; String output_vcf = output_file_base_name + if compress then "".vcf.gz"" else "".vcf""; String output_vcf_idx = output_vcf + if compress then "".tbi"" else "".idx""; String output_file = if output_format == ""MAF"" then output_maf else output_vcf; String output_file_index = if output_format == ""MAF"" then output_maf_index else output_vcf_idx; String transcript_selection_arg = if defined(transcript_selection_list) then "" --transcript-list "" else """"; String annotation_def_arg = if defined(annotation_defaults) then "" --annotation-default "" else """"; String annotation_over_arg = if defined(annotation_overrides) then "" --annotation-override "" else """"; String filter_fun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:38766,Modifiability,config,configuration,38766,"~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation_over_arg}~{default="""" sep="" --annotation-override "" annotation_overrides} \; ~{excluded_fields_args}~{default="""" sep="" --exclude-field "" funcotator_excluded_fields} \; ~{filter_funcotations_args} \; ~{extra_args_arg}; # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:; if [[ ""~{output_format}"" == ""MAF"" ]] ; then; touch ~{output_maf_index}; fi; >>>. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + select_first([disk_space, runtime_params.disk]) + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File funcotated_output_file = ""~{output_file}""; File funcotated_output_file_index = ""~{output_file_index}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:445,Performance,concurren,concurrently,445,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. I'm trying to run the workflow below. There is a working version for a single sample (https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl), but I want to run the workflow on many samples concurrently. My attempt at creating a workflow to do so failed with the following error:. ```; Failed to process scatter block (reason 1 of 1): No conversion defined for Ast with name Outputs to WorkflowGraphElement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:1699,Performance,perform,performs,1699,"lement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ## max_retries: how many times to retry failed tasks -- very important on the cloud when there are transient errors; ## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file to be used instead of the GATK 4 jar; ## in the docker image. This must be supplied when running in an environment that does not support docker; ## (e.g. SGE cluster on a Broad on-prem VM); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over interval",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:4540,Performance,perform,performance,4540,"# gnomad, gnomad_idx: optional database of known germline variants (and its index) (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_idx: VCF of common variants (and its index)with allele frequencies for calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## realignment_index_bundle: resource for FilterAlignmentArtifacts, which runs if and only if it is specified. Generated by BwaMemIndexImageCreator.; ##; ## Funcotator parameters (see Funcotator help for more details).; ## funco_reference_version: ""hg19"" for hg19 or b37. ""hg38"" for hg38. Default: ""hg19""; ## funco_output_format: ""MAF"" to produce a MAF file, ""VCF"" to procude a VCF file. Default: ""MAF""; ## funco_compress: (Only valid if funco_output_format == ""VCF"" ) If true, will compress the output of Funcotator. If false, produces an uncompressed output file. Default: false; ## funco_use_gnomad_AF: If true, will include gnomAD allele frequency annotations in output by connecting to the internet to query gnomAD (this impacts performance). If false, will not annotate with gnomAD. Default: false; ## funco_transcript_selection_mode: How to select transcripts in Funcotator. ALL, CANONICAL, or BEST_EFFECT; ## funco_transcript_selection_list: Transcripts (one GENCODE ID per line) to give priority during selection process.; ## funco_data_sources_tar_gz: Funcotator datasources tar gz file. Bucket location is recommended when running on the cloud.; ## funco_annotation_defaults: Default values for annotations, when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funco_annotation_overrides: Values for annotations, even when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:6266,Security,authoriz,authorized,6266,"""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true, will only annotate variants that have passed filtering (. or PASS value in the FILTER column). If false, will annotate all variants in the input file. Default: true; ## funcotator_extra_args: Any additional arguments to pass to Funcotator. Default: """"; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested; a bamout.bam; ## file of reassembled reads if requested; ##; ## Cromwell version support; ## - Successfully tested on v34; ##; ## LICENSING :; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information; ## pertaining to the included programs. struct Runtime {; String gatk_docker; File? gatk_override; Int max_retries; Int preemptible; Int cpu; Int machine_mem; Int command_mem; Int disk; Int boot_disk_size; }. workflow Mutect2 {; input {; # Mutect2 inputs; File? intervals; File ref_fasta; File ref_fai; File ref_dict; File file_tumor_reads; Array[File] all_tumor_reads = read_lines(file_tumor_reads); File file_tumor_reads_indexes; Array[File] all_tumor_reads_indexes = read_lines(file_tumor_reads_indexes); Array[Pair[File,File]] tumor_reads_and_indexes = zip(all_tumor_reads,all_tumor_reads_indexes); File? normal_reads; File? normal_reads_index; File? pon; File? pon_idx; Int scatter_count; File? gnomad; File? gnomad_idx; File? variants_for_contamination; File? variants_for_contamination_idx; File? realignment_index_bundle; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:38811,Security,PASSWORD,PASSWORDS,38811,"~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation_over_arg}~{default="""" sep="" --annotation-override "" annotation_overrides} \; ~{excluded_fields_args}~{default="""" sep="" --exclude-field "" funcotator_excluded_fields} \; ~{filter_funcotations_args} \; ~{extra_args_arg}; # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:; if [[ ""~{output_format}"" == ""MAF"" ]] ; then; touch ~{output_maf_index}; fi; >>>. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + select_first([disk_space, runtime_params.disk]) + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File funcotated_output_file = ""~{output_file}""; File funcotated_output_file_index = ""~{output_file_index}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:5967,Testability,test,tested,5967,"ults: Default values for annotations, when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funco_annotation_overrides: Values for annotations, even when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true, will only annotate variants that have passed filtering (. or PASS value in the FILTER column). If false, will annotate all variants in the input file. Default: true; ## funcotator_extra_args: Any additional arguments to pass to Funcotator. Default: """"; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested; a bamout.bam; ## file of reassembled reads if requested; ##; ## Cromwell version support; ## - Successfully tested on v34; ##; ## LICENSING :; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information; ## pertaining to the included programs. struct Runtime {; String gatk_docker; File? gatk_override; Int max_retries; Int preemptible; Int cpu; Int machine_mem; Int command_mem; Int disk; Int boot_disk_size; }. workflow Mutect2 {; input {; # Mutect2 inputs; File? intervals; File ref_fasta; File ref_fai; File ref_dict; File file_tumor_reads; Array[File] all_tumor_reads = read_lines(file_tumor_reads); File file_tumor_reads_indexes; Array[File] all_tumor_reads_indexes = read_lines(file_tumor_reads_indexes); Array[Pair[File,File]] tumor_reads_and_in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:10433,Testability,log,logic,10433,"uncotations_or_default = select_first([filter_funcotations, true]). # Disk sizes used for dynamic sizing; Int ref_size = ceil(size(ref_fasta, ""GB"") + size(ref_dict, ""GB"") + size(ref_fai, ""GB"")); Int tumor_reads_size = ceil(size(tumor_reads, ""GB"") + size(tumor_reads_index, ""GB"")); Int gnomad_vcf_size = if defined(gnomad) then ceil(size(gnomad, ""GB"")) else 0; Int normal_reads_size = if defined(normal_reads) then ceil(size(normal_reads, ""GB"") + size(normal_reads_index, ""GB"")) else 0. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = if defined(funco_data_sources_tar_gz) then ceil(size(funco_data_sources_tar_gz, ""GB"") * 3) else 100; Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, ""GB"")) else 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size + select_first([emergency_extra_disk,0]). # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = basename(basename(tumor_reads, "".bam""),"".cram"") #hacky way to strip either .bam or .cram; String unfiltered_name = output_basename + ""-unfiltered""; String filtered_name = output_basename + ""-filtered""; String funcotated_name = output_basename + ""-funcotated"". String output_vcf_name = output_basename + "".vcf"". Int tumor_cram_to_bam_disk = ceil(tumor_reads_size * cram_to_bam_multiplier); Int normal_cram_to_bam_disk = ceil(normal_reads_size * cram_to_bam_multiplier). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk + disk_pad, ""boot_disk_size"": boot_disk_size}. if (basename(tumor_reads) != basename(tumor_reads, "".cram"")) {; call CramToBam as TumorCramToBam {; input:; ref_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:754,Usability,feedback,feedback,754,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. I'm trying to run the workflow below. There is a working version for a single sample (https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl), but I want to run the workflow on many samples concurrently. My attempt at creating a workflow to do so failed with the following error:. ```; Failed to process scatter block (reason 1 of 1): No conversion defined for Ast with name Outputs to WorkflowGraphElement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:14300,Usability,Learn,LearnReadOrientationModel,14300,"rvals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_bam = tumor_bam,; tumor_bai = tumor_bai,; normal_bam = normal_bam,; normal_bai = normal_bai,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx,; preemptible = preemptible,; max_retries = max_retries,; m2_extra_args = m2_extra_args,; variants_for_contamination = variants_for_contamination,; variants_for_contamination_idx = variants_for_contamination_idx,; make_bamout = make_bamout_or_default,; run_ob_filter = run_ob_filter,; compress = compress,; gga_vcf = gga_vcf,; gga_vcf_idx = gga_vcf_idx,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; disk_space = m2_per_scatter_size; }; }. Int merged_vcf_size = ceil(size(M2.unfiltered_vcf, ""GB"")); Int merged_bamout_size = ceil(size(M2.output_bamOut, ""GB"")); Int merged_tumor_pileups_size = ceil(size(M2.tumor_pileups, ""GB"")); Int merged_normal_pileups_size = ceil(size(M2.tumor_pileups, ""GB"")). if (run_ob_filter) {; call LearnReadOrientationModel {; input:; f1r2_tar_gz = M2.f1r2_counts,; runtime_params = standard_runtime,; mem = learn_read_orientation_mem; }; }. call MergeVCFs {; input:; input_vcfs = M2.unfiltered_vcf,; input_vcf_indices = M2.unfiltered_vcf_idx,; output_name = unfiltered_name,; compress = compress,; runtime_params = standard_runtime; }. if (make_bamout_or_default) {; call MergeBamOuts {; input:; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; bam_outs = M2.output_bamOut,; output_vcf_name = basename(MergeVCFs.merged_vcf, "".vcf""),; runtime_params = standard_runtime,; disk_space = ceil(merged_bamout_size * large_input_to_output_multiplier) + disk_pad,; }; }. call MergeStats { input: stats = M2.stats, runtime_params = standard_runtime }. if (defined(variants_for_contamination)) {; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = M2.tumor_pileups,; output_name = output_basename,; ref_dict = ref_dict,; runtime_params = standard_runtime; }. if (defined(normal_bam)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:16125,Usability,Learn,LearnReadOrientationModel,16125,"leupSummaries as MergeTumorPileups {; input:; input_tables = M2.tumor_pileups,; output_name = output_basename,; ref_dict = ref_dict,; runtime_params = standard_runtime; }. if (defined(normal_bam)){; call MergePileupSummaries as MergeNormalPileups {; input:; input_tables = M2.normal_pileups,; output_name = output_basename,; ref_dict = ref_dict,; runtime_params = standard_runtime; }; }. call CalculateContamination {; input:; tumor_pileups = MergeTumorPileups.merged_table,; normal_pileups = MergeNormalPileups.merged_table,; runtime_params = standard_runtime; }; }. call Filter {; input:; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; intervals = intervals,; unfiltered_vcf = MergeVCFs.merged_vcf,; unfiltered_vcf_idx = MergeVCFs.merged_vcf_idx,; output_name = filtered_name,; compress = compress,; mutect_stats = MergeStats.merged_stats,; contamination_table = CalculateContamination.contamination_table,; maf_segments = CalculateContamination.maf_segments,; artifact_priors_tar_gz = LearnReadOrientationModel.artifact_prior_table,; m2_extra_filtering_args = m2_extra_filtering_args,; runtime_params = standard_runtime,; disk_space = ceil(size(MergeVCFs.merged_vcf, ""GB"") * small_input_to_output_multiplier) + disk_pad; }. if (defined(realignment_index_bundle)) {; call FilterAlignmentArtifacts {; input:; bam = tumor_bam,; bai = tumor_bai,; realignment_index_bundle = select_first([realignment_index_bundle]),; realignment_extra_args = realignment_extra_args,; compress = compress,; output_name = filtered_name,; input_vcf = Filter.filtered_vcf,; input_vcf_idx = Filter.filtered_vcf_idx,; runtime_params = standard_runtime,; mem = filter_alignment_artifacts_mem; }; }. if (run_funcotator_or_default) {; File funcotate_vcf_input = select_first([FilterAlignmentArtifacts.filtered_vcf, Filter.filtered_vcf]); File funcotate_vcf_input_index = select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx]); call Funcotate {; input:; ref_fasta = ref_fasta,; ref",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:19112,Usability,Learn,LearnReadOrientationModel,19112,"; funcotator_excluded_fields = funcotator_excluded_fields,; filter_funcotations = filter_funcotations_or_default,; extra_args = funcotator_extra_args,; runtime_params = standard_runtime,; disk_space = ceil(size(funcotate_vcf_input, ""GB"") * large_input_to_output_multiplier) + funco_tar_size + disk_pad; }; ; }; output {; File filtered_vcf = select_first([FilterAlignmentArtifacts.filtered_vcf, Filter.filtered_vcf]); File filtered_vcf_idx = select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx]); File filtering_stats = Filter.filtering_stats; File mutect_stats = MergeStats.merged_stats; File? contamination_table = CalculateContamination.contamination_table. File? funcotated_file = Funcotate.funcotated_output_file; File? funcotated_file_index = Funcotate.funcotated_output_file_index; File? bamout = MergeBamOuts.merged_bam_out; File? bamout_index = MergeBamOuts.merged_bam_out_index; File? maf_segments = CalculateContamination.maf_segments; File? read_orientation_model_params = LearnReadOrientationModel.artifact_prior_table; }. }; }. task CramToBam {; input {; File ref_fasta; File ref_fai; File ref_dict; #cram and crai must be optional since Normal cram is optional; File? cram; File? crai; String name; Int disk_size; Int? mem; }. Int machine_mem = if defined(mem) then mem * 1000 else 6000. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools view -h -T ~{ref_fasta} ~{cram} |; samtools view -b -o ~{name}.bam -; samtools index -b ~{name}.bam; mv ~{name}.bam.bai ~{name}.bai; }. runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai""; }; }. task SplitIntervals {; input {; File? intervals; File ref_fasta; File ref_fai; File ref_dict; Int scatter_co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:29005,Usability,Learn,Learning,29005,"etries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File merged_stats = ""merged.stats""; }; }. task MergePileupSummaries {; input {; Array[File] input_tables; String output_name; File ref_dict; Runtime runtime_params; }. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" GatherPileupSummaries \; --sequence-dictionary ~{ref_dict} \; -I ~{sep=' -I ' input_tables} \; -O ~{output_name}.tsv; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File merged_table = ""~{output_name}.tsv""; }; }. # Learning step of the orientation bias mixture model, which is the recommended orientation bias filter as of September 2018; task LearnReadOrientationModel {; input {; Array[File] f1r2_tar_gz; Runtime runtime_params; Int? mem #override memory; }. Int machine_mem = select_first([mem, runtime_params.machine_mem]); Int command_mem = machine_mem - 1000. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{command_mem}m"" LearnReadOrientationModel \; -I ~{sep="" -I "" f1r2_tar_gz} \; -O ""artifact-priors.tar.gz""; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File artifact_prior_table = ""artifact-priors.tar.gz""; }. }. task CalculateContamination {; input {; String? intervals; File tumor_pileups; File? normal_pileups; Runtime runtime_params; }. command {; set -e. export GATK_LOCAL_JAR=~{default=""/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:29134,Usability,Learn,LearnReadOrientationModel,29134,"etries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File merged_stats = ""merged.stats""; }; }. task MergePileupSummaries {; input {; Array[File] input_tables; String output_name; File ref_dict; Runtime runtime_params; }. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" GatherPileupSummaries \; --sequence-dictionary ~{ref_dict} \; -I ~{sep=' -I ' input_tables} \; -O ~{output_name}.tsv; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File merged_table = ""~{output_name}.tsv""; }; }. # Learning step of the orientation bias mixture model, which is the recommended orientation bias filter as of September 2018; task LearnReadOrientationModel {; input {; Array[File] f1r2_tar_gz; Runtime runtime_params; Int? mem #override memory; }. Int machine_mem = select_first([mem, runtime_params.machine_mem]); Int command_mem = machine_mem - 1000. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{command_mem}m"" LearnReadOrientationModel \; -I ~{sep="" -I "" f1r2_tar_gz} \; -O ""artifact-priors.tar.gz""; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File artifact_prior_table = ""artifact-priors.tar.gz""; }. }. task CalculateContamination {; input {; String? intervals; File tumor_pileups; File? normal_pileups; Runtime runtime_params; }. command {; set -e. export GATK_LOCAL_JAR=~{default=""/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5345:29497,Usability,Learn,LearnReadOrientationModel,29497,"_mem}m"" GatherPileupSummaries \; --sequence-dictionary ~{ref_dict} \; -I ~{sep=' -I ' input_tables} \; -O ~{output_name}.tsv; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File merged_table = ""~{output_name}.tsv""; }; }. # Learning step of the orientation bias mixture model, which is the recommended orientation bias filter as of September 2018; task LearnReadOrientationModel {; input {; Array[File] f1r2_tar_gz; Runtime runtime_params; Int? mem #override memory; }. Int machine_mem = select_first([mem, runtime_params.machine_mem]); Int command_mem = machine_mem - 1000. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{command_mem}m"" LearnReadOrientationModel \; -I ~{sep="" -I "" f1r2_tar_gz} \; -O ""artifact-priors.tar.gz""; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File artifact_prior_table = ""artifact-priors.tar.gz""; }. }. task CalculateContamination {; input {; String? intervals; File tumor_pileups; File? normal_pileups; Runtime runtime_params; }. command {; set -e. export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" CalculateContamination -I ~{tumor_pileups} \; -O contamination.table --tumor-segmentation segments.table ~{""-matched "" + normal_pileups}; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345
https://github.com/broadinstitute/cromwell/issues/5346:3157,Availability,down,downsides,3157,"ed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3918,Availability,error,error,3918,"ashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:663,Deployability,update,update,663,"Hi,. _I'll manually synchronise this issue with Jira, I've raised it here as I think it has better exposure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4005,Deployability,configurat,configuration,4005," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4144,Deployability,configurat,configuration,4144," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:343,Modifiability,config,config,343,"Hi,. _I'll manually synchronise this issue with Jira, I've raised it here as I think it has better exposure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:912,Modifiability,Config,Configuring,912,"Hi,. _I'll manually synchronise this issue with Jira, I've raised it here as I think it has better exposure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4005,Modifiability,config,configuration,4005," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4144,Modifiability,config,configuration,4144," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4437,Modifiability,rewrite,rewriteBatchedStatements,4437," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4697,Modifiability,config,config,4697," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4704,Modifiability,Config,ConfigBackendLifecycleActorFactory,4704," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4743,Modifiability,config,config,4743," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:494,Performance,cache,cached-copy,494,"Hi,. _I'll manually synchronise this issue with Jira, I've raised it here as I think it has better exposure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1028,Performance,Cache,Cache,1028,"ure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1197,Performance,cache,cached-copy,1197,".atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I sel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1225,Performance,cache,cache,1225,".atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I sel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1253,Performance,Cache,Cache,1253,"kflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1826,Performance,cache,cache,1826,"ing/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2144,Performance,cache,cache,2144,"and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the down",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2451,Performance,cache,cache,2451,") is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2971,Performance,cache,cached,2971,"iders.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3027,Performance,cache,cache,3027,", use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3246,Performance,cache,cache,3246,"ing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-she",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3627,Performance,cache,cached,3627,"me.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-fac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3786,Performance,cache,cached,3786,"me.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-fac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4046,Performance,cache,cache,4046," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1259,Security,hash,hashing,1259,"kflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1317,Security,hash,hash,1317,"kflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1447,Security,hash,hash,1447," Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~Wha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1601,Security,hash,hash,1601," . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1821,Security,hash,hash-cache,1821,"ing/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:1869,Security,hash,hashes,1869,"ing/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2095,Security,hash,hash,2095,"`hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2240,Security,hash,hashing,2240," md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2399,Security,hash,hash,2399," md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2511,Security,hash,hash,2511,") is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2683,Security,hash,hashes,2683,".toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:2910,Security,hash,hashed,2910,"iders.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3241,Security,hash,hash-cache,3241,"ing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-she",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3324,Security,hash,hashing,3324,"ucceed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3580,Security,hash,hash,3580,"me.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-fac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3721,Security,hash,hashDifferential,3721,"me.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-fac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:3765,Security,hash,hash,3765,"me.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-fac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5346:4891,Security,hash,hashing-strategy,4891," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346
https://github.com/broadinstitute/cromwell/issues/5347:404,Availability,error,error,404,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:565,Availability,error,error,565,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:1902,Availability,error,error,1902,"ct2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:2123,Availability,error,error,2123,"utect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:6479,Deployability,configurat,configuration,6479," # Funcotator inputs; Boolean? run_funcotator; String? sequencing_center; String? sequence_source; String? funco_reference_version; String? funco_output_format; Boolean? funco_compress; Boolean? funco_use_gnomad_AF; File? funco_data_sources_tar_gz; String? funco_transcript_selection_mode; File? funco_transcript_selection_list; Array[String]? funco_annotation_defaults; Array[String]? funco_annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? funco_filter_funcotations; String? funcotator_extra_args. String funco_default_output_format = ""MAF""; ; # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk; }. Int contig_size = select_first([min_contig_size, 1000000]); Int preemptible_or_default = select_first([preemptible, 2]); Int max_retries_or_default = select_first([max_retries, 2]). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + ""--max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:6479,Modifiability,config,configuration,6479," # Funcotator inputs; Boolean? run_funcotator; String? sequencing_center; String? sequence_source; String? funco_reference_version; String? funco_output_format; Boolean? funco_compress; Boolean? funco_use_gnomad_AF; File? funco_data_sources_tar_gz; String? funco_transcript_selection_mode; File? funco_transcript_selection_list; Array[String]? funco_annotation_defaults; Array[String]? funco_annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? funco_filter_funcotations; String? funcotator_extra_args. String funco_default_output_format = ""MAF""; ; # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk; }. Int contig_size = select_first([min_contig_size, 1000000]); Int preemptible_or_default = select_first([preemptible, 2]); Int max_retries_or_default = select_first([max_retries, 2]). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + ""--max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:463,Safety,avoid,avoid,463,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:6524,Security,PASSWORD,PASSWORDS,6524," # Funcotator inputs; Boolean? run_funcotator; String? sequencing_center; String? sequence_source; String? funco_reference_version; String? funco_output_format; Boolean? funco_compress; Boolean? funco_use_gnomad_AF; File? funco_data_sources_tar_gz; String? funco_transcript_selection_mode; File? funco_transcript_selection_list; Array[String]? funco_annotation_defaults; Array[String]? funco_annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? funco_filter_funcotations; String? funcotator_extra_args. String funco_default_output_format = ""MAF""; ; # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk; }. Int contig_size = select_first([min_contig_size, 1000000]); Int preemptible_or_default = select_first([preemptible, 2]); Int max_retries_or_default = select_first([max_retries, 2]). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + ""--max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:848,Testability,test,test,848,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:893,Testability,test,test,893,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:1117,Testability,test,test,1117,"e you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nichol",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:1461,Testability,test,test,1461," be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:1542,Testability,test,test,1542," be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broad",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:2254,Testability,log,log,2254,"c97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; # vi : arrays of normal bams; # scatter_count:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5347:2311,Usability,feedback,feedback,2311,"c97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; # vi : arrays of normal bams; # scatter_count:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347
https://github.com/broadinstitute/cromwell/issues/5348:481,Availability,error,error,481,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:508,Availability,error,error,508,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:1155,Availability,error,errors,1155,"19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObjec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:2526,Availability,error,error,2526,"led to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObject: JsArray)`. I confirmed this by adding the case (I don't know scala, nor inner workings of Cromwell except enough to know this probably isn't a good way to do it, but just wanted to see if my suspicion was correct):. ```scala; case (key, subObject: JsArray) => Map(keyPrefix + key -> subObject.elements.mkString(""|"")).validNel; ```. Which fixed the error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:518,Integrability,message,message,518,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:50,Performance,cache,cache,50,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:395,Performance,cache,cacheMiss,395,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:719,Security,hash,hashes,719,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:1006,Security,hash,hashes,1006,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:1365,Security,hash,hashes,1365,"hen I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObject: JsArray)`. I confirmed this by adding the case (I don't know scala, nor inner workings of Cromwell except enough to know this probably isn't a good way to do it, but just wanted",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:1688,Security,hash,hashes,1688,"led to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObject: JsArray)`. I confirmed this by adding the case (I don't know scala, nor inner workings of Cromwell except enough to know this probably isn't a good way to do it, but just wanted to see if my suspicion was correct):. ```scala; case (key, subObject: JsArray) => Map(keyPrefix + key -> subObject.elements.mkString(""|"")).validNel; ```. Which fixed the error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/issues/5348:9,Usability,simpl,simple,9,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348
https://github.com/broadinstitute/cromwell/pull/5351:37,Deployability,deploy,deployment,37,"In order to de-risk a time-sensitive deployment, I am staging this as an option",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5351
https://github.com/broadinstitute/cromwell/pull/5351:15,Safety,risk,risk,15,"In order to de-risk a time-sensitive deployment, I am staging this as an option",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5351
https://github.com/broadinstitute/cromwell/issues/5352:591,Availability,avail,available,591,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. I'm using cromwell to kick off 1000 mutect2 jobs at a time, but right now the workflow doesn't kick off all 1000 sub-workflows simultaneously. When I look at my google cloud quotas none are close to being full. I'm wondering what settings I might need to change in the system or some other section of the google conf to better use all the compute I have available. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; I'm kicking off google cloud jobs using cromwell from a local VM.; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; # vi : arrays of normal bams; # scatter_count: number of parallel jobs when scattering over intervals; # pon_name: the resulting panel of normals is {pon_name}.vcf; # m2_extra_args: additional command line parameters for Mutect2. This should not involve --max-mnp-distance,; # which the wdl hard-codes to 0 because GenpmicsDBImport can't handle MNPs. #import """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:4880,Deployability,configurat,configuration,4880,", ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:5914,Deployability,Pipeline,Pipelines,5914,"er_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:4880,Modifiability,config,configuration,4880,", ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:5350,Modifiability,config,config,5350,"tter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ``",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:4925,Security,PASSWORD,PASSWORDS,4925,", ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:5722,Security,access,access,5722,"er_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:5506,Testability,test,test,5506,"tter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ``",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/issues/5352:649,Usability,feedback,feedback,649,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. I'm using cromwell to kick off 1000 mutect2 jobs at a time, but right now the workflow doesn't kick off all 1000 sub-workflows simultaneously. When I look at my google cloud quotas none are close to being full. I'm wondering what settings I might need to change in the system or some other section of the google conf to better use all the compute I have available. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; I'm kicking off google cloud jobs using cromwell from a local VM.; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; # vi : arrays of normal bams; # scatter_count: number of parallel jobs when scattering over intervals; # pon_name: the resulting panel of normals is {pon_name}.vcf; # m2_extra_args: additional command line parameters for Mutect2. This should not involve --max-mnp-distance,; # which the wdl hard-codes to 0 because GenpmicsDBImport can't handle MNPs. #import """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352
https://github.com/broadinstitute/cromwell/pull/5353:464,Deployability,pipeline,pipelines-tools,464,"This PR adds a workflow specific option (similar to `monitoring_script`) that will spin up an SSH server in a container on Google Genomics workers. It essentially uses the approach discussed [here](https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979) in an earlier GitHub issue, adding an additional Action that creates a Docker container with Google Genomics Tools'`ssh-server` entrypoint. (See [here]( https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) for Google's implementation.). I also updated the documentation to include the new parameter, and have tested it in our GCP environment with the attached files. There is a corresponding JIRA issue here: https://broadworkbench.atlassian.net/browse/BA-4966. Happy to add or fix up anything on here, just let me know. Thanks!; Adam. `workflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool; baseCommand: sleep; requirements:; - class: DockerRequirement; dockerPull: ""debian:stretch""; inputs:; time:; type: int; inputBinding:; position: 1; outputs: []; ```. `workflow-inputs.yml`; ```; time: 600; ```. `options.json`; ```; {; ""enable_ssh_access"": true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353
https://github.com/broadinstitute/cromwell/pull/5353:526,Deployability,pipeline,pipelines,526,"This PR adds a workflow specific option (similar to `monitoring_script`) that will spin up an SSH server in a container on Google Genomics workers. It essentially uses the approach discussed [here](https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979) in an earlier GitHub issue, adding an additional Action that creates a Docker container with Google Genomics Tools'`ssh-server` entrypoint. (See [here]( https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) for Google's implementation.). I also updated the documentation to include the new parameter, and have tested it in our GCP environment with the attached files. There is a corresponding JIRA issue here: https://broadworkbench.atlassian.net/browse/BA-4966. Happy to add or fix up anything on here, just let me know. Thanks!; Adam. `workflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool; baseCommand: sleep; requirements:; - class: DockerRequirement; dockerPull: ""debian:stretch""; inputs:; time:; type: int; inputBinding:; position: 1; outputs: []; ```. `workflow-inputs.yml`; ```; time: 600; ```. `options.json`; ```; {; ""enable_ssh_access"": true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353
https://github.com/broadinstitute/cromwell/pull/5353:614,Deployability,update,updated,614,"This PR adds a workflow specific option (similar to `monitoring_script`) that will spin up an SSH server in a container on Google Genomics workers. It essentially uses the approach discussed [here](https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979) in an earlier GitHub issue, adding an additional Action that creates a Docker container with Google Genomics Tools'`ssh-server` entrypoint. (See [here]( https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) for Google's implementation.). I also updated the documentation to include the new parameter, and have tested it in our GCP environment with the attached files. There is a corresponding JIRA issue here: https://broadworkbench.atlassian.net/browse/BA-4966. Happy to add or fix up anything on here, just let me know. Thanks!; Adam. `workflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool; baseCommand: sleep; requirements:; - class: DockerRequirement; dockerPull: ""debian:stretch""; inputs:; time:; type: int; inputBinding:; position: 1; outputs: []; ```. `workflow-inputs.yml`; ```; time: 600; ```. `options.json`; ```; {; ""enable_ssh_access"": true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353
https://github.com/broadinstitute/cromwell/pull/5353:679,Testability,test,tested,679,"This PR adds a workflow specific option (similar to `monitoring_script`) that will spin up an SSH server in a container on Google Genomics workers. It essentially uses the approach discussed [here](https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979) in an earlier GitHub issue, adding an additional Action that creates a Docker container with Google Genomics Tools'`ssh-server` entrypoint. (See [here]( https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) for Google's implementation.). I also updated the documentation to include the new parameter, and have tested it in our GCP environment with the attached files. There is a corresponding JIRA issue here: https://broadworkbench.atlassian.net/browse/BA-4966. Happy to add or fix up anything on here, just let me know. Thanks!; Adam. `workflow.cwl`; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool; baseCommand: sleep; requirements:; - class: DockerRequirement; dockerPull: ""debian:stretch""; inputs:; time:; type: int; inputBinding:; position: 1; outputs: []; ```. `workflow-inputs.yml`; ```; time: 600; ```. `options.json`; ```; {; ""enable_ssh_access"": true; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353
https://github.com/broadinstitute/cromwell/issues/5354:712,Availability,echo,echo,712,"I am following up on a [report](https://gatkforums.broadinstitute.org/wdl/discussion/23250/wdl-1-0-wont-let-me-call-an-optional-task-with-an-optional-input) (by someone else) filed over a year ago.; Now I have a slightly different need, that is, the task 2 will take the optional input `input_2_opt` for its required input.; So, the following is what I want to achieve. ```; version 1.0. workflow my_workflow {; input {; File input_1; File? input_2_opt; }. call task1 {; input:; input_1 = input_1; }. if (defined(input_2_opt)) {; call task2 {; input:; input_2 = input_2_opt; }; }. output {; File output_1 = task1.output_1; File? output_2 = task2.output_2; }; }. task task1 {; input{; File input_1; }; command {; echo ""Hello, world!"" > hello.txt; }; output {; File output_1 = ""hello.txt""; }; }. task task2 {; input{; File input_2; }; command {; cat ${input_2} > goodbye.txt; }; output {; File output_2 = ""goodbye.txt""; }; }. ```. Running `womtool validate` on this gives. ```; Failed to process workflow definition 'my_workflow' (reason 1 of 1): Failed to process 'call task2' (reason 1 of 1): Failed to supply input input_2 = input_2_opt (reason 1 of 1): Cannot coerce expression of type 'File?' to 'File'; ```. But like in the original post, if I take out the version specification and the `input` braces in the workflow and tasks, womtool thinks the WDL is OK. Can you please explain what is the cause? And is there a solution on my end?. Thanks. ----------------------; ### The Jira interface is way too overwhelming ; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5354
https://github.com/broadinstitute/cromwell/issues/5354:1486,Integrability,interface,interface,1486,"I am following up on a [report](https://gatkforums.broadinstitute.org/wdl/discussion/23250/wdl-1-0-wont-let-me-call-an-optional-task-with-an-optional-input) (by someone else) filed over a year ago.; Now I have a slightly different need, that is, the task 2 will take the optional input `input_2_opt` for its required input.; So, the following is what I want to achieve. ```; version 1.0. workflow my_workflow {; input {; File input_1; File? input_2_opt; }. call task1 {; input:; input_1 = input_1; }. if (defined(input_2_opt)) {; call task2 {; input:; input_2 = input_2_opt; }; }. output {; File output_1 = task1.output_1; File? output_2 = task2.output_2; }; }. task task1 {; input{; File input_1; }; command {; echo ""Hello, world!"" > hello.txt; }; output {; File output_1 = ""hello.txt""; }; }. task task2 {; input{; File input_2; }; command {; cat ${input_2} > goodbye.txt; }; output {; File output_2 = ""goodbye.txt""; }; }. ```. Running `womtool validate` on this gives. ```; Failed to process workflow definition 'my_workflow' (reason 1 of 1): Failed to process 'call task2' (reason 1 of 1): Failed to supply input input_2 = input_2_opt (reason 1 of 1): Cannot coerce expression of type 'File?' to 'File'; ```. But like in the original post, if I take out the version specification and the `input` braces in the workflow and tasks, womtool thinks the WDL is OK. Can you please explain what is the cause? And is there a solution on my end?. Thanks. ----------------------; ### The Jira interface is way too overwhelming ; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5354
https://github.com/broadinstitute/cromwell/issues/5354:946,Security,validat,validate,946,"I am following up on a [report](https://gatkforums.broadinstitute.org/wdl/discussion/23250/wdl-1-0-wont-let-me-call-an-optional-task-with-an-optional-input) (by someone else) filed over a year ago.; Now I have a slightly different need, that is, the task 2 will take the optional input `input_2_opt` for its required input.; So, the following is what I want to achieve. ```; version 1.0. workflow my_workflow {; input {; File input_1; File? input_2_opt; }. call task1 {; input:; input_1 = input_1; }. if (defined(input_2_opt)) {; call task2 {; input:; input_2 = input_2_opt; }; }. output {; File output_1 = task1.output_1; File? output_2 = task2.output_2; }; }. task task1 {; input{; File input_1; }; command {; echo ""Hello, world!"" > hello.txt; }; output {; File output_1 = ""hello.txt""; }; }. task task2 {; input{; File input_2; }; command {; cat ${input_2} > goodbye.txt; }; output {; File output_2 = ""goodbye.txt""; }; }. ```. Running `womtool validate` on this gives. ```; Failed to process workflow definition 'my_workflow' (reason 1 of 1): Failed to process 'call task2' (reason 1 of 1): Failed to supply input input_2 = input_2_opt (reason 1 of 1): Cannot coerce expression of type 'File?' to 'File'; ```. But like in the original post, if I take out the version specification and the `input` braces in the workflow and tasks, womtool thinks the WDL is OK. Can you please explain what is the cause? And is there a solution on my end?. Thanks. ----------------------; ### The Jira interface is way too overwhelming ; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5354
https://github.com/broadinstitute/cromwell/pull/5358:36,Availability,error,errors,36,"Might not eliminate the ""programmer errors"" just yet, but does improve logging on (unexpected) restarts and eliminates the weird and unintentional parenting of the RootWorkflowFileHashCacheActor by the WorkflowManagerActor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5358
https://github.com/broadinstitute/cromwell/pull/5358:71,Testability,log,logging,71,"Might not eliminate the ""programmer errors"" just yet, but does improve logging on (unexpected) restarts and eliminates the weird and unintentional parenting of the RootWorkflowFileHashCacheActor by the WorkflowManagerActor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5358
https://github.com/broadinstitute/cromwell/issues/5359:12,Modifiability,config,configure,12,the example configure file mentioned in [cromwell document](https://cromwell.readthedocs.io/en/stable/Configuring/) shows 404: [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359
https://github.com/broadinstitute/cromwell/issues/5359:102,Modifiability,Config,Configuring,102,the example configure file mentioned in [cromwell document](https://cromwell.readthedocs.io/en/stable/Configuring/) shows 404: [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359
https://github.com/broadinstitute/cromwell/pull/5361:33,Modifiability,config,config,33,I believe the deletion of $HOME/.config/gcloud/gce file before each gsutil invocation was intended to be a workaround for a gsutil issue. The issue was believed to be fixed in gcloud 275 but since the hanging issues appear to persist I added back in the deletion of this file everywhere I could find.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5361
https://github.com/broadinstitute/cromwell/issues/5370:1203,Availability,alive,alive,1203,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:755,Modifiability,config,config,755,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:762,Modifiability,Config,ConfigBackendLifecycleActorFactory,762,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:799,Modifiability,config,config,799,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:339,Performance,cache,cache,339,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:359,Performance,cache,cache,359,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:1475,Performance,cache,cache-results,1475,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:1708,Performance,cache,cached,1708,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:819,Safety,timeout,timeout-seconds,819,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:1381,Security,hash,hashing-strategy,1381,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5370:273,Testability,test,testing,273,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370
https://github.com/broadinstitute/cromwell/issues/5380:1466,Deployability,configurat,configuration,1466,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380
https://github.com/broadinstitute/cromwell/issues/5380:1466,Modifiability,config,configuration,1466,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380
https://github.com/broadinstitute/cromwell/issues/5380:1511,Security,PASSWORD,PASSWORDS,1511,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380
https://github.com/broadinstitute/cromwell/issues/5380:676,Usability,feedback,feedback,676,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380
https://github.com/broadinstitute/cromwell/issues/5387:395,Availability,error,error,395,"When I created a struct and then use Array[MyStruct] it works, but when I go to Array[Array[Mystruct]] if fails with the latest (dev) version of Cromwell, for both development and 1.0 versions of wdl. I enclose the zip with workflows (there I have Array[Array[Gentrome]] that crashes); [quant_index_batch.zip](https://github.com/broadinstitute/cromwell/files/4117802/quant_index_batch.zip). the error is:; ```; Workflow input processing failed. WorkflowFailure(Failed to evaluate input 'references' (reason 1 of 1): No coercion defined from '{""genome"":""/data/ensembl/99/species/acanthochromis_polyacanthus/Acanthochromis_polyacanthus.ASM210954v1.dna.toplevel.fa"",""species"":""acanthochromis_polyacanthus"",""subversion"":""ensembl_99"",""transcriptome"":""/data/ensembl/99/species/acanthochromis_polyacanthus/Acanthochromis_polyacanthus.ASM210954v1.cdna.all.fa"",""version"":""ASM210954v1""}' of type 'spray.json.JsObject' to 'Array[WomCompositeType { species -> String subversion -> String? version -> String genome -> File transcriptome -> File }]'.,List()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5387
https://github.com/broadinstitute/cromwell/pull/5390:52,Deployability,release,release,52,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5390
https://github.com/broadinstitute/cromwell/pull/5390:196,Deployability,release,release-notes,196,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5390
https://github.com/broadinstitute/cromwell/pull/5390:178,Performance,optimiz,optimized-os,178,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5390
https://github.com/broadinstitute/cromwell/pull/5391:52,Deployability,release,release,52,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5391
https://github.com/broadinstitute/cromwell/pull/5391:196,Deployability,release,release-notes,196,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5391
https://github.com/broadinstitute/cromwell/pull/5391:178,Performance,optimiz,optimized-os,178,77 predates the potentially-problematic December 17 release of 79 [0]. The first report of localization hanging reached us on December 22. [0] https://cloud.google.com/container-optimized-os/docs/release-notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5391
https://github.com/broadinstitute/cromwell/issues/5395:196,Availability,echo,echo,196,"I'm trying to run Cromwell-48 on [Sherlock](https://www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:344,Availability,echo,echoHello,344,"I'm trying to run Cromwell-48 on [Sherlock](https://www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:968,Availability,heartbeat,heartbeat,968,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1032,Availability,heartbeat,heartbeatInterval,1032,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1091,Availability,failure,failureShutdownDuration,1091,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1197,Availability,error,error,1197,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1280,Availability,error,error,1280,"ry to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" +",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1400,Availability,down,down,1400,":30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1439,Availability,error,error,1439,"tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1533,Availability,error,error,1533,"tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1652,Availability,down,down,1652,"ite batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1691,Availability,error,error,1691,"InMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2413,Availability,alive,alive,2413," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2547,Availability,error,error,2547," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:978,Deployability,configurat,configuration,978,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2346,Integrability,wrap,wrap,2346," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:978,Modifiability,config,configuration,978,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1984,Modifiability,config,config,1984,"tion:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:1991,Modifiability,Config,ConfigBackendLifecycleActorFactory,1991,"nutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2028,Modifiability,config,config,2028,"nutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2151,Performance,queue,queue,2151,"nutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2266,Performance,queue,queue,2266," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:2732,Testability,log,login,2732," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5395:3101,Testability,log,login,3101," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395
https://github.com/broadinstitute/cromwell/issues/5396:722,Availability,error,error,722,"Hello,. I'am running a wdl worflow that I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:3956,Availability,echo,echo,3956," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4899,Availability,echo,echo,4899,"g_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; el",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5925,Availability,echo,echo,5925,"nitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_sta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:1161,Energy Efficiency,schedul,scheduler,1161,"quencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:1569,Energy Efficiency,monitor,monitoring,1569,"tter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:1755,Energy Efficiency,adapt,adapters,1755,": Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:2658,Energy Efficiency,adapt,adapters,2658," quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:3807,Energy Efficiency,monitor,monitoring,3807,"ngth_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:3932,Energy Efficiency,monitor,monitoring,3932,"ngth_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:3965,Energy Efficiency,monitor,monitoring,3965," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4001,Energy Efficiency,monitor,monitoring,4001," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4750,Energy Efficiency,monitor,monitoring,4750,"s a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4875,Energy Efficiency,monitor,monitoring,4875,"s a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4908,Energy Efficiency,monitor,monitoring,4908,"g_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; el",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4944,Energy Efficiency,monitor,monitoring,4944,"g_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; el",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5776,Energy Efficiency,monitor,monitoring,5776," ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5901,Energy Efficiency,monitor,monitoring,5901," ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5934,Energy Efficiency,monitor,monitoring,5934,"nitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_sta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5970,Energy Efficiency,monitor,monitoring,5970,"nitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_sta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:6579,Energy Efficiency,monitor,monitoring,6579,"nd_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_start_R2"": 25,; ""scMethTask3.trim_end_R2"": -2,; ""scMethTask3.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMethTask3.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA"",. ""scMethTask3.memory_task2"": 20; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:398,Integrability,depend,depends,398,"Hello,. I'am running a wdl worflow that I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:737,Integrability,message,message,737,"Hello,. I'am running a wdl worflow that I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:1755,Integrability,adapter,adapters,1755,": Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:2658,Integrability,adapter,adapters,2658," quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:1755,Modifiability,adapt,adapters,1755,": Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:2658,Modifiability,adapt,adapters,2658," quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:1028,Safety,avoid,avoid,1028," I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:3943,Testability,log,log,3943," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4012,Testability,log,log,4012,"R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${rea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4886,Testability,log,log,4886,"g_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; el",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:4955,Testability,log,log,4955," ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5912,Testability,log,log,5912,"nitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_sta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/issues/5396:5981,Testability,log,log,5981,"rim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_start_R2"": 25,; ""scMethTask3.trim_end_R2"": -2,; ""scMethTask3.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMethTask3.adapters_2"": ""AGATCGGAAGAGCGTCGTG",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396
https://github.com/broadinstitute/cromwell/pull/5397:67,Modifiability,refactor,refactoring,67,Prose copy/paste/edited from the earlier `CALL_CACHING_HASH_ENTRY` refactoring.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5397
https://github.com/broadinstitute/cromwell/issues/5399:38,Testability,log,log,38,"Hi, the notes in this repo tell me to log issues in JIRA, but they are not picked up or replied to. ; Here are the issues:; - https://broadworkbench.atlassian.net/projects/BA/issues/BA-5915; - https://broadworkbench.atlassian.net/projects/BA/issues/BA-6171; - https://broadworkbench.atlassian.net/projects/BA/issues/BA-6214. I would appreciate a reply to these issues, especially the latest one which is a blocker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5399
https://github.com/broadinstitute/cromwell/issues/5400:103,Availability,alive,alive,103,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400
https://github.com/broadinstitute/cromwell/issues/5400:1182,Availability,alive,alive,1182,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400
https://github.com/broadinstitute/cromwell/issues/5400:1404,Availability,down,down,1404,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400
https://github.com/broadinstitute/cromwell/issues/5400:283,Integrability,depend,depending,283,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400
https://github.com/broadinstitute/cromwell/issues/5400:301,Modifiability,config,configured,301,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400
https://github.com/broadinstitute/cromwell/pull/5402:195,Deployability,patch,patch,195,This hard coded hard link breaks workflows on file systems that do no support cross-directory hard links (e.g. beegfs) . Just like https://github.com/ENCODE-DCC/chip-seq-pipeline2/issues/91 this patch fixed the issue for us and we run a patched version of cromwell for the last month without any problems. Hopefully we can fix this upstream as all pacbio sequencers are affected as well. ******* EDIT****; Just saw the PR #5250 proposes the same change ..... please feel free to discard mine,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402
https://github.com/broadinstitute/cromwell/pull/5402:237,Deployability,patch,patched,237,This hard coded hard link breaks workflows on file systems that do no support cross-directory hard links (e.g. beegfs) . Just like https://github.com/ENCODE-DCC/chip-seq-pipeline2/issues/91 this patch fixed the issue for us and we run a patched version of cromwell for the last month without any problems. Hopefully we can fix this upstream as all pacbio sequencers are affected as well. ******* EDIT****; Just saw the PR #5250 proposes the same change ..... please feel free to discard mine,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402
https://github.com/broadinstitute/cromwell/pull/5404:40,Integrability,message,message,40,I want to tell support to look for this message in Kibana and the current language is not a good look for our users.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5404
https://github.com/broadinstitute/cromwell/pull/5408:36,Integrability,depend,depending,36,"Completes (and possibly supersedes, depending on which merges first) the good work started in #5406.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5408
https://github.com/broadinstitute/cromwell/issues/5412:1063,Deployability,configurat,configuration,1063,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5412
https://github.com/broadinstitute/cromwell/issues/5412:1063,Modifiability,config,configuration,1063,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5412
https://github.com/broadinstitute/cromwell/issues/5412:1108,Security,PASSWORD,PASSWORDS,1108,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5412
https://github.com/broadinstitute/cromwell/issues/5412:273,Usability,feedback,feedback,273,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5412
https://github.com/broadinstitute/cromwell/issues/5414:448,Availability,echo,echo,448,"I know there have been some issues with read_object/write_object and structs but I haven't been able to find anything mentioning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414
https://github.com/broadinstitute/cromwell/issues/5414:771,Availability,echo,echo,771,"I know there have been some issues with read_object/write_object and structs but I haven't been able to find anything mentioning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414
https://github.com/broadinstitute/cromwell/issues/5414:932,Availability,error,error,932,"I know there have been some issues with read_object/write_object and structs but I haven't been able to find anything mentioning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414
https://github.com/broadinstitute/cromwell/issues/5414:1120,Availability,echo,echo,1120,"oning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seem",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414
https://github.com/broadinstitute/cromwell/issues/5414:1517,Availability,echo,echo,1517,"params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seems like it's casting the 3 to 3.0 etc.). In any case this only seems to be a problem with defining structs inline. The conversion seems to work okay if you use input.json or if you read_json from some params.json at runtime (however I want to fix parameters inline for my current use case). The current workaround is to use object definition inline:. ```; Params p = object {; boolean: true,; float",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414
https://github.com/broadinstitute/cromwell/issues/5414:1879,Availability,echo,echo,1879," workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seems like it's casting the 3 to 3.0 etc.). In any case this only seems to be a problem with defining structs inline. The conversion seems to work okay if you use input.json or if you read_json from some params.json at runtime (however I want to fix parameters inline for my current use case). The current workaround is to use object definition inline:. ```; Params p = object {; boolean: true,; float: 0.04; } ; ```. This is okay but it seems like the object syntax is going to be deprecated in newer versions (and the WDL 1.0 spec doesn't have any examples of this). Also it's generally awkward that the JSON syntax works for some combinations but not others. Thanks for any insights!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414
https://github.com/broadinstitute/cromwell/pull/5416:0,Modifiability,Config,Config,0,"Config only and no automated tests. Confirmed working in v2alpha1, code is in place for v2beta but may not be working there yet if the changes have not yet been promoted to beta.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5416
https://github.com/broadinstitute/cromwell/pull/5416:29,Testability,test,tests,29,"Config only and no automated tests. Confirmed working in v2alpha1, code is in place for v2beta but may not be working there yet if the changes have not yet been promoted to beta.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5416
https://github.com/broadinstitute/cromwell/pull/5420:119,Testability,test,testing,119,"For Cromwell run mode, add the option to add outputs explicitly to an output json file. The thinking is that for a WDL testing framework, being able to diff expected outputs with generated outputs (both from Cromwell and miniWDL) if they are in a json format, it becomes easy to use them. The alternatives are digging through the metadata json or cromwell logs -- so this option is meant to just increase convenience of outputs for a test framework. . It could also be used as documentation for the expected output format for example WDLs in the WDL spec.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5420
https://github.com/broadinstitute/cromwell/pull/5420:356,Testability,log,logs,356,"For Cromwell run mode, add the option to add outputs explicitly to an output json file. The thinking is that for a WDL testing framework, being able to diff expected outputs with generated outputs (both from Cromwell and miniWDL) if they are in a json format, it becomes easy to use them. The alternatives are digging through the metadata json or cromwell logs -- so this option is meant to just increase convenience of outputs for a test framework. . It could also be used as documentation for the expected output format for example WDLs in the WDL spec.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5420
https://github.com/broadinstitute/cromwell/pull/5420:434,Testability,test,test,434,"For Cromwell run mode, add the option to add outputs explicitly to an output json file. The thinking is that for a WDL testing framework, being able to diff expected outputs with generated outputs (both from Cromwell and miniWDL) if they are in a json format, it becomes easy to use them. The alternatives are digging through the metadata json or cromwell logs -- so this option is meant to just increase convenience of outputs for a test framework. . It could also be used as documentation for the expected output format for example WDLs in the WDL spec.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5420
https://github.com/broadinstitute/cromwell/issues/5421:158,Availability,down,downloads,158,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:712,Availability,down,download,712,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:1069,Availability,echo,echo,1069,", not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:2201,Availability,echo,echo,2201,"io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3137,Availability,error,error,3137,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3609,Availability,error,error,3609,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3775,Availability,down,downloading,3775,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3806,Availability,error,error,3806,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:2069,Deployability,pipeline,pipeline,2069,"a27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3143,Integrability,message,messages,3143,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:553,Testability,log,log,553,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:583,Testability,log,log,583,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:1730,Testability,log,log,1730,"OT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_executi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:1897,Testability,log,log,1897,"Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3190,Testability,log,logs,3190,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3201,Testability,log,log,3201,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:3347,Testability,log,log,3347,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/issues/5421:744,Usability,simpl,simple,744,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421
https://github.com/broadinstitute/cromwell/pull/5424:95,Safety,timeout,timeout,95,Q: Ever wonder why the first two `checkDescriptions` always failed in centaur?; A: Because out timeout handler was eager instead of lazy 🤦‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5424
https://github.com/broadinstitute/cromwell/pull/5425:6,Deployability,hotfix,hotfix,6,Slick hotfix branch: https://github.com/grsterin/slick/tree/v3.3.2-2076hotfix; Slick PR: https://github.com/slick/slick/pull/2101,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5425
https://github.com/broadinstitute/cromwell/pull/5426:67,Performance,cache,cacheable,67,Adds a meta attribute that allows people to mark tasks as non-call-cacheable. Note: Marking as a community contribution since this was an out-of-hours project.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5426
https://github.com/broadinstitute/cromwell/pull/5427:253,Deployability,release,release,253,"I considered filing an issue, but it seems like the best avenue to get the maintainers thoughts. How do you all feel about creating a page where community-backed projects can be highlighted? The idea occurred to me as I was thinking about our impending release of `oliver` (not quite there yet). The only other project I was aware of was `cromshell`, not sure if there are any others you would want to add 🤷‍♂ .",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5427
https://github.com/broadinstitute/cromwell/pull/5429:189,Availability,avail,available,189,Simplification of #5415 removing the `attempt-1` redirection. ~Starting out with Don't Look At Me because I anticipate there might be some test fixup before it's ready for prime time.~ Now available for looking at.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429
https://github.com/broadinstitute/cromwell/pull/5429:139,Testability,test,test,139,Simplification of #5415 removing the `attempt-1` redirection. ~Starting out with Don't Look At Me because I anticipate there might be some test fixup before it's ready for prime time.~ Now available for looking at.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429
https://github.com/broadinstitute/cromwell/pull/5429:0,Usability,Simpl,Simplification,0,Simplification of #5415 removing the `attempt-1` redirection. ~Starting out with Don't Look At Me because I anticipate there might be some test fixup before it's ready for prime time.~ Now available for looking at.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429
https://github.com/broadinstitute/cromwell/pull/5430:2,Testability,test,tested,2,"I tested this manually by changing the values in the swagger pages and watching the returned `metadataSource` in the response change. I didn't add explicit regression tests for the feature in _this_ PR because:. 1. It's not a production feature, and we're testing the default ""not specified"" case with the existing metadata requests; 2. I anticipate the upcoming ""before vs after"" centaur additions to pretty heavily exercise this functionality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5430
https://github.com/broadinstitute/cromwell/pull/5430:167,Testability,test,tests,167,"I tested this manually by changing the values in the swagger pages and watching the returned `metadataSource` in the response change. I didn't add explicit regression tests for the feature in _this_ PR because:. 1. It's not a production feature, and we're testing the default ""not specified"" case with the existing metadata requests; 2. I anticipate the upcoming ""before vs after"" centaur additions to pretty heavily exercise this functionality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5430
https://github.com/broadinstitute/cromwell/pull/5430:256,Testability,test,testing,256,"I tested this manually by changing the values in the swagger pages and watching the returned `metadataSource` in the response change. I didn't add explicit regression tests for the feature in _this_ PR because:. 1. It's not a production feature, and we're testing the default ""not specified"" case with the existing metadata requests; 2. I anticipate the upcoming ""before vs after"" centaur additions to pretty heavily exercise this functionality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5430
https://github.com/broadinstitute/cromwell/issues/5434:283,Availability,down,downloaded,283,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:29,Security,validat,validate,29,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:175,Security,Validat,ValidateBamsWf,175,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:190,Security,Validat,ValidateBAM,190,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:369,Security,validat,validation,369,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:410,Security,validat,validate,410,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:419,Security,validat,validate-bam,419,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:439,Security,validat,validate-bam,439,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:510,Security,Validat,ValidateBamsWf,510,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:525,Security,Validat,ValidateBAM,525,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:571,Security,Validat,ValidateBamsWf,571,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:586,Security,Validat,ValidateBAM,586,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:613,Security,Validat,ValidateBamsWf,613,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:650,Security,Validat,ValidateBamsWf,650,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:665,Security,Validat,ValidateBAM,665,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:694,Security,Validat,ValidateBamsWf,694,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:729,Security,Validat,ValidateBamsWf,729,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:744,Security,Validat,ValidateBAM,744,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:772,Security,Validat,ValidateBamsWf,772,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:839,Security,Validat,ValidateBamsWf,839,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:854,Security,Validat,ValidateBAM,854,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:901,Security,Validat,ValidateBamsWf,901,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:916,Security,Validat,ValidateBAM,916,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:943,Security,Validat,ValidateBamsWf,943,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:980,Security,Validat,ValidateBamsWf,980,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:995,Security,Validat,ValidateBAM,995,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:1024,Security,Validat,ValidateBamsWf,1024,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:1059,Security,Validat,ValidateBamsWf,1059,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:1074,Security,Validat,ValidateBAM,1074,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/issues/5434:1102,Security,Validat,ValidateBamsWf,1102,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434
https://github.com/broadinstitute/cromwell/pull/5437:12,Deployability,update,updates,12,"Due to some updates that got in cromwell version 49 the regex that creates the relative outputs was evaluated differently (correctly, to be precise). This made it so that any directory structure created by the user is lost. This broke all the testing on biowdl.; I have updated the test cases to notice such a regression in the future. I also fixed the regex to behave like it is described in the documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437
https://github.com/broadinstitute/cromwell/pull/5437:270,Deployability,update,updated,270,"Due to some updates that got in cromwell version 49 the regex that creates the relative outputs was evaluated differently (correctly, to be precise). This made it so that any directory structure created by the user is lost. This broke all the testing on biowdl.; I have updated the test cases to notice such a regression in the future. I also fixed the regex to behave like it is described in the documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437
https://github.com/broadinstitute/cromwell/pull/5437:243,Testability,test,testing,243,"Due to some updates that got in cromwell version 49 the regex that creates the relative outputs was evaluated differently (correctly, to be precise). This made it so that any directory structure created by the user is lost. This broke all the testing on biowdl.; I have updated the test cases to notice such a regression in the future. I also fixed the regex to behave like it is described in the documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437
https://github.com/broadinstitute/cromwell/pull/5437:282,Testability,test,test,282,"Due to some updates that got in cromwell version 49 the regex that creates the relative outputs was evaluated differently (correctly, to be precise). This made it so that any directory structure created by the user is lost. This broke all the testing on biowdl.; I have updated the test cases to notice such a regression in the future. I also fixed the regex to behave like it is described in the documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437
https://github.com/broadinstitute/cromwell/pull/5438:25,Testability,test,tests,25,Clone of #5343 to run CI tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5438
https://github.com/broadinstitute/cromwell/pull/5440:26,Testability,test,test,26,"Includes:. * Primarily, a test that metadata will get deleted only after a minimum time has passed.; * A fix to return the _archived_ version of metadata for workflows in the `ArchivedAndPurged` state.; * Modification to a number of ""did anything change"" tests functions to use previously stored results as their expectations (rather than comparing against fresh results fetched with `metadataSource=Unarchived`). This allows the comparisons to run even after metadata has been deleted. Does not include (but you might think that this would be a nice idea?):. * Checking that after the workflow metadata is deleted, the `metadataSource: Unarchived` version of metadata includes only (a) workflow ID, (b) `metadataSource` and (c) `labels`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5440
https://github.com/broadinstitute/cromwell/pull/5440:255,Testability,test,tests,255,"Includes:. * Primarily, a test that metadata will get deleted only after a minimum time has passed.; * A fix to return the _archived_ version of metadata for workflows in the `ArchivedAndPurged` state.; * Modification to a number of ""did anything change"" tests functions to use previously stored results as their expectations (rather than comparing against fresh results fetched with `metadataSource=Unarchived`). This allows the comparisons to run even after metadata has been deleted. Does not include (but you might think that this would be a nice idea?):. * Checking that after the workflow metadata is deleted, the `metadataSource: Unarchived` version of metadata includes only (a) workflow ID, (b) `metadataSource` and (c) `labels`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5440
https://github.com/broadinstitute/cromwell/pull/5441:81,Safety,avoid,avoid,81,"This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I haven't tested this yet (hence the draft) and from what I have seen it isn't being tested by centaur either, so I'll still need to have a look at that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441
https://github.com/broadinstitute/cromwell/pull/5441:237,Testability,test,tested,237,"This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I haven't tested this yet (hence the draft) and from what I have seen it isn't being tested by centaur either, so I'll still need to have a look at that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441
https://github.com/broadinstitute/cromwell/pull/5441:312,Testability,test,tested,312,"This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I haven't tested this yet (hence the draft) and from what I have seen it isn't being tested by centaur either, so I'll still need to have a look at that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441
https://github.com/broadinstitute/cromwell/pull/5441:184,Usability,simpl,simply,184,"This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I haven't tested this yet (hence the draft) and from what I have seen it isn't being tested by centaur either, so I'll still need to have a look at that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441
https://github.com/broadinstitute/cromwell/pull/5447:356,Deployability,update,updated,356,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447
https://github.com/broadinstitute/cromwell/pull/5447:96,Safety,avoid,avoid,96,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447
https://github.com/broadinstitute/cromwell/pull/5447:250,Testability,test,testing,250,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447
https://github.com/broadinstitute/cromwell/pull/5447:199,Usability,simpl,simply,199,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447
https://github.com/broadinstitute/cromwell/pull/5450:918,Availability,reliab,reliable,918,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1924,Availability,reliab,reliably,1924,"integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2065,Availability,avail,available,2065,"operties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has prov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2583,Availability,avail,available,2583," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:152,Deployability,pipeline,pipeline,152,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2685,Deployability,configurat,configuration,2685," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:3101,Deployability,release,releases,3101," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:837,Energy Efficiency,power,power,837,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1150,Integrability,message,message,1150,"q) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/has",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1246,Integrability,message,message,1246,"work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1289,Integrability,message,message,1289,"5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1371,Integrability,message,messages,1371,"me` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1429,Integrability,message,message,1429,"ith file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2685,Modifiability,config,configuration,2685," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2985,Modifiability,config,config,2985," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:587,Security,hash,hash,587,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:927,Security,hash,hashes,927,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:943,Security,integrity,integrity,943,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1034,Security,hash,hash,1034,"problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1056,Security,hash,hash,1056,"caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations availabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1185,Security,hash,hash,1185,"q) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/has",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1221,Security,hash,hash,1221,"work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1317,Security,hash,hash,1317,"5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1394,Security,hash,hash,1394,"me` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1455,Security,hash,hash,1455,"ith file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1494,Security,hash,hash,1494,"ith file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1539,Security,hash,hash,1539,"ith file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1828,Security,hash,hashing,1828,"e submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1886,Security,hash,hashing,1886,"e submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:1943,Security,hash,hashes,1943,"integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2147,Security,hash,hashtest,2147," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2800,Security,hash,hash,2800," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:80,Testability,benchmark,benchmarking,80,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2101,Testability,benchmark,benchmarking,2101,"operties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has prov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/pull/5450:2939,Usability,clear,clear,2939," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450
https://github.com/broadinstitute/cromwell/issues/5451:532,Availability,echo,echo,532,"Question; -----------------; Is there any plan on speeding up `java -jar $JAR run`? Pipeline developer would definitely benefits from a faster startup. Symptom; -----------------; ```bash; time java -jar cromwell-49.jar run sub-flow.wdl -i input.json; ```. ```; real	1m12.833s; user	1m12.148s; sys	0m7.644s; ```. ```; $ cat input.json ; {; ""hello_and_goodbye.hello_and_goodbye_input"":""test1""; }; ```. Detail; ----------; backend: local. File: `sub-flow.wdl`; ```wdl; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451
https://github.com/broadinstitute/cromwell/issues/5451:84,Deployability,Pipeline,Pipeline,84,"Question; -----------------; Is there any plan on speeding up `java -jar $JAR run`? Pipeline developer would definitely benefits from a faster startup. Symptom; -----------------; ```bash; time java -jar cromwell-49.jar run sub-flow.wdl -i input.json; ```. ```; real	1m12.833s; user	1m12.148s; sys	0m7.644s; ```. ```; $ cat input.json ; {; ""hello_and_goodbye.hello_and_goodbye_input"":""test1""; }; ```. Detail; ----------; backend: local. File: `sub-flow.wdl`; ```wdl; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451
https://github.com/broadinstitute/cromwell/issues/5452:783,Availability,error,error,783,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:5725,Deployability,configurat,configuration,5725,"mazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:4240,Energy Efficiency,adapt,adapted,4240,"ls(DefaultCredentialsProvider.java:92); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsClientHandlerUtils.createExecutionContext(AwsClientHandlerUtils.java:70); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.createExecutionContext(AwsSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:479,Modifiability,config,config,479,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:611,Modifiability,config,config,611,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:958,Modifiability,config,config,958,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1824,Modifiability,variab,variable,1824,"ich shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:2035,Modifiability,variab,variable,2035,"t, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97); cromwell_1 | 	at software.amazon.awssdk.auth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:2760,Modifiability,variab,variables,2760,"e(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97); cromwell_1 | 	at software.amazon.awssdk.auth.credentials.AwsCredentialsProviderChain.resolveCredentials(AwsCredentialsProviderChain.java:112); cromwell_1 | 	at software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider.resolveCredentials(DefaultCredentialsProvider.java:92); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsClientHandlerUtils.createExecutionContext(AwsClientHandlerUtils.java:70); cromwell_1 | 	at software.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:4240,Modifiability,adapt,adapted,4240,"ls(DefaultCredentialsProvider.java:92); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsClientHandlerUtils.createExecutionContext(AwsClientHandlerUtils.java:70); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.createExecutionContext(AwsSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:5725,Modifiability,config,configuration,5725,"mazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:205,Performance,scalab,scalability,205,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1184,Performance,load,load,1184," to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1733,Performance,load,load,1733," to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1944,Performance,load,load,1944,"at profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:2828,Performance,load,load,2828,"t variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97); cromwell_1 | 	at software.amazon.awssdk.auth.credentials.AwsCredentialsProviderChain.resolveCredentials(AwsCredentialsProviderChain.java:112); cromwell_1 | 	at software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider.resolveCredentials(DefaultCredentialsProvider.java:92); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsClientHandlerUtils.createExecutionContext(AwsClientHandlerUtils.java:70); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.createExecutionContext(AwsSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.ama",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:30,Security,authenticat,authentication,30,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:156,Security,authenticat,authentication,156,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:539,Security,authenticat,authentication,539,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1772,Security,Access,Access,1772,"ich shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1877,Security,access,accessKeyId,1877,"h the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:1983,Security,Access,Access,1983,"t, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97); cromwell_1 | 	at software.amazon.awssdk.auth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:2088,Security,access,accessKeyId,2088,"omwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): Unable to load credentials from service endpoint.]; cromwell_1 | 	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97); cromwell_1 | 	at software.amazon.awssdk.auth.credentials.AwsCredentialsProviderChain.resolveCredentials(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:4340,Security,validat,validateCredential,4340,"rUtils.createExecutionContext(AwsClientHandlerUtils.java:70); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.createExecutionContext(AwsSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categori",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:4588,Security,validat,validateCredential,4588,".handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:5770,Security,PASSWORD,PASSWORDS,5770,"mazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/issues/5452:4935,Usability,feedback,feedback,4935,"mazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452
https://github.com/broadinstitute/cromwell/pull/5453:302,Availability,error,error,302,"Dear Cromwell developers,. I am working on SQLite support at the moment. I have been successful in instantiating the database with all the correct tables. In other words the Liquibase migration seems to function fine. Unfortunately, the LiquibaseComparisonSpec keeps failing with a rather non-descript error:; ```; java.lang.NullPointerException was thrown.; java.lang.NullPointerException; 	at liquibase.structure.core.Index.setColumns(Index.java:118); 	at liquibase.snapshot.jvm.PrimaryKeySnapshotGenerator.snapshotObject(PrimaryKeySnapshotGenerator.java:80); 	at liquibase.snapshot.jvm.JdbcSnapshotGenerator.snapshot(JdbcSnapshotGenerator.java:66); ...; ```; If I write the database to a file, all the correct tables seem to be present (I might have missed one, but the database is certainly populated with a schema). Now I have been doing some digging and this is the database object that is created:; ```; database = {SQLiteDatabase@5510} ""null @ jdbc:sqlite::memory:""; systemTables = {HashSet@5517} size = 2; reservedWords = {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `def",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2390,Availability,error,error,2390,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2450,Availability,error,error,2450,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2496,Availability,error,error,2496,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2147,Deployability,configurat,configuration,2147,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2226,Deployability,configurat,configuration,2226,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2840,Deployability,configurat,configuration,2840,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2147,Modifiability,config,configuration,2147,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2226,Modifiability,config,configuration,2226,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2840,Modifiability,config,configuration,2840,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:991,Security,Hash,HashSet,991,"liquibase.structure.core.Index.setColumns(Index.java:118); 	at liquibase.snapshot.jvm.PrimaryKeySnapshotGenerator.snapshotObject(PrimaryKeySnapshotGenerator.java:80); 	at liquibase.snapshot.jvm.JdbcSnapshotGenerator.snapshot(JdbcSnapshotGenerator.java:66); ...; ```; If I write the database to a file, all the correct tables seem to be present (I might have missed one, but the database is certainly populated with a schema). Now I have been doing some digging and this is the database object that is created:; ```; database = {SQLiteDatabase@5510} ""null @ jdbc:sqlite::memory:""; systemTables = {HashSet@5517} size = 2; reservedWords = {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:1032,Security,Hash,HashSet,1032,"liquibase.structure.core.Index.setColumns(Index.java:118); 	at liquibase.snapshot.jvm.PrimaryKeySnapshotGenerator.snapshotObject(PrimaryKeySnapshotGenerator.java:80); 	at liquibase.snapshot.jvm.JdbcSnapshotGenerator.snapshot(JdbcSnapshotGenerator.java:66); ...; ```; If I write the database to a file, all the correct tables seem to be present (I might have missed one, but the database is certainly populated with a schema). Now I have been doing some digging and this is the database object that is created:; ```; database = {SQLiteDatabase@5510} ""null @ jdbc:sqlite::memory:""; systemTables = {HashSet@5517} size = 2; reservedWords = {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:1943,Security,Hash,HashMap,1943,"liquibase.structure.core.Index.setColumns(Index.java:118); 	at liquibase.snapshot.jvm.PrimaryKeySnapshotGenerator.snapshotObject(PrimaryKeySnapshotGenerator.java:80); 	at liquibase.snapshot.jvm.JdbcSnapshotGenerator.snapshot(JdbcSnapshotGenerator.java:66); ...; ```; If I write the database to a file, all the correct tables seem to be present (I might have missed one, but the database is certainly populated with a schema). Now I have been doing some digging and this is the database object that is created:; ```; database = {SQLiteDatabase@5510} ""null @ jdbc:sqlite::memory:""; systemTables = {HashSet@5517} size = 2; reservedWords = {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5453:2142,Testability,test,test,2142,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453
https://github.com/broadinstitute/cromwell/pull/5456:164,Availability,error,error,164,Shard's are also in the path. Also the zero `0` should be included in the attempt and shard regexes. In case the number of attempts is higher than 9.; I found this error while running the dev version of cromwell.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456
https://github.com/broadinstitute/cromwell/pull/5459:0,Integrability,Depend,Dependent,0,Dependent PR in firecloud-develop: https://github.com/broadinstitute/firecloud-develop/pull/2079,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5459
https://github.com/broadinstitute/cromwell/issues/5460:743,Availability,echo,echo,743,"Created in collaboration with: @TMiguelT. The OpenWDL spec states when interpolating a string in the command block:; > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. Ie:; - `string + null + string -> null`:. That is, if `str` is not defined (`null`), the following should resolve to null and empty:; ```; ~{'""--prefix"" ""' + str + '""'}; ```. Currently, it's resolving to `""` (a single double-quote). Eg: In the task:. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. A fix to this would reduce our usages of `if defined(name) then """" else """"`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5460
https://github.com/broadinstitute/cromwell/issues/5460:861,Energy Efficiency,reduce,reduce,861,"Created in collaboration with: @TMiguelT. The OpenWDL spec states when interpolating a string in the command block:; > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. Ie:; - `string + null + string -> null`:. That is, if `str` is not defined (`null`), the following should resolve to null and empty:; ```; ~{'""--prefix"" ""' + str + '""'}; ```. Currently, it's resolving to `""` (a single double-quote). Eg: In the task:. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. A fix to this would reduce our usages of `if defined(name) then """" else """"`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5460
https://github.com/broadinstitute/cromwell/pull/5461:0,Testability,test,test,0,test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5461
https://github.com/broadinstitute/cromwell/pull/5464:705,Availability,echo,echo,705,"Closes #5460 . Cromwell isn't correctly cascading `string + optional + string` behaviour. It seems that `string + optional` is evaluating to an empty string. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. That is, if `str` resolves to `None`, then this should resolve to `echo `. Instead this is currently resolving to `echo ""` (single quote).; ```; echo ~{'""--prefix"" ""' + str + '""'}; ```. ## Example. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. Without value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl ; # Job quotetest:NA:1 exited with return code -1; # STDERR: unexpected EOF while looking for matching '""'; ```. With value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl -i quotestest-inp.json ; # ""outputs"": {; # ""quotetest.out"": ""--prefix Hello""; # }; ```. ## This PR. Addresses the spec, by:. - Changing type checker: OptionalType<T> + T => Optional<T> (within the current rules).; - Optional + String => String else None",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464
https://github.com/broadinstitute/cromwell/pull/5464:753,Availability,echo,echo,753,"Closes #5460 . Cromwell isn't correctly cascading `string + optional + string` behaviour. It seems that `string + optional` is evaluating to an empty string. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. That is, if `str` resolves to `None`, then this should resolve to `echo `. Instead this is currently resolving to `echo ""` (single quote).; ```; echo ~{'""--prefix"" ""' + str + '""'}; ```. ## Example. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. Without value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl ; # Job quotetest:NA:1 exited with return code -1; # STDERR: unexpected EOF while looking for matching '""'; ```. With value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl -i quotestest-inp.json ; # ""outputs"": {; # ""quotetest.out"": ""--prefix Hello""; # }; ```. ## This PR. Addresses the spec, by:. - Changing type checker: OptionalType<T> + T => Optional<T> (within the current rules).; - Optional + String => String else None",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464
https://github.com/broadinstitute/cromwell/pull/5464:783,Availability,echo,echo,783,"Closes #5460 . Cromwell isn't correctly cascading `string + optional + string` behaviour. It seems that `string + optional` is evaluating to an empty string. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. That is, if `str` resolves to `None`, then this should resolve to `echo `. Instead this is currently resolving to `echo ""` (single quote).; ```; echo ~{'""--prefix"" ""' + str + '""'}; ```. ## Example. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. Without value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl ; # Job quotetest:NA:1 exited with return code -1; # STDERR: unexpected EOF while looking for matching '""'; ```. With value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl -i quotestest-inp.json ; # ""outputs"": {; # ""quotetest.out"": ""--prefix Hello""; # }; ```. ## This PR. Addresses the spec, by:. - Changing type checker: OptionalType<T> + T => Optional<T> (within the current rules).; - Optional + String => String else None",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464
https://github.com/broadinstitute/cromwell/pull/5464:921,Availability,echo,echo,921,"Closes #5460 . Cromwell isn't correctly cascading `string + optional + string` behaviour. It seems that `string + optional` is evaluating to an empty string. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. That is, if `str` resolves to `None`, then this should resolve to `echo `. Instead this is currently resolving to `echo ""` (single quote).; ```; echo ~{'""--prefix"" ""' + str + '""'}; ```. ## Example. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. Without value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl ; # Job quotetest:NA:1 exited with return code -1; # STDERR: unexpected EOF while looking for matching '""'; ```. With value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl -i quotestest-inp.json ; # ""outputs"": {; # ""quotetest.out"": ""--prefix Hello""; # }; ```. ## This PR. Addresses the spec, by:. - Changing type checker: OptionalType<T> + T => Optional<T> (within the current rules).; - Optional + String => String else None",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464
https://github.com/broadinstitute/cromwell/pull/5468:260,Deployability,integrat,integration,260,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468
https://github.com/broadinstitute/cromwell/pull/5468:420,Deployability,Update,Updated,420,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468
https://github.com/broadinstitute/cromwell/pull/5468:260,Integrability,integrat,integration,260,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468
https://github.com/broadinstitute/cromwell/pull/5468:344,Testability,log,logging,344,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468
https://github.com/broadinstitute/cromwell/pull/5468:195,Usability,simpl,simplifications,195,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468
https://github.com/broadinstitute/cromwell/pull/5468:247,Usability,simpl,simplify,247,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468
https://github.com/broadinstitute/cromwell/pull/5470:286,Integrability,message,message,286,"What do you mean ""metricable"" isn't a word?. \<looks quickly on google\>. ... ""I am **not** using [Metrizable](https://www.yourdictionary.com/metrizable)...!"". ---. Additional commentary and/or things worth sanity checking me on:. * We lose ""root workflow ID"" and ""bucket"" from the log message.; * We gain task name and hog group in the metric path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5470
https://github.com/broadinstitute/cromwell/pull/5470:207,Safety,sanity check,sanity checking,207,"What do you mean ""metricable"" isn't a word?. \<looks quickly on google\>. ... ""I am **not** using [Metrizable](https://www.yourdictionary.com/metrizable)...!"". ---. Additional commentary and/or things worth sanity checking me on:. * We lose ""root workflow ID"" and ""bucket"" from the log message.; * We gain task name and hog group in the metric path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5470
https://github.com/broadinstitute/cromwell/pull/5470:282,Testability,log,log,282,"What do you mean ""metricable"" isn't a word?. \<looks quickly on google\>. ... ""I am **not** using [Metrizable](https://www.yourdictionary.com/metrizable)...!"". ---. Additional commentary and/or things worth sanity checking me on:. * We lose ""root workflow ID"" and ""bucket"" from the log message.; * We gain task name and hog group in the metric path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5470
https://github.com/broadinstitute/cromwell/pull/5471:228,Deployability,update,update,228,"…lines. ^^ that title-split was made by github. Apparently github line-splitting has a sense of humour now?. ---. This test has burned me a few times recently, but nothing obvious in the code has changed for many months. Did we update the akka http library in the last few days?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471
https://github.com/broadinstitute/cromwell/pull/5471:119,Testability,test,test,119,"…lines. ^^ that title-split was made by github. Apparently github line-splitting has a sense of humour now?. ---. This test has burned me a few times recently, but nothing obvious in the code has changed for many months. Did we update the akka http library in the last few days?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471
https://github.com/broadinstitute/cromwell/issues/5476:718,Energy Efficiency,adapt,adapt,718,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful. The issue is related to https://github.com/broadinstitute/cromwell/issues/1695",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476
https://github.com/broadinstitute/cromwell/issues/5476:718,Modifiability,adapt,adapt,718,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful. The issue is related to https://github.com/broadinstitute/cromwell/issues/1695",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476
https://github.com/broadinstitute/cromwell/issues/5476:162,Performance,optimiz,optimizations,162,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful. The issue is related to https://github.com/broadinstitute/cromwell/issues/1695",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476
https://github.com/broadinstitute/cromwell/issues/5477:439,Integrability,depend,dependent,439,"Also posted in JIRA. Posted here for cross-reference by other users. The size function works correctly for. * absolute paths; * Relative paths from the execution directory. This means the size function works fine for any outputs. It fails however on inputs of a relative path, since it checks from the execution directory. Since nothing is in the execution directory when evaluating inputs, this always fails. The size function is context dependent. When calculating outputs, any relative paths should be evaluated from the cromwell execution directory. When calculating the size of inputs any relative paths should be evaluated from the CWD of cromwell (they are also localized that way).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5477
https://github.com/broadinstitute/cromwell/pull/5478:808,Integrability,depend,dependent,808,"Fixes #5477 . Calculating size of inputs does not work when the inputs are relative. Usually relative paths are calculated from the execution directory. This is fine for outputs, but not for inputs. This issue was discovered because it is extremely useful to have the size of inputs. Because this way you can guesstimate the resource requirements that should be put in the `runtime` section. Currently it works fine for absolute paths, so that can be used as a workaround. However: all BioWDL workflows are tested in git repos with inputs JSON files that use relative paths. So we can not test our changes on the CI. Also, some user might want to use relative paths in conjunction with `cromwell run`. This adds a `forInput` boolean to the expressionFunction classes so the size function can be made context-dependent. Currently this is only implemented for the SFSbackend which is presumably the only backend that suffers from this. ~~EDIT: This is now implemented for all backends. It is quite useful as resource estimates using size can be used to request specific VM sizes in the cloud.~~ Size for inputs is now calculated in the same way as inputs with relative paths are localized (from the CWD of the cromwell process). I added a test that tests this fixed behavior (sizerelativepath). Tests for the current behavior (sizeenginefunction) do not break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478
https://github.com/broadinstitute/cromwell/pull/5478:507,Testability,test,tested,507,"Fixes #5477 . Calculating size of inputs does not work when the inputs are relative. Usually relative paths are calculated from the execution directory. This is fine for outputs, but not for inputs. This issue was discovered because it is extremely useful to have the size of inputs. Because this way you can guesstimate the resource requirements that should be put in the `runtime` section. Currently it works fine for absolute paths, so that can be used as a workaround. However: all BioWDL workflows are tested in git repos with inputs JSON files that use relative paths. So we can not test our changes on the CI. Also, some user might want to use relative paths in conjunction with `cromwell run`. This adds a `forInput` boolean to the expressionFunction classes so the size function can be made context-dependent. Currently this is only implemented for the SFSbackend which is presumably the only backend that suffers from this. ~~EDIT: This is now implemented for all backends. It is quite useful as resource estimates using size can be used to request specific VM sizes in the cloud.~~ Size for inputs is now calculated in the same way as inputs with relative paths are localized (from the CWD of the cromwell process). I added a test that tests this fixed behavior (sizerelativepath). Tests for the current behavior (sizeenginefunction) do not break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478
https://github.com/broadinstitute/cromwell/pull/5478:589,Testability,test,test,589,"Fixes #5477 . Calculating size of inputs does not work when the inputs are relative. Usually relative paths are calculated from the execution directory. This is fine for outputs, but not for inputs. This issue was discovered because it is extremely useful to have the size of inputs. Because this way you can guesstimate the resource requirements that should be put in the `runtime` section. Currently it works fine for absolute paths, so that can be used as a workaround. However: all BioWDL workflows are tested in git repos with inputs JSON files that use relative paths. So we can not test our changes on the CI. Also, some user might want to use relative paths in conjunction with `cromwell run`. This adds a `forInput` boolean to the expressionFunction classes so the size function can be made context-dependent. Currently this is only implemented for the SFSbackend which is presumably the only backend that suffers from this. ~~EDIT: This is now implemented for all backends. It is quite useful as resource estimates using size can be used to request specific VM sizes in the cloud.~~ Size for inputs is now calculated in the same way as inputs with relative paths are localized (from the CWD of the cromwell process). I added a test that tests this fixed behavior (sizerelativepath). Tests for the current behavior (sizeenginefunction) do not break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478
https://github.com/broadinstitute/cromwell/pull/5478:1237,Testability,test,test,1237,"Fixes #5477 . Calculating size of inputs does not work when the inputs are relative. Usually relative paths are calculated from the execution directory. This is fine for outputs, but not for inputs. This issue was discovered because it is extremely useful to have the size of inputs. Because this way you can guesstimate the resource requirements that should be put in the `runtime` section. Currently it works fine for absolute paths, so that can be used as a workaround. However: all BioWDL workflows are tested in git repos with inputs JSON files that use relative paths. So we can not test our changes on the CI. Also, some user might want to use relative paths in conjunction with `cromwell run`. This adds a `forInput` boolean to the expressionFunction classes so the size function can be made context-dependent. Currently this is only implemented for the SFSbackend which is presumably the only backend that suffers from this. ~~EDIT: This is now implemented for all backends. It is quite useful as resource estimates using size can be used to request specific VM sizes in the cloud.~~ Size for inputs is now calculated in the same way as inputs with relative paths are localized (from the CWD of the cromwell process). I added a test that tests this fixed behavior (sizerelativepath). Tests for the current behavior (sizeenginefunction) do not break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478
https://github.com/broadinstitute/cromwell/pull/5478:1247,Testability,test,tests,1247,"Fixes #5477 . Calculating size of inputs does not work when the inputs are relative. Usually relative paths are calculated from the execution directory. This is fine for outputs, but not for inputs. This issue was discovered because it is extremely useful to have the size of inputs. Because this way you can guesstimate the resource requirements that should be put in the `runtime` section. Currently it works fine for absolute paths, so that can be used as a workaround. However: all BioWDL workflows are tested in git repos with inputs JSON files that use relative paths. So we can not test our changes on the CI. Also, some user might want to use relative paths in conjunction with `cromwell run`. This adds a `forInput` boolean to the expressionFunction classes so the size function can be made context-dependent. Currently this is only implemented for the SFSbackend which is presumably the only backend that suffers from this. ~~EDIT: This is now implemented for all backends. It is quite useful as resource estimates using size can be used to request specific VM sizes in the cloud.~~ Size for inputs is now calculated in the same way as inputs with relative paths are localized (from the CWD of the cromwell process). I added a test that tests this fixed behavior (sizerelativepath). Tests for the current behavior (sizeenginefunction) do not break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478
https://github.com/broadinstitute/cromwell/pull/5478:1293,Testability,Test,Tests,1293,"Fixes #5477 . Calculating size of inputs does not work when the inputs are relative. Usually relative paths are calculated from the execution directory. This is fine for outputs, but not for inputs. This issue was discovered because it is extremely useful to have the size of inputs. Because this way you can guesstimate the resource requirements that should be put in the `runtime` section. Currently it works fine for absolute paths, so that can be used as a workaround. However: all BioWDL workflows are tested in git repos with inputs JSON files that use relative paths. So we can not test our changes on the CI. Also, some user might want to use relative paths in conjunction with `cromwell run`. This adds a `forInput` boolean to the expressionFunction classes so the size function can be made context-dependent. Currently this is only implemented for the SFSbackend which is presumably the only backend that suffers from this. ~~EDIT: This is now implemented for all backends. It is quite useful as resource estimates using size can be used to request specific VM sizes in the cloud.~~ Size for inputs is now calculated in the same way as inputs with relative paths are localized (from the CWD of the cromwell process). I added a test that tests this fixed behavior (sizerelativepath). Tests for the current behavior (sizeenginefunction) do not break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478
https://github.com/broadinstitute/cromwell/issues/5479:769,Availability,echo,echo,769,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call ‘hello’ doesn’t have an output ‘say_hello’ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479
https://github.com/broadinstitute/cromwell/issues/5479:859,Availability,error,error,859,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call ‘hello’ doesn’t have an output ‘say_hello’ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479
https://github.com/broadinstitute/cromwell/issues/5479:183,Security,access,access,183,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call ‘hello’ doesn’t have an output ‘say_hello’ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479
https://github.com/broadinstitute/cromwell/issues/5479:97,Usability,simpl,simple,97,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call ‘hello’ doesn’t have an output ‘say_hello’ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479
https://github.com/broadinstitute/cromwell/pull/5481:169,Deployability,deploy,deployments,169,- Makes carboniting and metadata deletion a singleton activity in our centaur testing framework.; - Turns on metadata carboniting and deletion assertions in horicromtal deployments,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5481
https://github.com/broadinstitute/cromwell/pull/5481:78,Testability,test,testing,78,- Makes carboniting and metadata deletion a singleton activity in our centaur testing framework.; - Turns on metadata carboniting and deletion assertions in horicromtal deployments,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5481
https://github.com/broadinstitute/cromwell/pull/5481:143,Testability,assert,assertions,143,- Makes carboniting and metadata deletion a singleton activity in our centaur testing framework.; - Turns on metadata carboniting and deletion assertions in horicromtal deployments,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5481
https://github.com/broadinstitute/cromwell/issues/5482:1175,Deployability,release,release,1175,"Many tools get input files and create the output files in the same folder as the input instead of the execution folder, as a result, I have nothing in execution folder but in the input folder I have a newly created file.; For instance, if I have a task; ```; task create_reference_fai {; input {; File reference; }. String name = sub(basename(reference, "".fasta""), "".fa"", """"). command { ; samtools faidx ~{reference}; }; runtime {; docker: ""quay.io/biocontainers/samtools@sha256:97b9627711c16125fe1b57cf8745396064fd88ebeff6ab00cf6a68aeacecfcda"" #1.2-0; }. output {; File out = name+"".fai""; }; }; ```; It will fail, because samtools faidx created the "".fai"" file in the Input folder as it is the same folder that is reference.fast ( /data/cromwell-executions/index_reference/8b9eaeee-bb8e-4754-9a6b-a853c772cf03/call-create_reference_fai/inputs/1627505414/ in my case) instead of execution ( /data/cromwell-executions/index_reference/8b9eaeee-bb8e-4754-9a6b-a853c772cf03/call-create_reference_fai/execution/ in my case) folder. I believe that Cromwell should control for this as it is against logic to have newly created files in the Input folder. Note: I use latest cromwell release with local backend on Ubuntu 18.10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482
https://github.com/broadinstitute/cromwell/issues/5482:1092,Testability,log,logic,1092,"Many tools get input files and create the output files in the same folder as the input instead of the execution folder, as a result, I have nothing in execution folder but in the input folder I have a newly created file.; For instance, if I have a task; ```; task create_reference_fai {; input {; File reference; }. String name = sub(basename(reference, "".fasta""), "".fa"", """"). command { ; samtools faidx ~{reference}; }; runtime {; docker: ""quay.io/biocontainers/samtools@sha256:97b9627711c16125fe1b57cf8745396064fd88ebeff6ab00cf6a68aeacecfcda"" #1.2-0; }. output {; File out = name+"".fai""; }; }; ```; It will fail, because samtools faidx created the "".fai"" file in the Input folder as it is the same folder that is reference.fast ( /data/cromwell-executions/index_reference/8b9eaeee-bb8e-4754-9a6b-a853c772cf03/call-create_reference_fai/inputs/1627505414/ in my case) instead of execution ( /data/cromwell-executions/index_reference/8b9eaeee-bb8e-4754-9a6b-a853c772cf03/call-create_reference_fai/execution/ in my case) folder. I believe that Cromwell should control for this as it is against logic to have newly created files in the Input folder. Note: I use latest cromwell release with local backend on Ubuntu 18.10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482
https://github.com/broadinstitute/cromwell/pull/5483:42,Deployability,upgrade,upgrade,42,"Dump the dependency tree before and after upgrade: ; ```; sbt coursierDependencyTree > coursier_dependency_tree_$(date +%s).txt; ```. look for all the `:netty-codec:`s, with a dash of `sed` and `sort -u` to squash duplicates:. before:; ```; $ grep ':netty-codec:' coursier_dependency_tree_1586897217.txt | sed -E 's/.*:(.*)/\1/' | sort -u; 4.1.30.Final -> 4.1.33.Final; 4.1.32.Final -> 4.1.33.Final; 4.1.33.Final; $; ```. after:; ```; $ grep ':netty-codec:' coursier_dependency_tree_1586898071.txt | sed -E 's/.*:(.*)/\1/' | sort -u; 4.1.32.Final -> 4.1.46.Final; 4.1.46.Final; $; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5483
https://github.com/broadinstitute/cromwell/pull/5483:9,Integrability,depend,dependency,9,"Dump the dependency tree before and after upgrade: ; ```; sbt coursierDependencyTree > coursier_dependency_tree_$(date +%s).txt; ```. look for all the `:netty-codec:`s, with a dash of `sed` and `sort -u` to squash duplicates:. before:; ```; $ grep ':netty-codec:' coursier_dependency_tree_1586897217.txt | sed -E 's/.*:(.*)/\1/' | sort -u; 4.1.30.Final -> 4.1.33.Final; 4.1.32.Final -> 4.1.33.Final; 4.1.33.Final; $; ```. after:; ```; $ grep ':netty-codec:' coursier_dependency_tree_1586898071.txt | sed -E 's/.*:(.*)/\1/' | sort -u; 4.1.32.Final -> 4.1.46.Final; 4.1.46.Final; $; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5483
https://github.com/broadinstitute/cromwell/pull/5485:653,Integrability,interface,interface,653,"Early access MVP version of the extractor script, to make sure the extractor, digester and comparer can all be singing from the same hymn sheet as soon as possible. Also to course correct as soon as possible if I've gone in completely the wrong direction, I guess. Featuring:; * Extraction and upload of:; * Workflow metadata; * Operations metadata (PAPI v2alpha1). Not yet implemented; coming soon in part 2 (or 3 (or ...)):; * Subworkflows; * Other operations metadata flavors; * Cromwell code dump upload; * Config dump upload. Especially interested in early feedback on:; * Coding styles (if we want to standardize on one for these scripts); * User interface (eg if you try to use it, is it intuitive?); * If I got the directory structure completely wrong. Changes I suspect might happen in subsequent PRs but I'm reluctant to make in this one-script MVP:; * This function could be re-used, can we extract it to a shared location?; * Could we use a data structure to store this type of information between scripts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5485
https://github.com/broadinstitute/cromwell/pull/5485:511,Modifiability,Config,Config,511,"Early access MVP version of the extractor script, to make sure the extractor, digester and comparer can all be singing from the same hymn sheet as soon as possible. Also to course correct as soon as possible if I've gone in completely the wrong direction, I guess. Featuring:; * Extraction and upload of:; * Workflow metadata; * Operations metadata (PAPI v2alpha1). Not yet implemented; coming soon in part 2 (or 3 (or ...)):; * Subworkflows; * Other operations metadata flavors; * Cromwell code dump upload; * Config dump upload. Especially interested in early feedback on:; * Coding styles (if we want to standardize on one for these scripts); * User interface (eg if you try to use it, is it intuitive?); * If I got the directory structure completely wrong. Changes I suspect might happen in subsequent PRs but I'm reluctant to make in this one-script MVP:; * This function could be re-used, can we extract it to a shared location?; * Could we use a data structure to store this type of information between scripts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5485
https://github.com/broadinstitute/cromwell/pull/5485:6,Security,access,access,6,"Early access MVP version of the extractor script, to make sure the extractor, digester and comparer can all be singing from the same hymn sheet as soon as possible. Also to course correct as soon as possible if I've gone in completely the wrong direction, I guess. Featuring:; * Extraction and upload of:; * Workflow metadata; * Operations metadata (PAPI v2alpha1). Not yet implemented; coming soon in part 2 (or 3 (or ...)):; * Subworkflows; * Other operations metadata flavors; * Cromwell code dump upload; * Config dump upload. Especially interested in early feedback on:; * Coding styles (if we want to standardize on one for these scripts); * User interface (eg if you try to use it, is it intuitive?); * If I got the directory structure completely wrong. Changes I suspect might happen in subsequent PRs but I'm reluctant to make in this one-script MVP:; * This function could be re-used, can we extract it to a shared location?; * Could we use a data structure to store this type of information between scripts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5485
https://github.com/broadinstitute/cromwell/pull/5485:562,Usability,feedback,feedback,562,"Early access MVP version of the extractor script, to make sure the extractor, digester and comparer can all be singing from the same hymn sheet as soon as possible. Also to course correct as soon as possible if I've gone in completely the wrong direction, I guess. Featuring:; * Extraction and upload of:; * Workflow metadata; * Operations metadata (PAPI v2alpha1). Not yet implemented; coming soon in part 2 (or 3 (or ...)):; * Subworkflows; * Other operations metadata flavors; * Cromwell code dump upload; * Config dump upload. Especially interested in early feedback on:; * Coding styles (if we want to standardize on one for these scripts); * User interface (eg if you try to use it, is it intuitive?); * If I got the directory structure completely wrong. Changes I suspect might happen in subsequent PRs but I'm reluctant to make in this one-script MVP:; * This function could be re-used, can we extract it to a shared location?; * Could we use a data structure to store this type of information between scripts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5485
https://github.com/broadinstitute/cromwell/pull/5485:695,Usability,intuit,intuitive,695,"Early access MVP version of the extractor script, to make sure the extractor, digester and comparer can all be singing from the same hymn sheet as soon as possible. Also to course correct as soon as possible if I've gone in completely the wrong direction, I guess. Featuring:; * Extraction and upload of:; * Workflow metadata; * Operations metadata (PAPI v2alpha1). Not yet implemented; coming soon in part 2 (or 3 (or ...)):; * Subworkflows; * Other operations metadata flavors; * Cromwell code dump upload; * Config dump upload. Especially interested in early feedback on:; * Coding styles (if we want to standardize on one for these scripts); * User interface (eg if you try to use it, is it intuitive?); * If I got the directory structure completely wrong. Changes I suspect might happen in subsequent PRs but I'm reluctant to make in this one-script MVP:; * This function could be re-used, can we extract it to a shared location?; * Could we use a data structure to store this type of information between scripts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5485
https://github.com/broadinstitute/cromwell/pull/5486:77,Testability,test,test,77,Note to reviewers:; * The majority of the content of this PR is two metadata test cases. Feel free to skip over those. Reminder to myself: . - [x] Rebase onto develop when.`cjl_6346_extractor` merged. Question for pythonistas:; * ~Is there a more canonical way to lay out python test cases~. A thought for later:; * Can we add python tests to our travis CI?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5486
https://github.com/broadinstitute/cromwell/pull/5486:279,Testability,test,test,279,Note to reviewers:; * The majority of the content of this PR is two metadata test cases. Feel free to skip over those. Reminder to myself: . - [x] Rebase onto develop when.`cjl_6346_extractor` merged. Question for pythonistas:; * ~Is there a more canonical way to lay out python test cases~. A thought for later:; * Can we add python tests to our travis CI?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5486
https://github.com/broadinstitute/cromwell/pull/5486:334,Testability,test,tests,334,Note to reviewers:; * The majority of the content of this PR is two metadata test cases. Feel free to skip over those. Reminder to myself: . - [x] Rebase onto develop when.`cjl_6346_extractor` merged. Question for pythonistas:; * ~Is there a more canonical way to lay out python test cases~. A thought for later:; * Can we add python tests to our travis CI?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5486
https://github.com/broadinstitute/cromwell/pull/5488:13,Testability,test,tests,13,"Only run sbt tests, not the full CI suite",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5488
https://github.com/broadinstitute/cromwell/issues/5490:257,Deployability,install,installed,257,"Hi there,. Just to make sure there is no duplication of effort: I have a [branch](https://github.com/rhpvorderman/cromwell/tree/sqlite) on which I am working on sqlite support for cromwell. There are some difficulties:; - The liquibase dependency currently installed in cromwell crashes when using sqlite (liquibase 3.6.3) ; - Higher versions of liquibase fix this problem, but cause other problems. Using the newest version causes dependency clashes in the jar.; - Liquibase does not support a way for defining unique constraints when creating tables. Currently all tables get their contstaints added with addUniqueConstraint, however in SQLite constraints have to be defined at table creation. Foreign key constraints can be applied at table creation, so luckily there are no problems here.; - To solve the above problem I have a PR open at liquibase https://github.com/liquibase/liquibase/pull/1059. It may take some time before this is reviewed. Even after liquibase has the necessary support, it will take some time to make sure everything in cromwell works with this newest version of liquibase. So this will take some time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490
https://github.com/broadinstitute/cromwell/issues/5490:236,Integrability,depend,dependency,236,"Hi there,. Just to make sure there is no duplication of effort: I have a [branch](https://github.com/rhpvorderman/cromwell/tree/sqlite) on which I am working on sqlite support for cromwell. There are some difficulties:; - The liquibase dependency currently installed in cromwell crashes when using sqlite (liquibase 3.6.3) ; - Higher versions of liquibase fix this problem, but cause other problems. Using the newest version causes dependency clashes in the jar.; - Liquibase does not support a way for defining unique constraints when creating tables. Currently all tables get their contstaints added with addUniqueConstraint, however in SQLite constraints have to be defined at table creation. Foreign key constraints can be applied at table creation, so luckily there are no problems here.; - To solve the above problem I have a PR open at liquibase https://github.com/liquibase/liquibase/pull/1059. It may take some time before this is reviewed. Even after liquibase has the necessary support, it will take some time to make sure everything in cromwell works with this newest version of liquibase. So this will take some time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490
https://github.com/broadinstitute/cromwell/issues/5490:432,Integrability,depend,dependency,432,"Hi there,. Just to make sure there is no duplication of effort: I have a [branch](https://github.com/rhpvorderman/cromwell/tree/sqlite) on which I am working on sqlite support for cromwell. There are some difficulties:; - The liquibase dependency currently installed in cromwell crashes when using sqlite (liquibase 3.6.3) ; - Higher versions of liquibase fix this problem, but cause other problems. Using the newest version causes dependency clashes in the jar.; - Liquibase does not support a way for defining unique constraints when creating tables. Currently all tables get their contstaints added with addUniqueConstraint, however in SQLite constraints have to be defined at table creation. Foreign key constraints can be applied at table creation, so luckily there are no problems here.; - To solve the above problem I have a PR open at liquibase https://github.com/liquibase/liquibase/pull/1059. It may take some time before this is reviewed. Even after liquibase has the necessary support, it will take some time to make sure everything in cromwell works with this newest version of liquibase. So this will take some time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490
https://github.com/broadinstitute/cromwell/pull/5493:178,Testability,test,test,178,Headline changes:. * Moves operation analysis functions from `argument_regex.py` and `extractor.py` into their own lib file; * Also moved the `test_argument_regex.py` file into `test/lib/` alongside operation_id tests and a helper function file; * Creates a `PapiClients` class to abstract the details of operations metadata retrieval; * Added equivalent forkjoin metadata tests for v1 and v2beta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5493
https://github.com/broadinstitute/cromwell/pull/5493:212,Testability,test,tests,212,Headline changes:. * Moves operation analysis functions from `argument_regex.py` and `extractor.py` into their own lib file; * Also moved the `test_argument_regex.py` file into `test/lib/` alongside operation_id tests and a helper function file; * Creates a `PapiClients` class to abstract the details of operations metadata retrieval; * Added equivalent forkjoin metadata tests for v1 and v2beta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5493
https://github.com/broadinstitute/cromwell/pull/5493:373,Testability,test,tests,373,Headline changes:. * Moves operation analysis functions from `argument_regex.py` and `extractor.py` into their own lib file; * Also moved the `test_argument_regex.py` file into `test/lib/` alongside operation_id tests and a helper function file; * Creates a `PapiClients` class to abstract the details of operations metadata retrieval; * Added equivalent forkjoin metadata tests for v1 and v2beta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5493
https://github.com/broadinstitute/cromwell/pull/5494:343,Availability,error,errors,343,"Relevant OpenWDL PR: ~~https://github.com/openwdl/wdl/pull/229~~ (rebased into) https://github.com/openwdl/wdl/pull/366. Adding a `sep` function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:607,Availability,error,errors,607,"Relevant OpenWDL PR: ~~https://github.com/openwdl/wdl/pull/229~~ (rebased into) https://github.com/openwdl/wdl/pull/366. Adding a `sep` function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1144,Availability,error,error,1144," function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.v",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1158,Availability,error,error,1158,".com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1348,Availability,error,error,1348,"errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1381,Availability,Error,ErrorOr,1381,"y having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1389,Availability,Error,ErrorOr,1389,"roblems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forComm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1471,Availability,error,error,1471," the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInsta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1622,Availability,error,error,1622,"teValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the foll",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1657,Availability,Error,ErrorOr,1657,"xpressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1665,Availability,Error,ErrorOr,1665,"alueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1746,Availability,error,error,1746,"tions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1896,Availability,error,error,1896,"es[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2001,Availability,error,error,2001,"es[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2036,Availability,Error,ErrorOr,2036,"ue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOpt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2044,Availability,Error,ErrorOr,2044,"ng(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2145,Availability,Error,ErrorOr,2145,":. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2153,Availability,Error,ErrorOr,2153,"rror] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2633,Availability,error,error,2633,"r] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiatio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2647,Availability,error,error,2647,"om.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2905,Availability,error,error,2905,"[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3148,Availability,error,error,3148,"pression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".inval",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3159,Availability,error,error,3159,"pression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".inval",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3418,Availability,error,error,3418,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3661,Availability,error,error,3661,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3672,Availability,error,error,3672,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3683,Availability,error,errors,3683,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1370,Security,validat,validation,1370," but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1507,Security,Validat,Validated,1507,"om `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1646,Security,validat,validation,1646,"value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:1782,Security,Validat,Validated,1782,"val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/Biscayne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2025,Security,validat,validation,2025,"aluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstant",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:2134,Security,validat,validation,2134,"wing error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3916,Security,validat,validateParamType,3916,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3958,Security,validat,validateParamType,3958,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3715,Testability,test,tests,3715,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5494:3747,Testability,test,tests,3747,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494
https://github.com/broadinstitute/cromwell/pull/5495:85,Integrability,interface,interfaces,85,"This is a duplicate of #5478 but with a less intrusive code change. I bypass all the interfaces by setting a variable using a method. ~~It is gruesome, but much less lines need to be changed. I have some changes that it is slightly nicer. It still involves a `var` but now it is set only for the sfsBackend.~~ EDIT: It now looks quite OK. Only two lines of code in cromwell's general code. The rest of the bug is fixed specifically for the sfsBackend only. Most of the lines changed are the extra tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495
https://github.com/broadinstitute/cromwell/pull/5495:109,Modifiability,variab,variable,109,"This is a duplicate of #5478 but with a less intrusive code change. I bypass all the interfaces by setting a variable using a method. ~~It is gruesome, but much less lines need to be changed. I have some changes that it is slightly nicer. It still involves a `var` but now it is set only for the sfsBackend.~~ EDIT: It now looks quite OK. Only two lines of code in cromwell's general code. The rest of the bug is fixed specifically for the sfsBackend only. Most of the lines changed are the extra tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495
https://github.com/broadinstitute/cromwell/pull/5495:497,Testability,test,tests,497,"This is a duplicate of #5478 but with a less intrusive code change. I bypass all the interfaces by setting a variable using a method. ~~It is gruesome, but much less lines need to be changed. I have some changes that it is slightly nicer. It still involves a `var` but now it is set only for the sfsBackend.~~ EDIT: It now looks quite OK. Only two lines of code in cromwell's general code. The rest of the bug is fixed specifically for the sfsBackend only. Most of the lines changed are the extra tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495
https://github.com/broadinstitute/cromwell/issues/5499:287,Availability,error,error,287,"https://github.com/broadinstitute/cromwell/blob/4c386f80ab83e78c953e412382526a47d6b36ba4/cromwell.example.backends/TES.conf#L32. There is a missing `}` in this file. It also doesn't seem to work if used with Funnel (https://github.com/ohsu-comp-bio/funnel) as TES back end, although the error happens after the TES execution is complete:. ```; [INFO] [04/28/2020 13:37:00.957] [cromwell-system-akka.dispatchers.backend-dispatcher-63] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-18b18b73-17d7-4569-a58f-e44af1ec43a5/WorkflowExecutionActor-18b18b73-17d7-4569-a58f-e44af1ec43a5/18b18b73-17d7-4569-a58f-e44af1ec43a5-EngineJobExecutionActor-md5_.cwl:NA:1/18b18b73-17d7-4569-a58f-e44af1ec43a5-BackendJobExecutionActor-md5_.cwl:NA:1/TesAsyncBackendJobExecutionActor] TesAsyncBackendJobExecutionActor [UUID(18b18b73)md5_.cwl:NA:1]: Status change from - to Complete; [INFO] [04/28/2020 13:37:01.083] [cromwell-system-akka.dispatchers.engine-dispatcher-41] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 18b18b73-17d7-4569-a58f-e44af1ec43a5 failed (during ExecutingWorkflowState): Job md5_.cwl:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /Users/asenf/devel/alexandersenf/cromwelltes/cromwell-executions/md5_.cwl/18b18b73-17d7-4569-a58f-e44af1ec43a5/call-md5_.cwl/execution/stderr.; [First 300 bytes]:mkdir: cannot create directory '/cromwell-executions/md5_.cwl/18b18b73-17d7-4569-a58f-e44af1ec43a5/call-md5_.cwl/tmp.7db856a3': Read-only file system; chmod: cannot access '': No such file or directory; mkfifo: cannot create fifo '/out.1': Read-only file system; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5499
https://github.com/broadinstitute/cromwell/issues/5499:1730,Security,access,access,1730,"https://github.com/broadinstitute/cromwell/blob/4c386f80ab83e78c953e412382526a47d6b36ba4/cromwell.example.backends/TES.conf#L32. There is a missing `}` in this file. It also doesn't seem to work if used with Funnel (https://github.com/ohsu-comp-bio/funnel) as TES back end, although the error happens after the TES execution is complete:. ```; [INFO] [04/28/2020 13:37:00.957] [cromwell-system-akka.dispatchers.backend-dispatcher-63] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-18b18b73-17d7-4569-a58f-e44af1ec43a5/WorkflowExecutionActor-18b18b73-17d7-4569-a58f-e44af1ec43a5/18b18b73-17d7-4569-a58f-e44af1ec43a5-EngineJobExecutionActor-md5_.cwl:NA:1/18b18b73-17d7-4569-a58f-e44af1ec43a5-BackendJobExecutionActor-md5_.cwl:NA:1/TesAsyncBackendJobExecutionActor] TesAsyncBackendJobExecutionActor [UUID(18b18b73)md5_.cwl:NA:1]: Status change from - to Complete; [INFO] [04/28/2020 13:37:01.083] [cromwell-system-akka.dispatchers.engine-dispatcher-41] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 18b18b73-17d7-4569-a58f-e44af1ec43a5 failed (during ExecutingWorkflowState): Job md5_.cwl:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /Users/asenf/devel/alexandersenf/cromwelltes/cromwell-executions/md5_.cwl/18b18b73-17d7-4569-a58f-e44af1ec43a5/call-md5_.cwl/execution/stderr.; [First 300 bytes]:mkdir: cannot create directory '/cromwell-executions/md5_.cwl/18b18b73-17d7-4569-a58f-e44af1ec43a5/call-md5_.cwl/tmp.7db856a3': Read-only file system; chmod: cannot access '': No such file or directory; mkfifo: cannot create fifo '/out.1': Read-only file system; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5499
https://github.com/broadinstitute/cromwell/pull/5502:250,Testability,test,tests,250,"Our Python scripts can only be run by Python version >= 3.6, because we use string interpolation which was introduced in Python 3.6. But Travis environment is Ubuntu 16.04 Xenial LTS, which officially supports only Python <=3.5.; Thus running Python tests in Docker container.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5502
https://github.com/broadinstitute/cromwell/issues/5503:732,Energy Efficiency,meter,meters,732,"Hi - If I submit ~100 or so jobs to a cromwell server, is there a way I can limit the number of workflows that get fully executed at a time? . I have it restricted as 'concurrent-job-limit = 10' for the backend, and even though cromwell is only allowing 10 jobs to be running at any point, it seems to be going through all hundred jobs and running parts of each at a time rather than getting through 10 complete workflows. So at this point, I've got a hundred partially completed workflows rather than any number of fully completed workflows. I'd prefer to get some number of completed workflows through before dispatching new jobs. Anything I can do here other than having my own dispatcher to the cromwell server that proactively meters the jobs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503
https://github.com/broadinstitute/cromwell/issues/5503:168,Performance,concurren,concurrent-job-limit,168,"Hi - If I submit ~100 or so jobs to a cromwell server, is there a way I can limit the number of workflows that get fully executed at a time? . I have it restricted as 'concurrent-job-limit = 10' for the backend, and even though cromwell is only allowing 10 jobs to be running at any point, it seems to be going through all hundred jobs and running parts of each at a time rather than getting through 10 complete workflows. So at this point, I've got a hundred partially completed workflows rather than any number of fully completed workflows. I'd prefer to get some number of completed workflows through before dispatching new jobs. Anything I can do here other than having my own dispatcher to the cromwell server that proactively meters the jobs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503
https://github.com/broadinstitute/cromwell/pull/5504:65,Modifiability,config,configured,65,Makes ~ 2 * &lt;base&gt;^2 rows of metadata (~80000 as currently configured) in one burst. I Intentionally did not write a .test for this as I doubt Travis would survive. Not sure where this should live.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5504
https://github.com/broadinstitute/cromwell/pull/5504:124,Testability,test,test,124,Makes ~ 2 * &lt;base&gt;^2 rows of metadata (~80000 as currently configured) in one burst. I Intentionally did not write a .test for this as I doubt Travis would survive. Not sure where this should live.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5504
https://github.com/broadinstitute/cromwell/pull/5509:131,Testability,test,testing,131,Non-summarizable metadata (>>> 99% of real world metadata) uses the fast insertion path. Currently includes some spamming code for testing that definitely should not be merged.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509
https://github.com/broadinstitute/cromwell/issues/5511:837,Deployability,configurat,configuration,837,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:977,Deployability,pipeline,pipeline,977,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:990,Deployability,pipeline,pipeline,990,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:1017,Deployability,pipeline,pipeline,1017,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:837,Modifiability,config,configuration,837,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:1379,Modifiability,variab,variables,1379,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:882,Security,PASSWORD,PASSWORDS,882,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/issues/5511:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511
https://github.com/broadinstitute/cromwell/pull/5512:107,Performance,cache,cache,107,"Inspired by hog groups and recent production conflagrations, allow for workflows to share a blacklist call cache as specified by a workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5512
https://github.com/broadinstitute/cromwell/pull/5513:22,Performance,cache,cache,22,"Blacklists individual cache hits that fail copying for whatever reason. This is in addition to the existing bucket-based 403 blacklisting, not instead of it. Needs tests and docs. Apologies for the diff noise of `CallCachingEntryId` changing subprojects. I would also like to rename this type at some point but I'm holding off for now since that would create even more noise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5513
https://github.com/broadinstitute/cromwell/pull/5513:164,Testability,test,tests,164,"Blacklists individual cache hits that fail copying for whatever reason. This is in addition to the existing bucket-based 403 blacklisting, not instead of it. Needs tests and docs. Apologies for the diff noise of `CallCachingEntryId` changing subprojects. I would also like to rename this type at some point but I'm holding off for now since that would create even more noise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5513
https://github.com/broadinstitute/cromwell/pull/5515:192,Availability,Ping,Pinging,192,"This PR documents how to use the singularity cache in a way that every image only gets pulled once. It also documents why the `--containall` flag should be used with singularity at all times. Pinging @illusional @TMiguelT and @vsoch as heavy singularity promotors. What do you think of this?. EDIT: I cancelled the tests, as this is documentation and should not influence the Cromwell code. EDIT2: Fixes #5063",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515
https://github.com/broadinstitute/cromwell/pull/5515:45,Performance,cache,cache,45,"This PR documents how to use the singularity cache in a way that every image only gets pulled once. It also documents why the `--containall` flag should be used with singularity at all times. Pinging @illusional @TMiguelT and @vsoch as heavy singularity promotors. What do you think of this?. EDIT: I cancelled the tests, as this is documentation and should not influence the Cromwell code. EDIT2: Fixes #5063",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515
https://github.com/broadinstitute/cromwell/pull/5515:315,Testability,test,tests,315,"This PR documents how to use the singularity cache in a way that every image only gets pulled once. It also documents why the `--containall` flag should be used with singularity at all times. Pinging @illusional @TMiguelT and @vsoch as heavy singularity promotors. What do you think of this?. EDIT: I cancelled the tests, as this is documentation and should not influence the Cromwell code. EDIT2: Fixes #5063",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515
https://github.com/broadinstitute/cromwell/pull/5516:56,Availability,error,error,56,"1. When the AWS backend job actually failed with memory error, instead of returning that error code the commandScript returned zero. Fixed it to return the error code so that it a failed job in batch.; 2. Added AWS-EFS expression post mapping function so the output expressions with files with relative paths get mapped to the correct path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516
https://github.com/broadinstitute/cromwell/pull/5516:89,Availability,error,error,89,"1. When the AWS backend job actually failed with memory error, instead of returning that error code the commandScript returned zero. Fixed it to return the error code so that it a failed job in batch.; 2. Added AWS-EFS expression post mapping function so the output expressions with files with relative paths get mapped to the correct path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516
https://github.com/broadinstitute/cromwell/pull/5516:156,Availability,error,error,156,"1. When the AWS backend job actually failed with memory error, instead of returning that error code the commandScript returned zero. Fixed it to return the error code so that it a failed job in batch.; 2. Added AWS-EFS expression post mapping function so the output expressions with files with relative paths get mapped to the correct path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516
https://github.com/broadinstitute/cromwell/pull/5518:94,Testability,test,testing,94,Combines work for two tickets that was changing the same bits of code. Anxiously awaiting dev testing results 🤞,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518
https://github.com/broadinstitute/cromwell/pull/5523:800,Availability,avail,available,800,"This implements the specification for unspecified task inputs https://github.com/openwdl/wdl/pull/359 . . It has been backported to wdl 1.0 because it enables useful functionality (I.e. allowing nested optional inputs to be defined if this is specified in the meta section of the workflow). . It may break some workflows that have their required inputs nested deeply inside subworkflows and tasks, but Broad's own WDL linter for IntelliJ has been warning against this behavior for a while now. . To me it always was a bit weird that Cromwell allowed `""my_workflow.my_subworkflow.my_required_input"": 5` in the input because it was required, but not `""my_workflow.my_subworkflow.my_optional_input"": 5` because it was optional. Now both are not allowed by default. And the optional nested input is only available after explicitly enabling it in the top-level workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523
https://github.com/broadinstitute/cromwell/issues/5524:510,Testability,test,test,510,"Thanks for your work with Cromwell.; I think below may be not a bug.; I write a workflow in wdl and use glob to output the result, like this.; ```; task xxx {; output {; Array[File] output_files = glob(""~{output_basename}_?P.fq.gz""); }; }; ```; xxx is a task name.; I output the task result in the workflow output part. Besides, I using this options.json to change some workflow options.; ```; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```; xxx is a absolute path.; When I test it on a cromwell server V49 running in a SGE cluster, the os is Centos7.4, everything seems ok. xxx folder contains the task xxx's result.; When I test it on a comwell server V50 running in the same SGE cluster, xxx folder contains a glob folder and the task result is in it.; I want all the result files in xxx folder. Is it possible in V50?; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524
https://github.com/broadinstitute/cromwell/issues/5524:662,Testability,test,test,662,"Thanks for your work with Cromwell.; I think below may be not a bug.; I write a workflow in wdl and use glob to output the result, like this.; ```; task xxx {; output {; Array[File] output_files = glob(""~{output_basename}_?P.fq.gz""); }; }; ```; xxx is a task name.; I output the task result in the workflow output part. Besides, I using this options.json to change some workflow options.; ```; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```; xxx is a absolute path.; When I test it on a cromwell server V49 running in a SGE cluster, the os is Centos7.4, everything seems ok. xxx folder contains the task xxx's result.; When I test it on a comwell server V50 running in the same SGE cluster, xxx folder contains a glob folder and the task result is in it.; I want all the result files in xxx folder. Is it possible in V50?; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524
https://github.com/broadinstitute/cromwell/pull/5525:24,Safety,sanity check,sanity checking,24,"Never merge this.; Need sanity checking. Testing method which I used was to run HelloWorld workflow using Cromwell built from this branch and wait for successful Carboniting (or OOM, or Cromwell starting to crumble in other ways).; Modifications made in this branch force Cromwell to copy the given number of root workflow's metadata jsons into the resulted carbonited json as if they were subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525
https://github.com/broadinstitute/cromwell/pull/5525:41,Testability,Test,Testing,41,"Never merge this.; Need sanity checking. Testing method which I used was to run HelloWorld workflow using Cromwell built from this branch and wait for successful Carboniting (or OOM, or Cromwell starting to crumble in other ways).; Modifications made in this branch force Cromwell to copy the given number of root workflow's metadata jsons into the resulted carbonited json as if they were subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525
https://github.com/broadinstitute/cromwell/pull/5529:136,Modifiability,Config,Configuring,136,"- There are *five* different authentication schemes (It looks like ""four"" is a typo); - Made each of the five options subheaders under ""Configuring Authentication"" for clarity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5529
https://github.com/broadinstitute/cromwell/pull/5529:29,Security,authenticat,authentication,29,"- There are *five* different authentication schemes (It looks like ""four"" is a typo); - Made each of the five options subheaders under ""Configuring Authentication"" for clarity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5529
https://github.com/broadinstitute/cromwell/pull/5529:148,Security,Authenticat,Authentication,148,"- There are *five* different authentication schemes (It looks like ""four"" is a typo); - Made each of the five options subheaders under ""Configuring Authentication"" for clarity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5529
https://github.com/broadinstitute/cromwell/issues/5530:108,Modifiability,config,config,108,"I'm trying to pass the `memory` attribute to various backends, including both AWS and SGE (using a modified config where I added `String? memory` in place of `Float? memory_gb`). Passing values like ""4G"" as the manual suggests works fine on AWS, but on SGE I get this:; ```; cromwell.core.CromwellFatalException: java.lang.RuntimeException: Unsupported wdl type for memory: WomStringType; ```; Am I missing something or is the type inconsistent between backends? That's going to make it a lot more work to set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530
https://github.com/broadinstitute/cromwell/issues/5533:792,Availability,echo,echo,792,"Noticed that cached-copy localization strategy is broken in Cromwell-51. This test works on a cluster where `execution-dir` is some place to do execution and copy the inputs to, and `/different/file/system/` is on a different disk. Small task (`catsmallfile.wdl`):. ```wdl; version development. task CatSmallFile {; input {; File inp; }; command {; cat ${inp}; }; output {; String out = read_string(stdout()); }; }; ```. Config (`cromwell.conf`):; ```hocon; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""<execution-dir>"",; ""filesystems.local.duplication-strategy"": [; ""cached-copy""; ]; }; }; }; }; ```. Command:. ```bash; echo ""Goodbye, call-caching"" >> /different/file/system/inp.txt; echo '{""inp"": ""/different/file/system/inp.txt""}' >> inputs.json. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is empty. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is populated; ```. ----. Anecdotally, I've noticed some of the permissions of localised files have changed, I wonder if this is related to that?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533
