id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8369,Modifiability,evolve,evolve,8369,"plying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_m",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:254,Performance,optimiz,optimizations,254,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:364,Performance,optimiz,optimization,364,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:382,Performance,perform,performed,382,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:481,Performance,optimiz,optimizer,481,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:632,Performance,optimiz,optimization,632,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:744,Performance,optimiz,optimizations,744,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1153,Performance,optimiz,optimizer,1153,"==. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <std",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1176,Performance,optimiz,optimizations,1176,"==. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <std",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1258,Performance,optimiz,optimizer,1258,"h can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1360,Performance,optimiz,optimization,1360,"ion when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2761,Performance,optimiz,optimizer,2761,"`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or lib",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2839,Performance,optimiz,optimizer,2839," foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2953,Performance,optimiz,optimizer,2953,"void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3133,Performance,optimiz,optimizer,3133,"e <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super too",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3282,Performance,optimiz,optimizer,3282,"n:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3335,Performance,optimiz,optimizer,3335,"n.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3480,Performance,optimiz,optimizer,3480,"ple, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3606,Performance,optimiz,optimizer,3606,"code file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3839,Performance,optimiz,optimization,3839,"ves ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:4255,Performance,optimiz,optimizer,4255,"r driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; fro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:4447,Performance,optimiz,optimizer,4447,"nformation collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:5139,Performance,optimiz,optimizer,5139,"is increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:5261,Performance,optimiz,optimizer,5261,"is increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:5367,Performance,optimiz,optimizer,5367,"nker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase communication between ``libLTO`` and linker; =======================================================. The linker collects information about symbol definitions and uses in various; link objects which is more accurate than any information collected by other; tools during typical build cycles. The linker collects this information by; looking at the definitions and uses of symbols in native .o files and using; symbol visibility information. The linker also uses user-supplied information,; such as a list of exported symbols. LLVM optimizer collects control flow; information, data flow information and knows much more about program structure; from the optimizer's point of view. Our goal is to take advantage of tight; integration between the linker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implemented in a shared object libLTO. This allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the share",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:6381,Performance,load,loaded,6381,"nker and the optimizer by sharing this information; during various linking phases. Phase 1 : Read LLVM Bitcode Files; ---------------------------------. The linker first reads all object files in natural order and collects symbol; information. This includes native object files as well as LLVM bitcode files.; To minimize the cost to the linker in the case that all .o files are native; object files, the linker only calls ``lto_module_create()`` when a supplied; object file is found to not be a native object file. If ``lto_module_create()``; returns that the file is an LLVM bitcode file, the linker then iterates over the; module using ``lto_module_get_symbol_name()`` and; ``lto_module_get_symbol_attribute()`` to get all symbols defined and referenced.; This information is added to the linker's global symbol table. The lto* functions are all implemented in a shared object libLTO. This allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7172,Performance,optimiz,optimizer,7172,"s allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7335,Performance,optimiz,optimization,7335,"s allows; the LLVM LTO code to be updated independently of the linker tool. On platforms; that support it, the shared object is lazily loaded. Phase 2 : Symbol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7390,Performance,optimiz,optimization,7390,"ol Resolution; ---------------------------. In this stage, the linker resolves symbols using global symbol table. It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7484,Performance,optimiz,optimized,7484,"It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object fil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7877,Performance,perform,performs,7877," Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8208,Performance,optimiz,optimizer,8208,"ators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8346,Performance,optimiz,optimizer,8346,"plying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_m",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10029,Performance,load,loaded,10029,"`, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10822,Performance,optimiz,optimized,10822,"-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c. lto_codegen_compile(lto_code_gen_t, size*). which returns a pointer to a buffer containing the generated native object file.; The linker then parses that and links it with the rest of the native object; files.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1271,Safety,avoid,avoid,1271,"h can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11713,Availability,redundant,redundant,11713,"=====================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:23037,Availability,error,error,23037,",; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invariant Code Motion (:ref:`-licm <passes-licm>`).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5746,Deployability,pipeline,pipeline,5746,"ase structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13869,Deployability,update,updated,13869," simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13970,Deployability,update,update,13970," simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5976,Energy Efficiency,power,power,5976,"en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10241,Energy Efficiency,schedul,scheduling,10241,"There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11904,Energy Efficiency,schedul,scheduling,11904,"ly inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9298,Integrability,interface,interface,9298,"). If the loop should; not be executed at all, a **loop guard** must skip the entire loop:. .. image:: ./loop-guard.svg; :width: 500 px. Since the first thing a loop header might do is to check whether there; is another execution and if not, immediately exit without doing any work; (also see :ref:`loop-terminology-loop-rotate`), loop trip count is not; the best measure of a loop's number of iterations. For instance, the; number of header executions of the code below for a non-positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when sc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9504,Integrability,interface,interface,9504,"ny work; (also see :ref:`loop-terminology-loop-rotate`), loop trip count is not; the best measure of a loop's number of iterations. For instance, the; number of header executions of the code below for a non-positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14622,Modifiability,variab,variable,14622,"ccordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops that are in LCSSA form.; The evolution of a scalar (loop-variant) expression that; SCEV can analyze is, by definition, relative to a loop.; An expression is represented in LLVM by an; `llvm::Instruction <",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14701,Modifiability,variab,variable,14701," = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops that are in LCSSA form.; The evolution of a scalar (loop-variant) expression that; SCEV can analyze is, by definition, relative to a loop.; An expression is represented in LLVM by an; `llvm::Instruction <https://llvm.org/doxygen/classllvm_1_1Instruction.html>`_.; If the expression is inside two (or more) loops (which",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:15034,Modifiability,variab,variable,15034," closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops that are in LCSSA form.; The evolution of a scalar (loop-variant) expression that; SCEV can analyze is, by definition, relative to a loop.; An expression is represented in LLVM by an; `llvm::Instruction <https://llvm.org/doxygen/classllvm_1_1Instruction.html>`_.; If the expression is inside two (or more) loops (which can only; happen if the loops are nested, like in the example above) and you want; to get an analysis of its evolution (from SCEV),; you have to also specify relative to what Loop you want it.; Specifically, you have to use; `getSCEVAtScope() <https://llvm.org/doxygen/classllv",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:258,Performance,optimiz,optimizer,258,".. _loop-terminology:. ===========================================; LLVM Loop Terminology (and Canonical Forms); ===========================================. .. contents::; :local:. Loop Definition; ===============. Loops are an important concept for a code optimizer. In LLVM, detection; of loops in a control-flow graph is done by :ref:`loopinfo`. It is based; on the following definition. A loop is a subset of nodes from the control-flow graph (CFG; where; nodes represent basic blocks) with the following properties:. 1. The induced subgraph (which is the subgraph that contains all the; edges from the CFG within the loop) is strongly connected; (every node is reachable from all others). 2. All edges from outside the subset into the subset point to the same; node, called the **header**. As a consequence, the header dominates; all nodes in the loop (i.e. every execution path to any of the loop's; node will have to pass through the header). 3. The loop is the maximum subset with these properties. That is, no; additional nodes from the CFG can be added such that the induced; subgraph would still be strongly connected and the header would; remain the same. In computer science literature, this is often called a *natural loop*.; In LLVM, a more generalized definition is called a; :ref:`cycle <cycle-terminology>`. Terminology; -----------. The definition of a loop comes with some additional terminology:. * An **entering block** (or **loop predecessor**) is a non-loop node; that has an edge into the loop (necessarily the header). If there is; only one entering block, and its only edge is to the; header, it is also called the loop's **preheader**. The preheader; dominates the loop without itself being part of the loop. * A **latch** is a loop node that has an edge to the header. * A **backedge** is an edge from a latch to the header. * An **exiting edge** is an edge from inside the loop to a node outside; of the loop. The source of such an edge is called an **exiting block**, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5727,Performance,optimiz,optimization,5727,"ase structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:7160,Performance,optimiz,optimizer,7160,"+i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is annotated with; :ref:`llvm.loop.mustprogress <langref_llvm_loop_mustprogress>` metadata,; the compiler is allowed to assume that it will eventually terminate, even; if it cannot prove it. For instance, it may remove a mustprogress-loop; that does not have any side-effect in its body even though the program; could be stuck in that loop forever. Languages such as C and; `C++ <https://eel.is/c++draft/intro.progress#1>`_ have such; forward-progress guarantees for some loops. Also see the; :ref:`mustprogress <langref_mustprogress>` and; :ref:`willreturn <langref_willreturn>` function attributes, as well as; the older :ref:`llvm.sideeffect <llvm_sideeffect>` intrinsic. * The number of executions of the loop header before leaving the loop is; the **loop trip count** (or **",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:7362,Performance,optimiz,optimizer,7362," have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is annotated with; :ref:`llvm.loop.mustprogress <langref_llvm_loop_mustprogress>` metadata,; the compiler is allowed to assume that it will eventually terminate, even; if it cannot prove it. For instance, it may remove a mustprogress-loop; that does not have any side-effect in its body even though the program; could be stuck in that loop forever. Languages such as C and; `C++ <https://eel.is/c++draft/intro.progress#1>`_ have such; forward-progress guarantees for some loops. Also see the; :ref:`mustprogress <langref_mustprogress>` and; :ref:`willreturn <langref_willreturn>` function attributes, as well as; the older :ref:`llvm.sideeffect <llvm_sideeffect>` intrinsic. * The number of executions of the loop header before leaving the loop is; the **loop trip count** (or **iteration count**). If the loop should; not be executed at all, a **loop guard** must skip the entire loop:. .. image:: ./loop-guard.svg; :width: 50",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9727,Performance,optimiz,optimization,9727,"positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); ===========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9825,Performance,optimiz,optimization,9825,"r (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11943,Performance,optimiz,optimizations,11943,"lue that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12922,Performance,optimiz,optimizations,12922," scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14358,Performance,perform,perform,14358,"de of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14482,Performance,perform,performs,14482,"de of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22257,Performance,load,loads,22257,"r, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22494,Performance,load,loading,22494,"image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22594,Performance,load,load,22594,"plicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22855,Performance,load,load,22855,"der basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22968,Performance,load,loading,22968,"xit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:278,Safety,detect,detection,278,".. _loop-terminology:. ===========================================; LLVM Loop Terminology (and Canonical Forms); ===========================================. .. contents::; :local:. Loop Definition; ===============. Loops are an important concept for a code optimizer. In LLVM, detection; of loops in a control-flow graph is done by :ref:`loopinfo`. It is based; on the following definition. A loop is a subset of nodes from the control-flow graph (CFG; where; nodes represent basic blocks) with the following properties:. 1. The induced subgraph (which is the subgraph that contains all the; edges from the CFG within the loop) is strongly connected; (every node is reachable from all others). 2. All edges from outside the subset into the subset point to the same; node, called the **header**. As a consequence, the header dominates; all nodes in the loop (i.e. every execution path to any of the loop's; node will have to pass through the header). 3. The loop is the maximum subset with these properties. That is, no; additional nodes from the CFG can be added such that the induced; subgraph would still be strongly connected and the header would; remain the same. In computer science literature, this is often called a *natural loop*.; In LLVM, a more generalized definition is called a; :ref:`cycle <cycle-terminology>`. Terminology; -----------. The definition of a loop comes with some additional terminology:. * An **entering block** (or **loop predecessor**) is a non-loop node; that has an edge into the loop (necessarily the header). If there is; only one entering block, and its only edge is to the; header, it is also called the loop's **preheader**. The preheader; dominates the loop without itself being part of the loop. * A **latch** is a loop node that has an edge to the header. * A **backedge** is an edge from a latch to the header. * An **exiting edge** is an edge from inside the loop to a node outside; of the loop. The source of such an edge is called an **exiting block**, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:4237,Safety,detect,detect,4237,"de set can; be a subset of another loop with a different loop header. The loop; hierarchy in a function forms a forest: Each top-level loop is the; root of the tree of the loops nested inside it. .. image:: ./loop-nested.svg; :width: 350 px. * It is not possible that two loops share only a few of their nodes.; Two loops are either disjoint or one is nested inside the other. In; the example below the left and right subsets both violate the; maximality condition. Only the merge of both sets is considered a loop. .. image:: ./loop-nonmaximal.svg; :width: 250 px. * It is also possible that two logical loops share a header, but are; considered a single loop by LLVM:. .. code-block:: C. for (int i = 0; i < 128; ++i); for (int j = 0; j < 128; ++j); body(i,j);. which might be represented in LLVM-IR as follows. Note that there is; only a single header and hence just a single loop. .. image:: ./loop-merge.svg; :width: 400 px. The :ref:`LoopSimplify <loop-terminology-loop-simplify>` pass will; detect the loop and ensure separate headers for the outer and inner loop. .. image:: ./loop-separate.svg; :width: 400 px. * A cycle in the CFG does not imply there is a loop. The example below; shows such a CFG, where there is no header node that dominates all; other nodes in the cycle. This is called **irreducible control-flow**. .. image:: ./loop-irreducible.svg; :width: 150 px. The term reducible results from the ability to collapse the CFG into a; single node by successively replacing one of three base structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:6345,Safety,abort,abort,6345,"n be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:6398,Safety,abort,abort,6398,"ef:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the control flow to eventually leave the; loop, i.e. a loop can be infinite. A **statically infinite loop** is a; loop that has no exiting edges. A **dynamically infinite loop** has; exiting edges, but it is possible to be never taken. This may happen; only under some circumstances, such as when n == UINT_MAX in the code; below. .. code-block:: C. for (unsigned i = 0; i <= n; ++i); body(i);. It is possible for the optimizer to turn a dynamically infinite loop; into a statically infinite loop, for instance when it can prove that the; exiting condition is always false. Because the exiting edge is never; taken, the optimizer can change the conditional branch into an; unconditional one. If a is loop is a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9454,Safety,detect,detection,9454,"is to check whether there; is another execution and if not, immediately exit without doing any work; (also see :ref:`loop-terminology-loop-rotate`), loop trip count is not; the best measure of a loop's number of iterations. For instance, the; number of header executions of the code below for a non-positive n; (before loop rotation) is 1, even though the loop body is not executed; at all. .. code-block:: C. for (int i = 0; i < n; ++i); body(i);. A better measure is the **backedge-taken count**, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:11713,Safety,redund,redundant,11713,"=====================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particular, consider the following loop:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2); // X3 defined; }. ... = X3 + 4; // X3 used, i.e. live; // outside the loop. In the inner loop, the X3 is defined inside the loop, but used; outside of it. In Loop Closed SSA form, this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:23151,Safety,avoid,avoid,23151,",; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invariant Code Motion (:ref:`-licm <passes-licm>`).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:23569,Safety,avoid,avoid,23569,",; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while (i < n);; }. Note that LoopRotate does not generally do such; hoisting. Rather, it is an enabling transformation for other; passes like Loop-Invariant Code Motion (:ref:`-licm <passes-licm>`).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:3836,Testability,log,logical,3836,"is trivially strongly connected. .. image:: ./loop-single.svg; :width: 300 px. In this case, the role of header, exiting block and latch fall to the; same node. :ref:`loopinfo` reports this as:. .. code-block:: console. $ opt input.ll -passes='print<loops>'; Loop at depth 1 containing: %for.body<header><latch><exiting>. * Loops can be nested inside each other. That is, a loop's node set can; be a subset of another loop with a different loop header. The loop; hierarchy in a function forms a forest: Each top-level loop is the; root of the tree of the loops nested inside it. .. image:: ./loop-nested.svg; :width: 350 px. * It is not possible that two loops share only a few of their nodes.; Two loops are either disjoint or one is nested inside the other. In; the example below the left and right subsets both violate the; maximality condition. Only the merge of both sets is considered a loop. .. image:: ./loop-nonmaximal.svg; :width: 250 px. * It is also possible that two logical loops share a header, but are; considered a single loop by LLVM:. .. code-block:: C. for (int i = 0; i < 128; ++i); for (int j = 0; j < 128; ++j); body(i,j);. which might be represented in LLVM-IR as follows. Note that there is; only a single header and hence just a single loop. .. image:: ./loop-merge.svg; :width: 400 px. The :ref:`LoopSimplify <loop-terminology-loop-simplify>` pass will; detect the loop and ensure separate headers for the outer and inner loop. .. image:: ./loop-separate.svg; :width: 400 px. * A cycle in the CFG does not imply there is a loop. The example below; shows such a CFG, where there is no header node that dominates all; other nodes in the cycle. This is called **irreducible control-flow**. .. image:: ./loop-irreducible.svg; :width: 150 px. The term reducible results from the ability to collapse the CFG into a; single node by successively replacing one of three base structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or sw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:18852,Testability,test,test,18852,"s only used as an example and does not pose any strict; requirements. For example, the value might dominate the current block; but we can still insert a PHI (as we do with LCSSA PHI nodes) *and*; use the original value afterwards (in which case the two live ranges overlap,; although in LCSSA (the whole point is that) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:18960,Testability,test,test,18960,"rent block; but we can still insert a PHI (as we do with LCSSA PHI nodes) *and*; use the original value afterwards (in which case the two live ranges overlap,; although in LCSSA (the whole point is that) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:19210,Testability,test,test,19210,"at) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:19293,Testability,test,test,19293,"l the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Value; in an internal data structure. ""More Canonical"" Loops; ======================. .. _loop-terminology-loop-rotate:. Rotated Loops; -------------. Loops are rotated by the LoopRotate (:ref:`loop-rotate <passes-loop-rotate>`); pass, which converts loops into do/while style loops and is; implemented in; `LoopRotation.h <https://llvm.org/doxygen/LoopRotation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:19781,Testability,test,test,19781,"ation_8h_source.html>`_. Example:. .. code-block:: C. void test(int n) {; for (int i = 0; i < n; i += 1); // Loop body; }. is transformed to:. .. code-block:: C. void test(int n) {; int i = 0;; do {; // Loop body; i += 1;; } while (i < n);; }. **Warning**: This transformation is valid only if the compiler; can prove that the loop body will be executed at least once. Otherwise,; it has to insert a guard which will test it at runtime. In the example; above, that would be:. .. code-block:: C. void test(int n) {; int i = 0;; if (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %body. body:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %exit. exit:; ret void; }. .. image:: ./loop-terminology-rotated-loop.png; :width: 400 px. Note two things:. * The condition check was moved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:20309,Testability,test,test,20309,"f (n > 0) {; do {; // Loop body; i += 1;; } while (i < n);; }; }. It's important to understand the effect of loop rotation; at the LLVM IR level. We follow with the previous examples; in LLVM IR while also providing a graphical representation; of the control-flow graphs (CFG). You can get the same graphical; results by utilizing the :ref:`view-cfg <passes-view-cfg>` pass. The initial **for** loop could be translated to:. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %for.header. for.header:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; %cond = icmp slt i32 %i, %n; br i1 %cond, label %body, label %exit. body:; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; br label %for.header. exit:; ret void; }. .. image:: ./loop-terminology-initial-loop.png; :width: 400 px. Before we explain how LoopRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %body. body:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %exit. exit:; ret void; }. .. image:: ./loop-terminology-rotated-loop.png; :width: 400 px. Note two things:. * The condition check was moved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the loop to the latch.; * The compiler in this case can't deduce that the loop will; definitely execute at least once so the above transformation; is not valid. As mentioned above, a guard has to be inserted,; which is something that LoopRotate will do. This is how LoopRotate transforms this loop:. .. code-block:: none. define void @test(i32 %n) {; entry:; %guard_cond = icmp slt i32 0, %n; br i1 %guard_cond, label %loop.preheader, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:21125,Testability,test,test,21125,"pRotate will actually; transform this loop, here's how we could convert; it (by hand) to a do-while style loop. .. code-block:: none. define void @test(i32 %n) {; entry:; br label %body. body:; %i = phi i32 [ 0, %entry ], [ %i.next, %latch ]; ; Loop body; br label %latch. latch:; %i.next = add nsw i32 %i, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %exit. exit:; ret void; }. .. image:: ./loop-terminology-rotated-loop.png; :width: 400 px. Note two things:. * The condition check was moved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the loop to the latch.; * The compiler in this case can't deduce that the loop will; definitely execute at least once so the above transformation; is not valid. As mentioned above, a guard has to be inserted,; which is something that LoopRotate will do. This is how LoopRotate transforms this loop:. .. code-block:: none. define void @test(i32 %n) {; entry:; %guard_cond = icmp slt i32 0, %n; br i1 %guard_cond, label %loop.preheader, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:4215,Usability,simpl,simplify,4215,"de set can; be a subset of another loop with a different loop header. The loop; hierarchy in a function forms a forest: Each top-level loop is the; root of the tree of the loops nested inside it. .. image:: ./loop-nested.svg; :width: 350 px. * It is not possible that two loops share only a few of their nodes.; Two loops are either disjoint or one is nested inside the other. In; the example below the left and right subsets both violate the; maximality condition. Only the merge of both sets is considered a loop. .. image:: ./loop-nonmaximal.svg; :width: 250 px. * It is also possible that two logical loops share a header, but are; considered a single loop by LLVM:. .. code-block:: C. for (int i = 0; i < 128; ++i); for (int j = 0; j < 128; ++j); body(i,j);. which might be represented in LLVM-IR as follows. Note that there is; only a single header and hence just a single loop. .. image:: ./loop-merge.svg; :width: 400 px. The :ref:`LoopSimplify <loop-terminology-loop-simplify>` pass will; detect the loop and ensure separate headers for the outer and inner loop. .. image:: ./loop-separate.svg; :width: 400 px. * A cycle in the CFG does not imply there is a loop. The example below; shows such a CFG, where there is no header node that dominates all; other nodes in the cycle. This is called **irreducible control-flow**. .. image:: ./loop-irreducible.svg; :width: 150 px. The term reducible results from the ability to collapse the CFG into a; single node by successively replacing one of three base structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:9928,Usability,simpl,simplify,9928,"*, which is the number of; times any of the backedges is taken before the loop. It is one less than; the trip count for executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10071,Usability,simpl,simpler,10071,"or executions that enter the header. .. _loopinfo:. LoopInfo; ========. LoopInfo is the core analysis for obtaining information about loops.; There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10148,Usability,simpl,simplify,10148,"There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:10170,Usability,simpl,simplify,10170,"There are few key implications of the definitions given above which; are important for working successfully with this interface. * LoopInfo does not contain information about non-loop cycles. As a; result, it is not suitable for any algorithm which requires complete; cycle detection for correctness. * LoopInfo provides an interface for enumerating all top level loops; (e.g. those not contained in any other loop). From there, you may; walk the tree of sub-loops rooted in that top level loop. * Loops which become statically unreachable during optimization *must*; be removed from LoopInfo. If this can not be done for some reason,; then the optimization is *required* to preserve the static; reachability of the loop. .. _loop-terminology-loop-simplify:. Loop Simplify Form; ==================. The Loop Simplify Form is a canonical form that makes; several analyses and transformations simpler and more effective.; It is ensured by the LoopSimplify; (:ref:`-loop-simplify <passes-loop-simplify>`) pass and is automatically; added by the pass managers when scheduling a LoopPass.; This pass is implemented in; `LoopSimplify.h <https://llvm.org/doxygen/LoopSimplify_8h_source.html>`_.; When it is successful, the loop has:. * A preheader.; * A single backedge (which implies that there is a single latch).; * Dedicated exits. That is, no exit block for the loop; has a predecessor that is outside the loop. This implies; that all exit blocks are dominated by the loop header. .. _loop-terminology-lcssa:. Loop Closed SSA (LCSSA); =======================. A program is in Loop Closed SSA Form if it is in SSA form; and all values that are defined in a loop are used only inside; this loop. Programs written in LLVM IR are always in SSA form but not necessarily; in LCSSA. To achieve the latter, for each value that is live across the; loop boundary, single entry PHI nodes are inserted to each of the exit blocks; [#lcssa-construction]_ in order to ""close"" these values inside the loop.; In particul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12530,Usability,clear,clearly,12530,", this would be represented as follows:. .. code-block:: C. c = ...;; for (...) {; if (c); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; X4 = phi(X3);. ... = X4 + 4;. This is still valid LLVM; the extra phi nodes are purely redundant,; but all LoopPass'es are required to preserve them.; This form is ensured by the LCSSA (:ref:`-lcssa <passes-lcssa>`); pass and is added automatically by the LoopPassManager when; scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12937,Usability,simpl,simpler,12937," scheduling a LoopPass.; After the loop optimizations are done, these extra phi nodes; will be deleted by :ref:`-instcombine <passes-instcombine>`. Note that an exit block is outside of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:12962,Usability,simpl,simple,12962,"side of a loop, so how can such a phi ""close""; the value inside the loop since it uses it outside of it ? First of all,; for phi nodes, as; `mentioned in the LangRef <https://llvm.org/docs/LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13252,Usability,simpl,simple-loop-unswitch,13252,"LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that bec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:13281,Usability,simpl,simple-loop-unswitch,13281,"LangRef.html#id311>`_:; ""the use of each incoming value is deemed to occur on the edge from the; corresponding predecessor block to the current block"". Now, an; edge to an exit block is considered outside of the loop because; if we take that edge, it leads us clearly out of the loop. However, an edge doesn't actually contain any IR, so in source code,; we have to choose a convention of whether the use happens in; the current block or in the respective predecessor. For LCSSA's purpose,; we consider the use happens in the latter (so as to consider the; use inside) [#point-of-use-phis]_. The major benefit of LCSSA is that it makes many other loop optimizations; simpler. First of all, a simple observation is that if one needs to see all; the outside users, they can just iterate over all the (loop closing); PHI nodes in the exit blocks (the alternative would be to; scan the def-use chain [#def-use-chain]_ of all instructions in the loop). Then, consider for example; :ref:`simple-loop-unswitch <passes-simple-loop-unswitch>` ing the loop above.; Because it is in LCSSA form, we know that any value defined inside of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that bec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:17382,Usability,intuit,intuition,17382,"an now just use; `getSCEV() <https://llvm.org/doxygen/classllvm_1_1ScalarEvolution.html#a30bd18ac905eacf3601bc6a553a9ff49>`_.; and which of these two llvm::Instructions you pass to it disambiguates; the context / scope / relative loop. .. rubric:: Footnotes. .. [#lcssa-construction] To insert these loop-closing PHI nodes, one has to; (re-)compute dominance frontiers (if the loop has multiple exits). .. [#point-of-use-phis] Considering the point of use of a PHI entry value; to be in the respective predecessor is a convention across the whole LLVM.; The reason is mostly practical; for example it preserves the dominance; property of SSA. It is also just an overapproximation of the actual; number of uses; the incoming block could branch to another block in which; case the value is not actually used but there are no side-effects (it might; increase its live range which is not relevant in LCSSA though).; Furthermore, we can gain some intuition if we consider liveness:; A PHI is *usually* inserted in the current block because the value can't; be used from this point and onwards (i.e. the current block is a dominance; frontier). It doesn't make sense to consider that the value is used in; the current block (because of the PHI) since the value stops being live; before the PHI. In some sense the PHI definition just ""replaces"" the original; value definition and doesn't actually use it. It should be stressed that; this analogy is only used as an example and does not pose any strict; requirements. For example, the value might dominate the current block; but we can still insert a PHI (as we do with LCSSA PHI nodes) *and*; use the original value afterwards (in which case the two live ranges overlap,; although in LCSSA (the whole point is that) we never do that). .. [#def-use-chain] A property of SSA is that there exists a def-use chain; for each definition, which is a list of all the uses of this definition.; LLVM implements this property by keeping a list of all the uses of a Valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:21742,Usability,simpl,simplify,21742,"ved to the ""bottom"" of the loop, i.e.; the latch. This is something that LoopRotate does by copying the header; of the loop to the latch.; * The compiler in this case can't deduce that the loop will; definitely execute at least once so the above transformation; is not valid. As mentioned above, a guard has to be inserted,; which is something that LoopRotate will do. This is how LoopRotate transforms this loop:. .. code-block:: none. define void @test(i32 %n) {; entry:; %guard_cond = icmp slt i32 0, %n; br i1 %guard_cond, label %loop.preheader, label %exit. loop.preheader:; br label %body. body:; %i2 = phi i32 [ 0, %loop.preheader ], [ %i.next, %latch ]; br label %latch. latch:; %i.next = add nsw i32 %i2, 1; %cond = icmp slt i32 %i.next, %n; br i1 %cond, label %body, label %loop.exit. loop.exit:; br label %exit. exit:; ret void; }. .. image:: ./loop-terminology-guarded-loop.png; :width: 500 px. The result is a little bit more complicated than we may expect; because LoopRotate ensures that the loop is in; :ref:`Loop Simplify Form <loop-terminology-loop-simplify>`; after rotation.; In this case, it inserted the %loop.preheader basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:795,Availability,robust,robust,795,"=======================================; LLVM's Optional Rich Disassembly Output; =======================================. .. contents::; :local:. Introduction; ============. LLVM's default disassembly output is raw text. To allow consumers more ability; to introspect the instructions' textual representation or to reformat for a more; user friendly display there is an optional rich disassembly output. This optional output is sufficient to reference into individual portions of the; instruction text. This is intended for clients like disassemblers, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:1564,Energy Efficiency,efficient,efficiently,1564,"rs, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:2646,Performance,load,load,2646,"p annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations to grow in richness without breaking older; clients. For example, a possible annotation of an ARM load of a stack-relative location; might be annotated as:. .. code-block:: text. ldr <reg gpr:r0>, <mem regoffset:[<reg gpr:sp>, <imm:#4>]>. 1: For assembly dialects in which '<' and/or '>' are legal tokens, a literal token is escaped by following immediately with a repeat of the character. For example, a literal '<' character is output as '<<' in an annotated assembly string. C API Details; -------------. The intended consumers of this information use the C API, therefore the new C; API function for the disassembler will be added to provide an option to produce; disassembled instructions with annotations, ``LLVMSetDisasmOptions()`` and the; ``LLVMDisassembler_Option_UseMarkup`` option (see above).; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:765,Usability,simpl,simple,765,"=======================================; LLVM's Optional Rich Disassembly Output; =======================================. .. contents::; :local:. Introduction; ============. LLVM's default disassembly output is raw text. To allow consumers more ability; to introspect the instructions' textual representation or to reformat for a more; user friendly display there is an optional rich disassembly output. This optional output is sufficient to reference into individual portions of the; instruction text. This is intended for clients like disassemblers, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:987,Usability,simpl,simply,987,"=======================================; LLVM's Optional Rich Disassembly Output; =======================================. .. contents::; :local:. Introduction; ============. LLVM's default disassembly output is raw text. To allow consumers more ability; to introspect the instructions' textual representation or to reformat for a more; user friendly display there is an optional rich disassembly output. This optional output is sufficient to reference into individual portions of the; instruction text. This is intended for clients like disassemblers, list file; generators, and pretty-printers, which need more than the raw instructions and; the ability to print them. To provide this functionality the assembly text is marked up with annotations.; The markup is simple enough in syntax to be robust even in the case of version; mismatches between consumers and producers. That is, the syntax generally does; not carry semantics beyond ""this text has an annotation,"" so consumers can; simply ignore annotations they do not understand or do not care about. After calling ``LLVMCreateDisasm()`` to create a disassembler context the; optional output is enable with this call:. .. code-block:: c. LLVMSetDisasmOptions(DC, LLVMDisassembler_Option_UseMarkup);. Then subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst:2219,Usability,simpl,simple,2219,"hen subsequent calls to ``LLVMDisasmInstruction()`` will return output strings; with the marked up annotations. Instruction Annotations; =======================. .. _contextual markups:. Contextual markups; ------------------. Annotated assembly display will supply contextual markup to help clients more; efficiently implement things like pretty printers. Most markup will be target; independent, so clients can effectively provide good display without any target; specific knowledge. Annotated assembly goes through the normal instruction printer, but optionally; includes contextual tags on portions of the instruction string. An annotation; is any '<' '>' delimited section of text(1). .. code-block:: bat. annotation: '<' tag-name tag-modifier-list ':' annotated-text '>'; tag-name: identifier; tag-modifier-list: comma delimited identifier list. The tag-name is an identifier which gives the type of the annotation. For the; first pass, this will be very simple, with memory references, registers, and; immediates having the tag names ""mem"", ""reg"", and ""imm"", respectively. The tag-modifier-list is typically additional target-specific context, such as; register class. Clients should accept and ignore any tag-names or tag-modifiers they do not; understand, allowing the annotations to grow in richness without breaking older; clients. For example, a possible annotation of an ARM load of a stack-relative location; might be annotated as:. .. code-block:: text. ldr <reg gpr:r0>, <mem regoffset:[<reg gpr:sp>, <imm:#4>]>. 1: For assembly dialects in which '<' and/or '>' are legal tokens, a literal token is escaped by following immediately with a repeat of the character. For example, a literal '<' character is output as '<<' in an annotated assembly string. C API Details; -------------. The intended consumers of this information use the C API, therefore the new C; API function for the disassembler will be added to provide an option to produce; disassembled instructions with annotations",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MarkedUpDisassembly.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:6586,Deployability,update,update,6586," shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::mapSectionAddress.; This should happen before the section memory is copied to its new; location. When MCJIT::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeDyldImpl stores the new; address in an internal data structure but does not update the code at this; time, since other sections are likely to change. When the client is finished remapping section addresses, it will call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations itera",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5265,Energy Efficiency,allocate,allocated,5265," including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:8685,Energy Efficiency,allocate,allocated,8685,"ll call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; functions, such as registering the EH frame information with a debugger. Finally, MCJIT calls the memory manager's finalizeMemory method. In this; method, the memory manager will invalidate the target code cache, if; necessary, and apply final permissions to the memory pages it has; allocated for code and data memory.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1753,Integrability,wrap,wrapper,1753,"MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to Targ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3003,Integrability,depend,depending,3003,"ter to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3696,Integrability,wrap,wrapper,3696,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4130,Integrability,wrap,wraps,4130,"ontains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1690,Modifiability,variab,variable,1690,"MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to Targ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:388,Performance,load,loading,388,"===============================; MCJIT Design and Implementation; ===============================. Introduction; ============. This document describes the internal workings of the MCJIT execution; engine and the RuntimeDyld component. It is intended as a high level; overview of the implementation, showing the flow and interactions of; objects throughout the code generation and dynamic loading process. Engine Creation; ===============. In most cases, an EngineBuilder object is used to create an instance of; the MCJIT execution engine. The EngineBuilder takes an llvm::Module; object as an argument to its constructor. The client may then set various; options that we control the later be passed along to the MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:1902,Performance,load,loaded,1902,"r::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:2501,Performance,cache,cached,2501,"rs to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object imag",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:2834,Performance,load,load,2834,"d the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3346,Performance,load,loaded,3346,"s described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3435,Performance,cache,cache,3435,"s described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3672,Performance,load,loaded,3672,"t uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3921,Performance,load,loadObject,3921,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3942,Performance,perform,perform,3942,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3963,Performance,load,loading,3963,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3994,Performance,load,load,3994,"r MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4021,Performance,load,loadObject,4021,"object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4368,Performance,load,loadObject,4368,"ust be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4550,Performance,load,loaded,4550,"ither through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject comp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4734,Performance,load,loadObject,4734,"ther RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to ap",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5122,Performance,load,load-object,5122,"ge, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5161,Performance,load,loadObject,5161," including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5246,Performance,load,loaded,5246," including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5545,Performance,load,loadObject,5545," data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this is done because the code was generated for an; external process and is being mapped into that process' address space.; The client remaps the section address by calling MCJIT::mapSectionAddress.; This should happen before the section memory is copied to its new; location. When MCJIT::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeD",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:7514,Performance,load,loaded,7514,"::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeDyldImpl stores the new; address in an internal data structure but does not update the code at this; time, since other sections are likely to change. When the client is finished remapping section addresses, it will call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; func",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:8607,Performance,cache,cache,8607,"ll call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; functions, such as registering the EH frame information with a debugger. Finally, MCJIT calls the memory manager's finalizeMemory method. In this; method, the memory manager will invalidate the target code cache, if; necessary, and apply final permissions to the memory pages it has; allocated for code and data memory.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:4228,Security,access,access,4228,"ontains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:1119,Availability,resilien,resilient,1119,"=======================. Here are several ideas you can take into account when designing your specific; LLVM Social. Before you start, it is essential to make sure that the meetup is as welcoming; as any other event related to LLVM. Therefore you shall follow LLVM's; `Code of Conduct <https://llvm.org/docs/CodeOfConduct.html>`_. Other than that - your mileage may vary. Please adapt your social to what works; best for your specific situation. General suggestions; -------------------. * We highly recommend that you join the official LLVM meetup organization. In; addition to covering the cost of the meetup, all LLVM meetups are advertised; together and easily found by potential attendees. Please contact; arnaud.degrandmaison@llvm.org for more details.; * Beware of cultural differences: what works well in one region may not work in; other part of the world.; * Do not be alone to organize the meetup. Try to work with a couple other; organizers. This is more motivating as an organizer, and this makes the; meetup more resilient over time.; * Each event can have a different form such as a social event, or; a hackathon/workshop, or a 'mini-conference' with one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2303,Deployability,rolling,rolling,2303," one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * It’s a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/café with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., it’s not a good idea to have LLVM meetu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2408,Deployability,pipeline,pipeline,2408,"eekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * It’s a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/café with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., it’s not a good idea to have LLVM meetup on Thursday after; C++ meetup on Wednesday).; * Meetups on weekends may attract people who live far away",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:471,Energy Efficiency,adapt,adapt,471,"=====================================; How to start LLVM Social in your town; =====================================. Here are several ideas you can take into account when designing your specific; LLVM Social. Before you start, it is essential to make sure that the meetup is as welcoming; as any other event related to LLVM. Therefore you shall follow LLVM's; `Code of Conduct <https://llvm.org/docs/CodeOfConduct.html>`_. Other than that - your mileage may vary. Please adapt your social to what works; best for your specific situation. General suggestions; -------------------. * We highly recommend that you join the official LLVM meetup organization. In; addition to covering the cost of the meetup, all LLVM meetups are advertised; together and easily found by potential attendees. Please contact; arnaud.degrandmaison@llvm.org for more details.; * Beware of cultural differences: what works well in one region may not work in; other part of the world.; * Do not be alone to organize the meetup. Try to work with a couple other; organizers. This is more motivating as an organizer, and this makes the; meetup more resilient over time.; * Each event can have a different form such as a social event, or; a hackathon/workshop, or a 'mini-conference' with one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2247,Energy Efficiency,schedul,scheduled,2247," one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * It’s a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/café with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., it’s not a good idea to have LLVM meetu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:2900,Energy Efficiency,charge,charge,2900,"oup. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitter and mention; `@llvmweekly <http://twitter.com/llvmweekly>`_ and; `@llvmorg <http://twitter.com/llvmorg>`_.; * Announce the next meetup in advance, and remind in one week or so. Tech talks; ----------. * It’s a great idea to have several talks scheduled for several upcoming; meetups to get the ball rolling.; * Keep looking for speakers far in advance, ideally you should have 2-3; speakers ready in the pipeline.; * Try to record the talks if possible. It adds visibility to the meetup and; just a good idea in general. Any modern smartphone or tablet should work, but; you can also get a camera. Though, it is recommended to get an external; microphone for better sound. Where to host the meetup?; -------------------------. * Look around for bars/café with projectors.; * Talk to tech companies in the area.; * Some co-working spaces provide their facilities for non-profit (i.e., you do; not charge attendees any fees) meetups.; * Ask nearby universities or university departments. How to pick the date?; ---------------------. * Make sure you do not clash with the similar meetups in the city (e.g.,; C++ user groups).; * Prefer not to have a meetup the same week when the other similar meetups; happen (e.g., it’s not a good idea to have LLVM meetup on Thursday after; C++ meetup on Wednesday).; * Meetups on weekends may attract people who live far away from the city,; but the people who live in the city may not attend.; * Make a poll, but beware that not every responder will join (we had ~20 votes; on the poll, while only ~8 people attended). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst:471,Modifiability,adapt,adapt,471,"=====================================; How to start LLVM Social in your town; =====================================. Here are several ideas you can take into account when designing your specific; LLVM Social. Before you start, it is essential to make sure that the meetup is as welcoming; as any other event related to LLVM. Therefore you shall follow LLVM's; `Code of Conduct <https://llvm.org/docs/CodeOfConduct.html>`_. Other than that - your mileage may vary. Please adapt your social to what works; best for your specific situation. General suggestions; -------------------. * We highly recommend that you join the official LLVM meetup organization. In; addition to covering the cost of the meetup, all LLVM meetups are advertised; together and easily found by potential attendees. Please contact; arnaud.degrandmaison@llvm.org for more details.; * Beware of cultural differences: what works well in one region may not work in; other part of the world.; * Do not be alone to organize the meetup. Try to work with a couple other; organizers. This is more motivating as an organizer, and this makes the; meetup more resilient over time.; * Each event can have a different form such as a social event, or; a hackathon/workshop, or a 'mini-conference' with one or more talks. You do; not have to stick to one format forever.; * Whatever format you choose, `LLVM Weekly <http://llvmweekly.org/>`_ is an; excellent topic starter: go through the 3-4 recent LLVM Weekly posts and; prepare a list of the most interesting/notable news and discuss them with the; group. Advertisement; -------------. * Try to advertise via similar meetups/user groups; * Advertise your meetup on the mailing lists (llvm-dev, cfe-dev, lldb-dev,; ...). Feel free to post to all of them, or at least to llvm-dev.; But as these mailing lists have high traffic and some LLVM developers are not; very active on them, you may reach more interested people using the mailing; feature from meetup.com.; * Advertise the meetup on Twitt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MeetupGuidelines.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6792,Availability,down,down,6792,"e.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14169,Deployability,update,updated,14169,"User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14197,Deployability,update,updated,14197,"User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14300,Deployability,update,update,14300,"d, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14430,Deployability,update,update,14430,"walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14533,Deployability,update,update,14533," U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19143,Deployability,update,updated,19143,"ions have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers'",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19204,Deployability,update,update,19204," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19474,Deployability,update,updates,19474," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11686,Energy Efficiency,reduce,reduce,11686,"is relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:8020,Integrability,depend,depends,8020,"at ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3438,Modifiability,variab,variables,3438,"s that there is a *single*; ``Def`` chain that connects all the ``Def``\ s, either directly; or indirectly. For example in:. .. code-block:: llvm. b = MemoryDef(a); c = MemoryDef(b); d = MemoryDef(c). ``d`` is connected directly with ``c`` and indirectly with ``b``.; This means that ``d`` potentially clobbers (see below) ``c`` *or*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3601,Modifiability,variab,variable,3601,". b = MemoryDef(a); c = MemoryDef(b); d = MemoryDef(c). ``d`` is connected directly with ``c`` and indirectly with ``b``.; This means that ``d`` potentially clobbers (see below) ``c`` *or*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9581,Modifiability,flexible,flexible,9581,"ruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16718,Modifiability,variab,variables,16718,"-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16805,Modifiability,variab,variable,16805,"-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16884,Modifiability,variab,variables,16884,"-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17413,Modifiability,variab,variables,17413," br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimiz",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17548,Modifiability,variab,variables,17548,"ause it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there ar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17590,Modifiability,variab,variables,17590,"sion; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17715,Modifiability,variab,variables,17715,"if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18126,Modifiability,variab,variables,18126,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18289,Modifiability,variab,variable,18289,"nalysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18445,Modifiability,variab,variable,18445,"esent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:2094,Performance,load,load,2094,"ocument goes over how ``MemorySSA`` is structured, and some basic; intuition on how ``MemorySSA`` works. A paper on MemorySSA (with notes about how it's implemented in GCC) `can be; found here <http://www.airs.com/dnovillo/Papers/mem-ssa.pdf>`_. Though, it's; relatively out-of-date; the paper references multiple memory partitions, but GCC; eventually swapped to just using one, like we now have in LLVM. Like; GCC's, LLVM's MemorySSA is intraprocedural. MemorySSA Structure; ===================. MemorySSA is a virtual IR. After it's built, ``MemorySSA`` will contain a; structure that maps ``Instruction``\ s to ``MemoryAccess``\ es, which are; ``MemorySSA``'s parallel to LLVM ``Instruction``\ s. Each ``MemoryAccess`` can be one of three types:. - ``MemoryDef``; - ``MemoryPhi``; - ``MemoryUse``. ``MemoryDef``\ s are operations which may either modify memory, or which; introduce some kind of ordering constraints. Examples of ``MemoryDef``\ s; include ``store``\ s, function calls, ``load``\ s with ``acquire`` (or higher); ordering, volatile operations, memory fences, etc. A ``MemoryDef``; always introduces a new version of the entire memory and is linked with a single; ``MemoryDef/MemoryPhi`` which is the version of memory that the new; version is based on. This implies that there is a *single*; ``Def`` chain that connects all the ``Def``\ s, either directly; or indirectly. For example in:. .. code-block:: llvm. b = MemoryDef(a); c = MemoryDef(b); d = MemoryDef(c). ``d`` is connected directly with ``c`` and indirectly with ``b``.; This means that ``d`` potentially clobbers (see below) ``c`` *or*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``Memo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:3717,Performance,load,load,3717,"*; ``b`` *or* both. This in turn implies that without the use of `The walker`_,; initially every ``MemoryDef`` clobbers every other ``MemoryDef``. ``MemoryPhi``\ s are ``PhiNode``\ s, but for memory operations. If at any; point we have two (or more) ``MemoryDef``\ s that could flow into a; ``BasicBlock``, the block's top ``MemoryAccess`` will be a; ``MemoryPhi``. As in LLVM IR, ``MemoryPhi``\ s don't correspond to any; concrete operation. As such, ``BasicBlock``\ s are mapped to ``MemoryPhi``\ s; inside ``MemorySSA``, whereas ``Instruction``\ s are mapped to ``MemoryUse``\ s; and ``MemoryDef``\ s. Note also that in SSA, Phi nodes merge must-reach definitions (that is,; definitions that *must* be new versions of variables). In MemorySSA, PHI nodes; merge may-reach definitions (that is, until disambiguated, the versions that; reach a phi node may or may not clobber a given variable). ``MemoryUse``\ s are operations which use but don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6088,Performance,load,load,6088,"modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6168,Performance,load,load,6168,"modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:6606,Performance,load,load,6606," of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or `",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7206,Performance,optimiz,optimization,7206,"ecede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; =======",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7609,Performance,load,load,7609," LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` .",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7827,Performance,load,load,7827,"g; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9439,Performance,optimiz,optimize,9439,"an then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you ch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10397,Performance,cache,cached,10397,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10795,Performance,optimiz,optimize,10795,"ies ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10964,Performance,optimiz,optimization,10964," the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11071,Performance,optimiz,optimized,11071,"esults; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11119,Performance,optimiz,optimized,11119,"ingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed an",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11241,Performance,optimiz,optimized,11241,"on ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11622,Performance,optimiz,optimize,11622,"is relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11853,Performance,optimiz,optimization,11853,"iningAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12016,Performance,optimiz,optimize,12016," is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12481,Performance,optimiz,optimized,12481,"r and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12594,Performance,cache,cache,12594,"; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<Memo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12793,Performance,optimiz,optimized,12793,"the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. /",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12912,Performance,optimiz,optimized,12912,"l ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13168,Performance,optimiz,optimized,13168,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13234,Performance,optimiz,optimized,13234,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13269,Performance,optimiz,optimized,13269,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13313,Performance,optimiz,optimized,13313,"'t have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13360,Performance,load,loads,13360,"'t have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13804,Performance,optimiz,optimized,13804,"ode snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14613,Performance,optimiz,optimization,14613," U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15214,Performance,load,load,15214,"tion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, labe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15294,Performance,load,load,15294,"tion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, labe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15746,Performance,optimiz,optimizing,15746,"actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15803,Performance,optimiz,optimizations,15803,"actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16032,Performance,optimiz,optimizations,16032,"i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results confl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16314,Performance,load,load,16314,"br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not me",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16481,Performance,load,load,16481," one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16542,Performance,load,load,16542,"else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new v",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17191,Performance,optimiz,optimization,17191,"hen trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18073,Performance,optimiz,optimizations,18073,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18332,Performance,optimiz,optimizations,18332,"o; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18825,Performance,optimiz,optimized,18825,"stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Develop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18928,Performance,optimiz,optimized,18928,"stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Develop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19334,Performance,optimiz,optimizations,19334," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19485,Performance,optimiz,optimized,19485," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19553,Performance,optimiz,optimization,19553," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19770,Performance,optimiz,optimize-memoryssa,19770," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19830,Performance,optimiz,optimizations,19830," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:4695,Security,access,access,4695,"t don't modify memory. An example of; a ``MemoryUse`` is a ``load``, or a ``readonly`` function call. Every function that exists has a special ``MemoryDef`` called ``liveOnEntry``.; It dominates every ``MemoryAccess`` in the function that ``MemorySSA`` is being; run on, and implies that we've hit the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:4958,Security,access,accessing,4958,"the top of the function. It's the only; ``MemoryDef`` that maps to no ``Instruction`` in LLVM IR. Use of; ``liveOnEntry`` implies that the memory being used is either undefined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:5098,Security,access,accesses,5098,"defined or; defined before the function begins. An example of all of this overlaid on LLVM IR (obtained by running ``opt; -passes='print<memoryssa>' -disable-output`` on an ``.ll`` file) is below. When; viewing this example, it may be helpful to view it in terms of clobbers.; The operands of a given ``MemoryAccess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:5483,Security,access,access,5483,"ess`` are all (potential) clobbers of said; ``MemoryAccess``, and the value produced by a ``MemoryAccess`` can act as a clobber; for other ``MemoryAccess``\ es. If a ``MemoryAccess`` is a *clobber* of another, it means that these two; ``MemoryAccess``\ es may access the same memory. For example, ``x = MemoryDef(y)``; means that ``x`` potentially modifies memory that ``y`` modifies/constrains; (or has modified / constrained).; In the same manner, ``a = MemoryPhi({BB1,b},{BB2,c})`` means that; anyone that uses ``a`` is accessing memory potentially modified / constrained; by either ``b`` or ``c`` (or both). And finally, ``MemoryUse(x)`` means; that this use accesses memory that ``x`` has modified / constrained; (as an example, think that if ``x = MemoryDef(...)``; and ``MemoryUse(x)`` are in the same loop, the use can't; be hoisted outside alone). Another useful way of looking at it is in terms of memory versions.; In that view, operands of a given ``MemoryAccess`` are the version; of the entire memory before the operation, and if the access produces; a value (i.e. ``MemoryDef/MemoryPhi``),; the value is the new version of the memory after the operation. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 6 = MemoryPhi({entry,1},{if.end,4}); br i1 undef, label %if.then, label %if.else. if.then:; ; 2 = MemoryDef(6); store i8 0, ptr %p1; br label %if.end. if.else:; ; 3 = MemoryDef(6); store i8 1, ptr %p2; br label %if.end. if.end:; ; 5 = MemoryPhi({if.then,2},{if.else,3}); ; MemoryUse(5); %1 = load i8, ptr %p1; ; 4 = MemoryDef(5); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. The ``MemorySSA`` IR is shown in comments that precede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9880,Security,access,access,9880," The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10001,Security,access,access,10001,"ven:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10093,Security,access,access,10093,"ven:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10214,Security,access,access,10214,"o ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Spec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10327,Security,access,access,10327,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10356,Security,access,access,10356,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11082,Security,access,access,11082,"esults; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12110,Security,access,access,12110," is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12209,Security,access,access,12209,"oryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12387,Security,access,accesses,12387,"oryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``Mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12491,Security,access,access,12491,"r and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12774,Security,access,accesses,12774,"the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. /",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13219,Security,access,access,13219,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13814,Security,access,access,13814,"ode snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13838,Security,access,access,13838,"ode snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:14552,Security,access,access,14552," U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR, it needs to be updated whenever; the IR is updated. ""Update"", in this case, includes the addition, deletion, and; motion of ``Instructions``. The update API is being made on an as-needed basis.; If you'd like examples, ``GVNHoist`` and ``LICM`` are users of ``MemorySSA``\ s; update API.; Note that adding new ``MemoryDef``\ s (by calling ``insertDef``) can be a; time-consuming update, if the new access triggers many ``MemoryPhi`` insertions and; renaming (optimization invalidation) of many ``MemoryAccesses``\ es. Phi placement; ^^^^^^^^^^^^^. ``MemorySSA`` only places ``MemoryPhi``\ s where they're actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:19495,Security,access,access,19495," has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``MemorySSA`` are not permitted.; There is currently a single, narrowly scoped exception where DSE (DeadStoreElimination); updates an optimized access of a store, after a traversal that guarantees the; optimization is correct. This is solely allowed due to the traversals and inferences; being beyond what ``MemorySSA`` does and them being ""free"" (i.e. DSE does them anyway).; This exception is set under a flag (""-dse-optimize-memoryssa"") and can be disabled to; help reproduce optimizations in isolation. LLVM Developers Meeting presentations; -------------------------------------. - `2016 LLVM Developers' Meeting: G. Burgess - MemorySSA in Five Minutes <https://www.youtube.com/watch?v=bdxWmryoHak>`_.; - `2020 LLVM Developers' Meeting: S. Baziotis & S. Moll - Finding Your Way Around the LLVM Dependence Analysis Zoo <https://www.youtube.com/watch?v=1e5y6WDbXCQ>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:1170,Usability,intuit,intuition,1170,"t the; interactions between various memory operations. Its goal is to replace; ``MemoryDependenceAnalysis`` for most (if not all) use-cases. This is because,; unless you're very careful, use of ``MemoryDependenceAnalysis`` can easily; result in quadratic-time algorithms in LLVM. Additionally, ``MemorySSA`` doesn't; have as many arbitrary limits as ``MemoryDependenceAnalysis``, so you should get; better results, too. One common use of ``MemorySSA`` is to quickly find out; that something definitely cannot happen (for example, reason that a hoist; out of a loop can't happen). At a high level, one of the goals of ``MemorySSA`` is to provide an SSA based; form for memory, complete with def-use and use-def chains, which; enables users to quickly find may-def and may-uses of memory operations.; It can also be thought of as a way to cheaply give versions to the complete; state of memory, and associate memory operations with those versions. This document goes over how ``MemorySSA`` is structured, and some basic; intuition on how ``MemorySSA`` works. A paper on MemorySSA (with notes about how it's implemented in GCC) `can be; found here <http://www.airs.com/dnovillo/Papers/mem-ssa.pdf>`_. Though, it's; relatively out-of-date; the paper references multiple memory partitions, but GCC; eventually swapped to just using one, like we now have in LLVM. Like; GCC's, LLVM's MemorySSA is intraprocedural. MemorySSA Structure; ===================. MemorySSA is a virtual IR. After it's built, ``MemorySSA`` will contain a; structure that maps ``Instruction``\ s to ``MemoryAccess``\ es, which are; ``MemorySSA``'s parallel to LLVM ``Instruction``\ s. Each ``MemoryAccess`` can be one of three types:. - ``MemoryDef``; - ``MemoryPhi``; - ``MemoryUse``. ``MemoryDef``\ s are operations which may either modify memory, or which; introduce some kind of ordering constraints. Examples of ``MemoryDef``\ s; include ``store``\ s, function calls, ``load``\ s with ``acquire`` (or higher); ordering, volatile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9204,Usability,clear,clearly,9204,"act with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10675,Usability,simpl,simple,10675,"ies ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:17961,Usability,simpl,simply,17961,", and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:18302,Usability,simpl,simply,18302,"o; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new virtual variables,; phi nodes, etc. Each of these variables may be clobbered at multiple def sites. To give an example, if you were to split up struct fields into; individual variables, all aliasing operations that may-def multiple struct; fields, will may-def more than one of them. This is pretty common (calls,; copies, field stores, etc). Experience with SSA forms for memory in other compilers has shown that; it is simply not possible to do this precisely, and in fact, doing it; precisely is not worth it, because now all the optimizations have to; walk tons and tons of virtual variables and phi nodes. So we partition. At the point at which you partition, again,; experience has shown us there is no point in partitioning to more than; one variable. It simply generates more IR, and optimizations still; have to query something to disambiguate further anyway. As a result, LLVM partitions to one variable. Precision in practice; ^^^^^^^^^^^^^^^^^^^^^. In practice, there are implementation details in LLVM that also affect the; results' precision provided by ``MemorySSA``. For example, AliasAnalysis has various; caps, or restrictions on looking through phis which can affect what ``MemorySSA``; can infer. Changes made by different passes may make MemorySSA either ""overly; optimized"" (it can provide a more accurate result than if it were recomputed; from scratch), or ""under optimized"" (it could infer more if it were recomputed).; This can lead to challenges to reproduced results in isolation with a single pass; when the result relies on the state acquired by ``MemorySSA`` due to being updated by; multiple subsequent passes.; Passes that use and update ``MemorySSA`` should do so through the APIs provided by the; ``MemorySSAUpdater``, or through calls on the Walker.; Direct optimizations to ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemorySSA.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:330,Availability,error,error,330,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:459,Availability,error,errors,459,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1864,Availability,error,errors,1864,"ink your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:3078,Deployability,update,update,3078,"he start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-a-profile-architecture-2018-developments-armv85a; .. _AddressSanitizer: https://clang.llvm.org/docs/AddressSanitizer.html; .. _HardwareAssistedAddressSanitizer: https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:3236,Energy Efficiency,allocate,allocated,3236,"he start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-a-profile-architecture-2018-developments-armv85a; .. _AddressSanitizer: https://clang.llvm.org/docs/AddressSanitizer.html; .. _HardwareAssistedAddressSanitizer: https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1927,Modifiability,variab,variable,1927,"ink your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2158,Modifiability,variab,variable,2158,"x -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2258,Modifiability,variab,variable,2258,"gy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2358,Modifiability,variab,variable,2358,"gy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2454,Modifiability,variab,variables,2454,"gy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2647,Modifiability,variab,variables,2647,"enerate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:336,Safety,detect,detector,336,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:432,Safety,detect,detects,432,"================; MemTagSanitizer; ================. .. contents::; :local:. Introduction; ============. **Note:** this page describes a tool under development. Part of this; functionality is planned but not implemented. Hardware capable of; running MemTagSanitizer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1308,Safety,safe,safety,1308,"zer does not exist as of Oct 2019. MemTagSanitizer is a fast memory error detector and **a code hardening; tool** based on the Armv8.5-A `Memory Tagging Extension`_. It; detects a similar class of errors as `AddressSanitizer`_ or `HardwareAssistedAddressSanitizer`_, but with; **much** lower overhead. MemTagSanitizer overhead is expected to be in low single digits, both; CPU and memory. There are plans for a debug mode with slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1875,Safety,detect,detected,1875,"ink your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2179,Safety,safe,safety,2179,"x -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:1659,Security,access,access,1659," slightly higher; memory overhead and better diagnostics. The primary use case of; MemTagSanitizer is code hardening in production binaries, where it is; expected to be a strong mitigation for both stack and heap-based; memory bugs. Usage; =====. Compile and link your program with ``-fsanitize=memtag`` flag. This; will only work when targeting AArch64 with MemTag extension. One; possible way to achieve that is to add ``-target; aarch64-linux -march=armv8+memtag`` to compilation flags. Implementation; ==============. See `HardwareAssistedAddressSanitizer`_ for a general overview of a; tag-based approach to memory safety. MemTagSanitizer follows a; similar implementation strategy, but with the tag storage (shadow); provided by the hardware. A quick overview of MTE hardware capabilities:. * Every 16 aligned bytes of memory can be assigned a 4-bit Allocation Tag.; * Every pointer can have a 4-bit Address Tag that is in its most significant byte.; * Most memory access instructions generate an exception if Address Tag != Allocation Tag.; * Special instructions are provided for fast tag manipulation. Stack instrumentation; =====================. Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst:2787,Security,sanitiz,sanitizers,2787,"Stack-based memory errors are detected by updating Allocation Tag for; each local variable to a random value at the start of its lifetime,; and resetting it to the stack pointer Address Tag at the end of; it. Unallocated stack space is expected to match the Address Tag of; SP; this allows to skip tagging of any variable when memory safety can; be statically proven. Allocating a truly random tag for each stack variable in a large; function may incur significant code size overhead, because it means; that each variable's address is an independent, non-rematerializable; value; thus a function with N local variables will have extra N live; values to keep through most of its life time. For this reason MemTagSanitizer generates at most one random tag per; function, called a ""base tag"". Other stack variables, if there are; any, are assigned tags at a fixed offset from the base. Please refer to `this document; <https://github.com/google/sanitizers/wiki/Stack-instrumentation-with-ARM-Memory-Tagging-Extension-(MTE)>`_; for more details about stack instrumentation. Heap tagging; ============. **Note:** this part is not implemented as of Oct 2019. MemTagSanitizer will use :doc:`ScudoHardenedAllocator`; with additional code to update memory tags when. * New memory is obtained from the system.; * An allocation is freed. There is no need to change Allocation Tags for the bulk of the; allocated memory in malloc(), as long as a pointer with the matching; Address Tag is returned. More information; ================. * `LLVM Developer Meeting 2018 talk on Memory Tagging <https://llvm.org/devmtg/2018-10/slides/Serebryany-Stepanov-Tsyrklevich-Memory-Tagging-Slides-LLVM-2018.pdf>`_; * `Memory Tagging Whitepaper <https://arxiv.org/pdf/1802.09517.pdf>`_. .. _Memory Tagging Extension: https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-a-profile-architecture-2018-developments-armv85a; .. _AddressSanitizer: https://clang.llvm.org/docs/AddressSanitizer.html",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemTagSanitizer.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:853,Availability,down,down,853,"=================================; MergeFunctions pass, how it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/do",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:3097,Availability,down,down,3097,"p://en.wikipedia.org/wiki/Basic_block>`_"",; ""`user <https://llvm.org/docs/ProgrammersManual.html#the-user-class>`_"",; ""`value <https://llvm.org/docs/ProgrammersManual.html#the-value-class>`_"",; ""`instruction; <https://llvm.org/docs/ProgrammersManual.html#the-instruction-class>`_"". As a good starting point, the Kaleidoscope tutorial can be used:. :doc:`tutorial/index`. It's especially important to understand chapter 3 of tutorial:. :doc:`tutorial/LangImpl03`. The reader should also know how passes work in LLVM. They could use this; article as a reference and start point here:. :doc:`WritingAnLLVMPass`. What else? Well perhaps the reader should also have some experience in LLVM pass; debugging and bug-fixing. Narrative structure; -------------------; The article consists of three parts. The first part explains pass functionality; on the top-level. The second part describes the comparison procedure itself.; The third part describes the merging process. In every part, the author tries to put the contents in the top-down form.; The top-level methods will first be described followed by the terminal ones at; the end, in the tail of each part. If the reader sees the reference to the; method that wasn't described yet, they will find its description a bit below. Basics; ======. How to do it?; -------------; Do we need to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20002,Availability,ping,ping,20002,"t procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, boo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20097,Availability,ping,ping,20097,"t procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, boo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:28025,Availability,alive,alive,28025,"ld be good to do the; next: after merging the places where overridable function were used, still use; overridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If “F” may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: “whether the definition of this; global may be replaced by something non-equivalent at link time”. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same name and attributes like function; *F*. It takes maximum alignment of *F* and *G*. 2. Replace all uses of function *F* with uses of function *H*. It is the two; steps procedure instead. First of all, we must take into account, all functions; from whom *F* is called would be changed: since we change the call argument; (from *F* to *H*). If so we must to review these caller functions again after; this procedure. We remove callers from ``FnTree``, method with name; ``removeUsers(F)`` does that (don't confuse with ``replaceAllUsesWith``):. 2.1. ``Inside removeUsers(Value*; V)`` we go through the all values that use value *V* (or *F* in our context).; If value is instruction, we go to function that holds this instruction and; mark it as to-be-analyzed-again (put to ``Deferred`` set), we also remove; caller from ``FnTre",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:1325,Deployability,update,update,1325,".g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/docs/ProgrammersManual.html#the-function-class>`_"",; ""`basic block <http://en.wikipedia.org/wiki/Basic_block>`_"",; ""`user <https://llvm.org/docs/ProgrammersManual.html#the-user-class>`_"",; ""`value <https://llvm.org/docs/ProgrammersManual.html#the-value-class>`_"",; ""`instruction; <https://llvm.o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25946,Energy Efficiency,reduce,reduce,25946," both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs – return the result. 3. Compare operation types, use *cmpType*. All the same – if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``rep",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5153,Integrability,rout,routines,5153,"`void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the funct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6087,Integrability,rout,routine,6087,"rger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the fla",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6279,Integrability,depend,depends,6279,"y another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7898,Integrability,wrap,wrapper,7898,"tation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented “<” operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred`` – merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumerate only strong functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *D",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10326,Integrability,rout,routine,10326,"ingle function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10363,Integrability,rout,routines,10363,"ingle function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10428,Integrability,rout,routines,10428,"ingle function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:16364,Integrability,depend,depends,16364,"t* are to be expanded and their element types will be checked the same; way. If we get -1 or 1 on some stage, return it. Otherwise return 0. 8. Steps 1-6 describe all the possible cases, if we passed steps 1-6 and didn't; get any conclusions, then invoke ``llvm_unreachable``, since it's quite an; unexpectable case. cmpValues(const Value*, const Value*); -------------------------------------; Method that compares local values. This method gives us an answer to a very curious question: whether we could; treat local values as equal, and which value is greater otherwise. It's; better to start from example:. Consider the situation when we're looking at the same place in left; function ""*FL*"" and in right function ""*FR*"". Every part of *left* place is; equal to the corresponding part of *right* place, and (!) both parts use; *Value* instances, for example:. .. code-block:: text. instr0 i32 %LV ; left side, function FL; instr0 i32 %RV ; right side, function FR. So, now our conclusion depends on *Value* instances comparison. The main purpose of this method is to determine relation between such values. What can we expect from equal functions? At the same place, in functions; ""*FL*"" and ""*FR*"" we expect to see *equal* values, or values *defined* at; the same place in ""*FL*"" and ""*FR*"". Consider a small example here:. .. code-block:: text. define void %f(i32 %pf0, i32 %pf1) {; instr0 i32 %pf0 instr1 i32 %pf1 instr2 i32 123; }. .. code-block:: text. define void %g(i32 %pg0, i32 %pg1) {; instr0 i32 %pg0 instr1 i32 %pg0 instr2 i32 123; }. In this example, *pf0* is associated with *pg0*, *pf1* is associated with; *pg1*, and we also declare that *pf0* < *pf1*, and thus *pg0* < *pf1*. Instructions with opcode ""*instr0*"" would be *equal*, since their types and; opcodes are equal, and values are *associated*. Instructions with opcode ""*instr1*"" from *f* is *greater* than instructions; with opcode ""*instr1*"" from *g*; here we have equal types and opcodes, but; ""*pf1* is greater than ""*",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:18697,Integrability,depend,depends,18697," instances. In basic-block enumeration loop we associate *i*-th; BasicBlock from the left function with *i*-th BasicBlock from the right; function.; * Instructions.; * Instruction operands. Note, we can meet *Value* here we have never seen; before. In this case it is not a function argument, nor *BasicBlock*, nor; *Instruction*. It is a global value. It is a constant, since it's the only; supposed global here. The method also compares: Constants that are of the; same type and if right constant can be losslessly bit-casted to the left; one, then we also compare them. How to implement cmpValues?; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; *Association* is a case of equality for us. We just treat such values as equal,; but, in general, we need to implement antisymmetric relation. As mentioned; above, to understand what is *less*, we can use order in which we; meet values. If both values have the same order in a function (met at the same; time), we then treat values as *associated*. Otherwise – it depends on who was; first. Every time we run the top-level compare method, we initialize two identical; maps (one for the left side, another one for the right side):. ``map<Value, int> sn_mapL, sn_mapR;``. The key of the map is the *Value* itself, the *value* – is its order (call it; *serial number*). To add value *V* we need to perform the next procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemen",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:23133,Integrability,depend,depends,23133,"lass, return 1. 2.1.3. If both types are of the first class type, proceed to the next step; (2.1.3.1). 2.1.3.1. If types are vectors, compare their bitwidth using the; *cmpNumbers*. If result is not 0, return it. 2.1.3.2. Different types, but not a vectors:. * if both of them are pointers, good for us, we can proceed to step 3.; * if one of types is pointer, return result of *isPointer* flags; comparison (*cmpFlags* operation).; * otherwise we have no methods to prove bitcastability, and thus return; result of types comparison (-1 or 1). Steps below are for the case when types are equal, or case when constants are; bitcastable:. 3. One of constants is a ""*null*"" value. Return the result of; ``cmpFlags(L->isNullValue, R->isNullValue)`` comparison. 4. Compare value IDs, and return result if it is not 0:. .. code-block:: c++. if (int Res = cmpNumbers(L->getValueID(), R->getValueID())); return Res;. 5. Compare the contents of constants. The comparison depends on the kind of; constants, but on this stage it is just a lexicographical comparison. Just see; how it was described in the beginning of ""*Functions comparison*"" paragraph.; Mathematically, it is equal to the next case: we encode left constant and right; constant (with similar way *bitcode-writer* does). Then compare left code; sequence and right code sequence. compare(const BasicBlock*, const BasicBlock*); ---------------------------------------------; Compares two *BasicBlock* instances. It enumerates instructions from left *BB* and right *BB*. 1. It assigns serial numbers to the left and right instructions, using; ``cmpValues`` method. 2. If one of left or right is *GEP* (``GetElementPtr``), then treat *GEP* as; greater than other instructions. If both instructions are *GEPs* use ``cmpGEP``; method for comparison. If result is -1 or 1, pass it to the top-level; comparison (return it). 3.1. Compare operations. Call ``cmpOperation`` method. If result is -1 or; 1, return it. 3.2. Compare number of operands, if resul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:27231,Integrability,wrap,wrapper,27231,"ions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``replaceAllUsesWith`` operation here (*RAUW*). 2. *F* could not be overridden, while *G* could. It would be good to do the; next: after merging the places where overridable function were used, still use; overridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If “F” may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: “whether the definition of this; global may be replaced by something non-equivalent at link time”. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same nam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30309,Integrability,wrap,wrapper,30309," *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If “F” could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. “Aliases act as *second name* for the aliasee value”. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. “Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.”. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. “As-usual”: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30332,Integrability,interface,interface,30332," *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If “F” could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. “Aliases act as *second name* for the aliasee value”. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. “Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.”. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. “As-usual”: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30393,Integrability,wrap,wrapper,30393," *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If “F” could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. “Aliases act as *second name* for the aliasee value”. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. “Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.”. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. “As-usual”: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:31180,Integrability,wrap,wrapper,31180,"e *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If “F” could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. “Aliases act as *second name* for the aliasee value”. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. “Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.”. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. “As-usual”: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:31209,Integrability,interface,interface,31209,"e *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If “F” could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. “Aliases act as *second name* for the aliasee value”. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. “Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.”. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. “As-usual”: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4365,Performance,perform,performs,4365," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4418,Performance,perform,performs,4418," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5462,Performance,optimiz,optimization,5462,"t by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and te",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:11476,Performance,perform,perform,11476,"ill use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object comparison results. It is similar to the next ""tree"" objects; comparison:. #. For two trees *T1* and *T2* we perform *depth-first-traversal* and have; two sequences as a product: ""*T1Items*"" and ""*T2Items*"". #. We then compare chains ""*T1Items*"" and ""*T2Items*"" in; the most-significant-item-first order. The result of items comparison; would be the result of *T1* and *T2* comparison itself. FunctionComparator::compare(void); ---------------------------------; A brief look at the source code tells us that the comparison starts in the; “``int FunctionComparator::compare(void)``” method. 1. The first parts to be compared are the function's attributes and some; properties that is outside the “attributes” term, but still could make the; function different without changing its body. This part of the comparison is; usually done within simple *cmpNumbers* or *cmpFlags* operations (e.g.; ``cmpFlags(F1->hasGC(), F2->hasGC())``). Below is a full list of function's; properties to be compared on this stage:. * *Attributes* (those are returned by ``Function::getAttributes()``; method). * *GC*, for equivalence, *RHS* and *LHS* should be bot",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:19028,Performance,perform,perform,19028,"ion*. It is a global value. It is a constant, since it's the only; supposed global here. The method also compares: Constants that are of the; same type and if right constant can be losslessly bit-casted to the left; one, then we also compare them. How to implement cmpValues?; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; *Association* is a case of equality for us. We just treat such values as equal,; but, in general, we need to implement antisymmetric relation. As mentioned; above, to understand what is *less*, we can use order in which we; meet values. If both values have the same order in a function (met at the same; time), we then treat values as *associated*. Otherwise – it depends on who was; first. Every time we run the top-level compare method, we initialize two identical; maps (one for the left side, another one for the right side):. ``map<Value, int> sn_mapL, sn_mapR;``. The key of the map is the *Value* itself, the *value* – is its order (call it; *serial number*). To add value *V* we need to perform the next procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25724,Performance,load,load,25724,"asic block). cmpGEP; ------; Compares two GEPs (``getelementptr`` instructions). It differs from regular operations comparison with the only thing: possibility; to use ``accumulateConstantOffset`` method. So, if we get constant offset for both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs – return the result. 3. Compare operation types, use *cmpType*. All the same – if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:1294,Safety,avoid,avoid,1294,".g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/docs/ProgrammersManual.html#the-function-class>`_"",; ""`basic block <http://en.wikipedia.org/wiki/Basic_block>`_"",; ""`user <https://llvm.org/docs/ProgrammersManual.html#the-user-class>`_"",; ""`value <https://llvm.org/docs/ProgrammersManual.html#the-value-class>`_"",; ""`instruction; <https://llvm.o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:3568,Safety,detect,detect,3568," reference and start point here:. :doc:`WritingAnLLVMPass`. What else? Well perhaps the reader should also have some experience in LLVM pass; debugging and bug-fixing. Narrative structure; -------------------; The article consists of three parts. The first part explains pass functionality; on the top-level. The second part describes the comparison procedure itself.; The third part describes the merging process. In every part, the author tries to put the contents in the top-down form.; The top-level methods will first be described followed by the terminal ones at; the end, in the tail of each part. If the reader sees the reference to the; method that wasn't described yet, they will find its description a bit below. Basics; ======. How to do it?; -------------; Do we need to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4681,Safety,detect,detection,4681,"do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4718,Safety,detect,detector,4718,"do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4830,Safety,detect,detector,4830,"have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into ac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4869,Safety,detect,detectors,4869,"have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into ac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:26103,Safety,detect,detected,26103,"-; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs – return the result. 3. Compare operation types, use *cmpType*. All the same – if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``replaceAllUsesWith`` operation here (*RAUW*). 2. *F* could not be overridden, while *G* could. It would be good to do the; next: after merging the places where overridable function were ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5411,Security,hash,hashing,5411,"t by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and te",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5598,Security,access,access,5598,"; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5642,Security,access,access,5642,"es that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5769,Security,hash,hash-table,5769,"es that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5802,Security,hash,hashes,5802,"r functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5826,Security,hash,hashing,5826,"of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5989,Security,hash,hash,5989,"n parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6079,Security,hash,hashing,6079,"rger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the fla",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6394,Security,access,access,6394,"ashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6589,Security,hash,hashing,6589,"uld this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are tw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6700,Security,hash,hashing,6700,"uld this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are tw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6960,Security,access,access,6960," we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7076,Security,access,access,7076,"garithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented “<” operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred`` – ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7437,Security,hash,hash,7437,"f the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented “<” operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred`` – merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:7555,Security,access,access,7555,"rprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented “<” operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred`` – merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:658,Testability,log,logic,658,"=================================; MergeFunctions pass, how it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/do",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:1068,Testability,log,logic,1068,"ow it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/docs/ProgrammersManual.html#the-function-class>`_"",; ""`basi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:4431,Testability,log,logical,4431," to merge functions? The obvious answer is: Yes, that is quite a; possible case. We usually *do* have duplicates and it would be good to get rid; of them. But how do we detect duplicates? This is the idea: we split functions; into smaller bricks or parts and compare the ""bricks"" amount. If equal,; we compare the ""bricks"" themselves, and then do our conclusions about functions; themselves. What could the difference be? For example, on a machine with 64-bit pointers; (let's assume we have only one address space), one function stores a 64-bit; integer, while another one stores a pointer. If the target is the machine; mentioned above, and if functions are identical, except the parameter type (we; could consider it as a part of function type), then we can treat a ``uint64_t``; and a ``void*`` as equal. This is just an example; more possible details are described a bit below. As another example, the reader may imagine two more functions. The first; function performs a multiplication by 2, while the second one performs an; logical left shift by 1. Possible solutions; ^^^^^^^^^^^^^^^^^^; Let's briefly consider possible options about how and what we have to implement; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:5565,Testability,log,logarithmical,5565,"; in order to create full-featured functions merging, and also what it would; mean for us. Equal function detection obviously supposes that a ""detector"" method to be; implemented and latter should answer the question ""whether functions are equal"".; This ""detector"" method consists of tiny ""sub-detectors"", which each answers; exactly the same question, but for function parts. As the second step, we should merge equal functions. So it should be a ""merger""; method. ""Merger"" accepts two functions *F1* and *F2*, and produces *F1F2*; function, the result of merging. Having such routines in our hands, we can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6239,Testability,log,logarithmical,6239,"e can process a whole module, and merge all; equal functions. In this case, we have to compare every function with every another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6323,Testability,log,log,6323,"y another function. As; the reader may notice, this way seems to be quite expensive. Of course we could; introduce hashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6405,Testability,log,logarithmical,6405,"ashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6447,Testability,test,tested,6447,"ashing and other helpers, but it is still just an optimization, and; thus the level of O(N*N) complexity. Can we reach another level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6527,Testability,log,logarithmical,6527,"r level? Could we introduce logarithmical search, or random; access lookup? The answer is: ""yes"". Random-access; """"""""""""""""""""""""""; How it could this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6738,Testability,log,logarithmical-search,6738,"uld this be done? Just convert each function to a number, and gather; all of them in a special hash-table. Functions with equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are tw",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6806,Testability,log,logarithmical,6806,"equal hashes are equal.; Good hashing means, that every function part must be taken into account. That; means we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merge",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:6858,Testability,log,logarithmical-search,6858," we have to convert every function part into some number, and then add it; into the hash. The lookup-up time would be small, but such an approach adds some; delay due to the hashing routine. Logarithmical search; """"""""""""""""""""""""""""""""""""""""; We could introduce total ordering among the functions set, once ordered we; could then implement a logarithmical search. Lookup time still depends on N,; but adds a little of delay (*log(N)*). Present state; """"""""""""""""""""""""""; Both of the approaches (random-access and logarithmical) have been implemented; and tested and both give a very good improvement. What was most; surprising is that logarithmical search was faster; sometimes by up to 15%. The; hashing method needs some extra CPU time, which is the main reason why it works; slower; in most cases, total ""hashing"" time is greater than total; ""logarithmical-search"" time. So, preference has been granted to the ""logarithmical search"". Though in the case of need, *logarithmical-search* (read ""total-ordering"") could; be used as a milestone on our way to the *random-access* implementation. Every comparison is based either on the numbers or on the flags comparison. In; the *random-access* approach, we could use the same comparison algorithm.; During comparison, we exit once we find the difference, but here we might have; to scan the whole function body every time (note, it could be slower). Like in; ""total-ordering"", we will track every number and flag, but instead of; comparison, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:9153,Testability,log,logarithmical,9153," from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumerate only strong functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *Deferred* list. If it is not empty: refill the *worklist* contents with; *Deferred* list and redo step 2, if the *Deferred* list is empty, then exit; from method. Comparison and logarithmical search; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Let's recall our task: for every function *F* from module *M*, we have to find; equal functions *F`* in the shortest time possible , and merge them into a; single function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:9543,Testability,log,log,9543,"g functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *Deferred* list. If it is not empty: refill the *worklist* contents with; *Deferred* list and redo step 2, if the *Deferred* list is empty, then exit; from method. Comparison and logarithmical search; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Let's recall our task: for every function *F* from module *M*, we have to find; equal functions *F`* in the shortest time possible , and merge them into a; single function. Defining total ordering among the functions set allows us to organize; functions into a binary tree. The lookup procedure complexity would be; estimated as O(log(N)) in this case. But how do we define *total-ordering*?. We have to introduce a single rule applicable to every pair of functions, and; following this rule, then evaluate which of them is greater. What kind of rule; could it be? Let's declare it as the ""compare"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the foll",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:10771,Testability,log,logic,10771,"e"" method that returns one of 3; possible values:. -1, left is *less* than right,. 0, left and right are *equal*,. 1, left is *greater* than right. Of course it means, that we have to maintain; *strict and non-strict order relation properties*:. * reflexivity (``a <= a``, ``a == a``, ``a >= a``),; * antisymmetry (if ``a <= b`` and ``b <= a`` then ``a == b``),; * transitivity (``a <= b`` and ``b <= c``, then ``a <= c``); * asymmetry (if ``a < b``, then ``a > b`` or ``a == b``). As mentioned before, the comparison routine consists of; ""sub-comparison-routines"", with each of them also consisting of; ""sub-comparison-routines"", and so on. Finally, it ends up with primitive; comparison. Below, we will use the following operations:. #. ``cmpNumbers(number1, number2)`` is a method that returns -1 if left is less; than right; 0, if left and right are equal; and 1 otherwise. #. ``cmpFlags(flag1, flag2)`` is a hypothetical method that compares two flags.; The logic is the same as in ``cmpNumbers``, where ``true`` is 1, and; ``false`` is 0. The rest of the article is based on *MergeFunctions.cpp* source code; (found in *<llvm_dir>/lib/Transforms/IPO/MergeFunctions.cpp*). We would like; to ask reader to keep this file open, so we could use it as a reference; for further explanations. Now, we're ready to proceed to the next chapter and see how it works. Functions comparison; ====================; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object comparison results. It is similar to the next ""tree"" objects; comparison:. #. For two trees *T1* and *T2* we perform *depth-first-traversal* and have; two sequences as a product: ""*T1Items*"" and ""*T2Items*"". #. We then compare chains ""*T1Items*"" and ""*T2Items*"" in; the most-significant-item-first order. The result of items comparison; would be the result of *T1* and *T2* comparison itself. FunctionComparator::compare(void); --------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20375,Testability,test,test-suite,20375,"on:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())),; RightRes = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; if (LeftRes.first->second == RightRes.first->second) return 0;; if (LeftRes.first->second < RightRes.first->second) return -1;; return 1;. Now when *cmpValues* returns 0, we can proceed the comparison procedure.; Otherwise, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:20474,Testability,log,log,20474,"on:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the only case; we can't convert to less-equal-greater comparison. It is a seldom case, 4-5; functions of 10000 (checked in test-suite), and, we hope, the reader would; forgive us for such a sacrifice in order to get the O(log(N)) pass time. 2. If left/right *Value* is a constant, we have to compare them. Return 0 if it; is the same constant, or use ``cmpConstants`` method otherwise. 3. If left/right is *InlineAsm* instance. Return result of *Value* pointers; comparison. 4. Explicit association of *L* (left value) and *R* (right value). We need to; find out whether values met at the same time, and thus are *associated*. Or we; need to put the rule: when we treat *L* < *R*. Now it is easy: we just return; the result of numbers comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())),; RightRes = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; if (LeftRes.first->second == RightRes.first->second) return 0;; if (LeftRes.first->second < RightRes.first->second) return -1;; return 1;. Now when *cmpValues* returns 0, we can proceed the comparison procedure.; Otherwise, i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25747,Testability,log,log,25747,"tions). It differs from regular operations comparison with the only thing: possibility; to use ``accumulateConstantOffset`` method. So, if we get constant offset for both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs – return the result. 3. Compare operation types, use *cmpType*. All the same – if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. S",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:26000,Testability,log,log,26000," both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs – return the result. 3. Compare operation types, use *cmpType*. All the same – if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``rep",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:27159,Testability,stub,stub,27159,"* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of them with aliases to the third strong function *H*. Actually *H*; is *F*. See below how it's made (but it's better to look straight into the; source code). Well, this is a case when we can just replace *G* with *F*; everywhere, we use ``replaceAllUsesWith`` operation here (*RAUW*). 2. *F* could not be overridden, while *G* could. It would be good to do the; next: after merging the places where overridable function were used, still use; overridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If “F” may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: “whether the definition of this; global may be replaced by something non-equivalent at link time”. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:28124,Testability,stub,stub,28124,"verridable stub. So try to make *G* alias to *F*, or create overridable tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If “F” may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: “whether the definition of this; global may be replaced by something non-equivalent at link time”. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same name and attributes like function; *F*. It takes maximum alignment of *F* and *G*. 2. Replace all uses of function *F* with uses of function *H*. It is the two; steps procedure instead. First of all, we must take into account, all functions; from whom *F* is called would be changed: since we change the call argument; (from *F* to *H*). If so we must to review these caller functions again after; this procedure. We remove callers from ``FnTree``, method with name; ``removeUsers(F)`` does that (don't confuse with ``replaceAllUsesWith``):. 2.1. ``Inside removeUsers(Value*; V)`` we go through the all values that use value *V* (or *F* in our context).; If value is instruction, we go to function that holds this instruction and; mark it as to-be-analyzed-again (put to ``Deferred`` set), we also remove; caller from ``FnTree``. 2.2. Now we can do the replacement: call ``F->replaceAllUsesWith(H)``. 3. *H* (that now ""officiall",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:28187,Testability,stub,stub,28187," tail; call wrapper around *F* and replace *G* with that call. 3. Neither *F* nor *G* could be overridden. We can't use *RAUW*. We can just; change the callers: call *F* instead of *G*. That's what; ``replaceDirectCallers`` does. Below is a detailed body description. If “F” may be overridden; ------------------------; As follows from ``mayBeOverridden`` comments: “whether the definition of this; global may be replaced by something non-equivalent at link time”. If so, that's; ok: we can use alias to *F* instead of *G* or change call instructions itself. HasGlobalAliases, removeUsers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; First consider the case when we have global aliases of one function name to; another. Our purpose is make both of them with aliases to the third strong; function. Though if we keep *F* alive and without major changes we can leave it; in ``FnTree``. Try to combine these two goals. Do stub replacement of *F* itself with an alias to *F*. 1. Create stub function *H*, with the same name and attributes like function; *F*. It takes maximum alignment of *F* and *G*. 2. Replace all uses of function *F* with uses of function *H*. It is the two; steps procedure instead. First of all, we must take into account, all functions; from whom *F* is called would be changed: since we change the call argument; (from *F* to *H*). If so we must to review these caller functions again after; this procedure. We remove callers from ``FnTree``, method with name; ``removeUsers(F)`` does that (don't confuse with ``replaceAllUsesWith``):. 2.1. ``Inside removeUsers(Value*; V)`` we go through the all values that use value *V* (or *F* in our context).; If value is instruction, we go to function that holds this instruction and; mark it as to-be-analyzed-again (put to ``Deferred`` set), we also remove; caller from ``FnTree``. 2.2. Now we can do the replacement: call ``F->replaceAllUsesWith(H)``. 3. *H* (that now ""officially"" plays *F*'s role) is replaced with alias to *F*.; Do the same with *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:893,Usability,learn,learn,893,"=================================; MergeFunctions pass, how it works; =================================. .. contents::; :local:. Introduction; ============; Sometimes code contains equal functions, or functions that does exactly the same; thing even though they are non-equal on the IR level (e.g.: multiplication on 2; and 'shl 1'). It could happen due to several reasons: mainly, the usage of; templates and automatic code generators. Though, sometimes the user itself could; write the same thing twice :-). The main purpose of this pass is to recognize such functions and merge them. This document is the extension to pass comments and describes the pass logic. It; describes the algorithm that is used in order to compare functions and; explains how we could combine equal functions correctly to keep the module; valid. Material is brought in a top-down form, so the reader could start to learn pass; from high level ideas and end with low-level algorithm details, thus preparing; him or her for reading the sources. The main goal is to describe the algorithm and logic here and the concept. If; you *don't want* to read the source code, but want to understand pass; algorithms, this document is good for you. The author tries not to repeat the; source-code and covers only common cases to avoid the cases of needing to; update this document after any minor code changes. What should I know to be able to follow along with this document?; -----------------------------------------------------------------. The reader should be familiar with common compile-engineering principles and; LLVM code fundamentals. In this article, we assume the reader is familiar with; `Single Static Assignment; <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; concept and has an understanding of; `IR structure <https://llvm.org/docs/LangRef.html#high-level-structure>`_. We will use terms such as; ""`module <https://llvm.org/docs/LangRef.html#high-level-structure>`_"",; ""`function <https://llvm.org/do",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:8400,Usability,simpl,simple,8400,"on, we should get the numbers sequence and then create the hash number.; So, once again, *total-ordering* could be considered as a milestone for even; faster (in theory) random-access approach. MergeFunctions, main fields and runOnModule; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; There are two main important fields in the class:. ``FnTree`` – the set of all unique functions. It keeps items that couldn't be; merged with each other. It is defined as:. ``std::set<FunctionNode> FnTree;``. Here ``FunctionNode`` is a wrapper for ``llvm::Function`` class, with; implemented “<” operator among the functions set (below we explain how it works; exactly; this is a key point in fast functions comparison). ``Deferred`` – merging process can affect bodies of functions that are in; ``FnTree`` already. Obviously, such functions should be rechecked again. In this; case, we remove them from ``FnTree``, and mark them to be rescanned, namely; put them into ``Deferred`` list. runOnModule; """"""""""""""""""""""; The algorithm is pretty simple:. 1. Put all module's functions into the *worklist*. 2. Scan *worklist*'s functions twice: first enumerate only strong functions and; then only weak ones:. 2.1. Loop body: take a function from *worklist* (call it *FCur*) and try to; insert it into *FnTree*: check whether *FCur* is equal to one of functions; in *FnTree*. If there *is* an equal function in *FnTree*; (call it *FExists*): merge function *FCur* with *FExists*. Otherwise add; the function from the *worklist* to *FnTree*. 3. Once the *worklist* scanning and merging operations are complete, check the; *Deferred* list. If it is not empty: refill the *worklist* contents with; *Deferred* list and redo step 2, if the *Deferred* list is empty, then exit; from method. Comparison and logarithmical search; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Let's recall our task: for every function *F* from module *M*, we have to find; equal functions *F`* in the shortest time possible , and merge them into a; single func",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:12206,Usability,simpl,simple,12206,"=========; At first, let's define how exactly we compare complex objects. Complex object comparison (function, basic-block, etc) is mostly based on its; sub-object comparison results. It is similar to the next ""tree"" objects; comparison:. #. For two trees *T1* and *T2* we perform *depth-first-traversal* and have; two sequences as a product: ""*T1Items*"" and ""*T2Items*"". #. We then compare chains ""*T1Items*"" and ""*T2Items*"" in; the most-significant-item-first order. The result of items comparison; would be the result of *T1* and *T2* comparison itself. FunctionComparator::compare(void); ---------------------------------; A brief look at the source code tells us that the comparison starts in the; “``int FunctionComparator::compare(void)``” method. 1. The first parts to be compared are the function's attributes and some; properties that is outside the “attributes” term, but still could make the; function different without changing its body. This part of the comparison is; usually done within simple *cmpNumbers* or *cmpFlags* operations (e.g.; ``cmpFlags(F1->hasGC(), F2->hasGC())``). Below is a full list of function's; properties to be compared on this stage:. * *Attributes* (those are returned by ``Function::getAttributes()``; method). * *GC*, for equivalence, *RHS* and *LHS* should be both either without; *GC* or with the same one. * *Section*, just like a *GC*: *RHS* and *LHS* should be defined in the; same section. * *Variable arguments*. *LHS* and *RHS* should be both either with or; without *var-args*. * *Calling convention* should be the same. 2. Function type. Checked by ``FunctionComparator::cmpType(Type*, Type*)``; method. It checks return type and parameters type; the method itself will be; described later. 3. Associate function formal parameters with each other. Then comparing function; bodies, if we see the usage of *LHS*'s *i*-th argument in *LHS*'s body, then,; we want to see usage of *RHS*'s *i*-th argument at the same place in *RHS*'s; body, otherwise fun",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:19288,Usability,simpl,simple,19288,"so compare them. How to implement cmpValues?; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; *Association* is a case of equality for us. We just treat such values as equal,; but, in general, we need to implement antisymmetric relation. As mentioned; above, to understand what is *less*, we can use order in which we; meet values. If both values have the same order in a function (met at the same; time), we then treat values as *associated*. Otherwise – it depends on who was; first. Every time we run the top-level compare method, we initialize two identical; maps (one for the left side, another one for the right side):. ``map<Value, int> sn_mapL, sn_mapR;``. The key of the map is the *Value* itself, the *value* – is its order (call it; *serial number*). To add value *V* we need to perform the next procedure:. ``sn_map.insert(std::make_pair(V, sn_map.size()));``. For the first *Value*, map will return *0*, for the second *Value* map will; return *1*, and so on. We can then check whether left and right values met at the same time with; a simple comparison:. ``cmpNumbers(sn_mapL[Left], sn_mapR[Right]);``. Of course, we can combine insertion and comparison:. .. code-block:: c++. std::pair<iterator, bool>; LeftRes = sn_mapL.insert(std::make_pair(Left, sn_mapL.size())), RightRes; = sn_mapR.insert(std::make_pair(Right, sn_mapR.size()));; return cmpNumbers(LeftRes.first->second, RightRes.first->second);. Let's look, how whole method could be implemented. 1. We have to start with the bad news. Consider function self and; cross-referencing cases:. .. code-block:: c++. // self-reference unsigned fact0(unsigned n) { return n > 1 ? n; * fact0(n-1) : 1; } unsigned fact1(unsigned n) { return n > 1 ? n *; fact1(n-1) : 1; }. // cross-reference unsigned ping(unsigned n) { return n!= 0 ? pong(n-1) : 0;; } unsigned pong(unsigned n) { return n!= 0 ? ping(n-1) : 0; }. .. This comparison has been implemented in initial *MergeFunctions* pass; version. But, unfortunately, it is not transitive. And this is the onl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:30967,Usability,simpl,simple,30967,"e *G* was used we; also have alias to *F*. 4. Set *F* linkage to private. Make it strong :-). No global aliases, replaceDirectCallers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; If global aliases are not supported. We call ``replaceDirectCallers``. Just; go through all calls of *G* and replace it with calls of *F*. If you look into; the method you will see that it scans all uses of *G* too, and if use is callee; (if user is call instruction and *G* is used as what to be called), we replace; it with use of *F*. If “F” could not be overridden, fix it!; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". We call ``writeThunkOrAlias(Function *F, Function *G)``. Here we try to replace; *G* with alias to *F* first. The next conditions are essential:. * target should support global aliases,; * the address itself of *G* should be not significant, not named and not; referenced anywhere,; * function should come with external, local or weak linkage. Otherwise we write thunk: some wrapper that has *G's* interface and calls *F*,; so *G* could be replaced with this wrapper. *writeAlias*. As follows from *llvm* reference:. “Aliases act as *second name* for the aliasee value”. So we just want to create; a second name for *F* and use it instead of *G*:. 1. create global alias itself (*GA*),. 2. adjust alignment of *F* so it must be maximum of current and *G's* alignment;. 3. replace uses of *G*:. 3.1. first mark all callers of *G* as to-be-analyzed-again, using; ``removeUsers`` method (see chapter above),. 3.2. call ``G->replaceAllUsesWith(GA)``. 4. Get rid of *G*. *writeThunk*. As it written in method comments:. “Replace G with a simple tail call to bitcast(F). Also replace direct uses of G; with bitcast(F). Deletes G.”. In general it does the same as usual when we want to replace callee, except the; first point:. 1. We generate tail call wrapper around *F*, but with interface that allows use; it instead of *G*. 2. “As-usual”: ``removeUsers`` and ``replaceAllUsesWith`` then. 3. Get rid of *G*. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:22518,Availability,mask,mask,22518,"FI Index.; The other operands are tracked by the ``MCCFIInstruction`` object. The syntax is:. .. code-block:: text. CFI_INSTRUCTION offset $w30, -16. which may be emitted later in the MC layer as:. .. code-block:: text. .cfi_offset w30, -16. IntrinsicID Operands; ^^^^^^^^^^^^^^^^^^^^. An Intrinsic ID operand contains a generic intrinsic ID or a target-specific ID. The syntax for the ``returnaddress`` intrinsic is:. .. code-block:: text. $x0 = COPY intrinsic(@llvm.returnaddress). Predicate Operands; ^^^^^^^^^^^^^^^^^^. A Predicate operand contains an IR predicate from ``CmpInst::Predicate``, like; ``ICMP_EQ``, etc. For an int eq predicate ``ICMP_EQ``, the syntax is:. .. code-block:: text. %2:gpr(s32) = G_ICMP intpred(eq), %0, %1. .. TODO: Describe the parsers default behaviour when optional YAML attributes; are missing.; .. TODO: Describe the syntax for virtual register YAML definitions.; .. TODO: Describe the machine function's YAML flag attributes.; .. TODO: Describe the syntax for the register mask machine operands.; .. TODO: Describe the frame information YAML mapping.; .. TODO: Describe the syntax of the stack object machine operands and their; YAML definitions.; .. TODO: Describe the syntax of the block address machine operands.; .. TODO: Describe the syntax of the metadata machine operands, and the; instructions debug location attribute.; .. TODO: Describe the syntax of the register live out machine operands.; .. TODO: Describe the syntax of the machine memory operands. Comments; ^^^^^^^^. Machine operands can have C/C++ style comments, which are annotations enclosed; between ``/*`` and ``*/`` to improve readability of e.g. immediate operands.; In the example below, ARM instructions EOR and BCC and immediate operands; ``14`` and ``0`` have been annotated with their condition codes (CC); definitions, i.e. the ``always`` and ``eq`` condition codes:. .. code-block:: text. dead renamable $r2, $cpsr = tEOR killed renamable $r2, renamable $r1, 14 /* CC::always */, $",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:27747,Availability,avail,available,27747,"$noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR_REF`` s position records where the variable takes on the; designated value in the same way. More information about how these constructs are used is available in; :doc:`InstrRefDebugInfo`. The related documents :doc:`SourceLevelDebugging` and; :doc:`HowToUpdateDebugInfo` may be useful as well.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:10947,Energy Efficiency,power,power,10947,"lock:: text. bb.0.entry:; successors: %bb.1.then(32), %bb.2.else(16). .. _bb-liveins:. Live In Registers; ^^^^^^^^^^^^^^^^^. The machine basic block's live in registers have to be specified before any of; the instructions:. .. code-block:: text. bb.0.entry:; liveins: $edi, $esi. The list of live in registers and successors can be empty. The language also; allows multiple live in register and successor lists - they are combined into; one list by the parser. Miscellaneous Attributes; ^^^^^^^^^^^^^^^^^^^^^^^^. The attributes ``IsAddressTaken``, ``IsLandingPad``,; ``IsInlineAsmBrIndirectTarget`` and ``Alignment`` can be specified in brackets; after the block's definition:. .. code-block:: text. bb.0.entry (address-taken):; <instructions>; bb.2.else (align 4):; <instructions>; bb.3(landing-pad, align 4):; <instructions>; bb.4 (inlineasm-br-indirect-target):; <instructions>. .. TODO: Describe the way the reference to an unnamed LLVM IR block can be; preserved. ``Alignment`` is specified in bytes, and must be a power of two. .. _mir-instructions:. Machine Instructions; --------------------. A machine instruction is composed of a name,; :ref:`machine operands <machine-operands>`,; :ref:`instruction flags <instruction-flags>`, and machine memory operands. The instruction's name is usually specified before the operands. The example; below shows an instance of the X86 ``RETQ`` instruction with a single machine; operand:. .. code-block:: text. RETQ $eax. However, if the machine instruction has one or more explicitly defined register; operands, the instruction's name has to be specified after them. The example; below shows an instance of the AArch64 ``LDPXpost`` instruction with three; defined register operands:. .. code-block:: text. $sp, $fp, $lr = LDPXpost $sp, 2. The instruction names are serialized using the exact definitions from the; target's ``*InstrInfo.td`` files, and they are case sensitive. This means that; similar instruction names like ``TSTri`` and ``tSTRi`` repres",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:17740,Energy Efficiency,power,power,17740,"bregister indices are target specific, and are typically; defined in the target's ``*RegisterInfo.td`` file. Constant Pool Indices; ^^^^^^^^^^^^^^^^^^^^^. A constant pool index (CPI) operand is printed using its index in the; function's ``MachineConstantPool`` and an offset. For example, a CPI with the index 1 and offset 8:. .. code-block:: text. %1:gr64 = MOV64ri %const.1 + 8. For a CPI with the index 0 and offset -12:. .. code-block:: text. %1:gr64 = MOV64ri %const.0 - 12. A constant pool entry is bound to a LLVM IR ``Constant`` or a target-specific; ``MachineConstantPoolValue``. When serializing all the function's constants the; following format is used:. .. code-block:: text. constants:; - id: <index>; value: <value>; alignment: <alignment>; isTargetSpecific: <target-specific>. where:; - ``<index>`` is a 32-bit unsigned integer;; - ``<value>`` is a `LLVM IR Constant; <https://www.llvm.org/docs/LangRef.html#constants>`_;; - ``<alignment>`` is a 32-bit unsigned integer specified in bytes, and must be; power of two;; - ``<target-specific>`` is either true or false. Example:. .. code-block:: text. constants:; - id: 0; value: 'double 3.250000e+00'; alignment: 8; - id: 1; value: 'g-(LPC0+8)'; alignment: 4; isTargetSpecific: true. Global Value Operands; ^^^^^^^^^^^^^^^^^^^^^. The global value machine operands reference the global values from the; :ref:`embedded LLVM IR module <embedded-module>`.; The example below shows an instance of the X86 ``MOV64rm`` instruction that has; a global value operand named ``G``:. .. code-block:: text. $rax = MOV64rm $rip, 1, _, @G, _. The named global values are represented using an identifier with the '@' prefix.; If the identifier doesn't match the regular expression; `[-a-zA-Z$._][-a-zA-Z$._0-9]*`, then this identifier must be quoted. The unnamed global values are represented using an unsigned numeric value with; the '@' prefix, like in the following examples: ``@0``, ``@989``. Target-dependent Index Operands; ^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2729,Integrability,depend,dependent,2729," You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing late",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3962,Integrability,depend,depend,3962," line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4384,Integrability,depend,depend,4384,"ice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which st",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4572,Integrability,depend,depend,4572,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6022,Integrability,depend,depends,6022,"ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6209,Integrability,depend,depends,6209,". _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:14427,Integrability,depend,dependent,14427,"o be; represented using a '``$noreg``' named register, although the former syntax; is preferred. .. _machine-operands:. Machine Operands; ----------------. There are eighteen different kinds of machine operands, and all of them can be; serialized. Immediate Operands; ^^^^^^^^^^^^^^^^^^. The immediate machine operands are untyped, 64-bit signed integers. The; example below shows an instance of the X86 ``MOV32ri`` instruction that has an; immediate machine operand ``-42``:. .. code-block:: text. $eax = MOV32ri -42. An immediate operand is also used to represent a subregister index when the; machine instruction has one of the following opcodes:. - ``EXTRACT_SUBREG``. - ``INSERT_SUBREG``. - ``REG_SEQUENCE``. - ``SUBREG_TO_REG``. In case this is true, the Machine Operand is printed according to the target. For example:. In AArch64RegisterInfo.td:. .. code-block:: text. def sub_32 : SubRegIndex<32>;. If the third operand is an immediate with the value ``15`` (target-dependent; value), based on the instruction's opcode and the operand's index the operand; will be printed as ``%subreg.sub_32``:. .. code-block:: text. %1:gpr64 = SUBREG_TO_REG 0, %0, %subreg.sub_32. For integers > 64bit, we use a special machine operand, ``MO_CImmediate``,; which stores the immediate in a ``ConstantInt`` using an ``APInt`` (LLVM's; arbitrary precision integers). .. TODO: Describe the FPIMM immediate operands. .. _register-operands:. Register Operands; ^^^^^^^^^^^^^^^^^. The :ref:`register <registers>` primitive is used to represent the register; machine operands. The register operands can also have optional; :ref:`register flags <register-flags>`,; :ref:`a subregister index <subregister-indices>`,; and a reference to the tied register operand.; The full syntax of a register operand is shown below:. .. code-block:: text. [<flags>] <register> [ :<subregister-idx-name> ] [ (tied-def <tied-op>) ]. This example shows an instance of the X86 ``XOR32rr`` instruction that has; 5 register operands with ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:18672,Integrability,depend,dependent,18672,"d integer specified in bytes, and must be; power of two;; - ``<target-specific>`` is either true or false. Example:. .. code-block:: text. constants:; - id: 0; value: 'double 3.250000e+00'; alignment: 8; - id: 1; value: 'g-(LPC0+8)'; alignment: 4; isTargetSpecific: true. Global Value Operands; ^^^^^^^^^^^^^^^^^^^^^. The global value machine operands reference the global values from the; :ref:`embedded LLVM IR module <embedded-module>`.; The example below shows an instance of the X86 ``MOV64rm`` instruction that has; a global value operand named ``G``:. .. code-block:: text. $rax = MOV64rm $rip, 1, _, @G, _. The named global values are represented using an identifier with the '@' prefix.; If the identifier doesn't match the regular expression; `[-a-zA-Z$._][-a-zA-Z$._0-9]*`, then this identifier must be quoted. The unnamed global values are represented using an unsigned numeric value with; the '@' prefix, like in the following examples: ``@0``, ``@989``. Target-dependent Index Operands; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. A target index operand is a target-specific index and an offset. The; target-specific index is printed using target-specific names and a positive or; negative offset. For example, the ``amdgpu-constdata-start`` is associated with the index ``0``; in the AMDGPU backend. So if we have a target index operand with the index 0; and the offset 8:. .. code-block:: text. $sgpr2 = S_ADD_U32 _, target-index(amdgpu-constdata-start) + 8, implicit-def _, implicit-def _. Jump-table Index Operands; ^^^^^^^^^^^^^^^^^^^^^^^^^. A jump-table index operand with the index 0 is printed as following:. .. code-block:: text. tBR_JTr killed $r0, %jump-table.0. A machine jump-table entry contains a list of ``MachineBasicBlocks``. When serializing all the function's jump-table entries, the following format is used:. .. code-block:: text. jumpTable:; kind: <kind>; entries:; - id: <index>; blocks: [ <bbreference>, <bbreference>, ... ]. where ``<kind>`` is describing how the jump ta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4196,Modifiability,variab,variables,4196,"function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will cr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5820,Modifiability,variab,variable,5820,"lock; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:23988,Modifiability,variab,variable,23988,"nds.; .. TODO: Describe the syntax of the machine memory operands. Comments; ^^^^^^^^. Machine operands can have C/C++ style comments, which are annotations enclosed; between ``/*`` and ``*/`` to improve readability of e.g. immediate operands.; In the example below, ARM instructions EOR and BCC and immediate operands; ``14`` and ``0`` have been annotated with their condition codes (CC); definitions, i.e. the ``always`` and ``eq`` condition codes:. .. code-block:: text. dead renamable $r2, $cpsr = tEOR killed renamable $r2, renamable $r1, 14 /* CC::always */, $noreg; t2Bcc %bb.4, 0 /* CC:eq */, killed $cpsr. As these annotations are comments, they are ignored by the MI parser.; Comments can be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24495,Modifiability,variab,variable,24495,"t2Bcc %bb.4, 0 /* CC:eq */, killed $cpsr. As these annotations are comments, they are ignored by the MI parser.; Comments can be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24578,Modifiability,variab,variable,24578,"otations are comments, they are ignored by the MI parser.; Comments can be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24628,Modifiability,variab,variable,24628,"an be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24746,Modifiability,variab,variable,24746,"s; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24937,Modifiability,variab,variable,24937," constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:25048,Modifiability,variab,variable,25048,"ay optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an exp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:25155,Modifiability,variab,variable,25155,"ode, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:25422,Modifiability,variab,variable,25422,"``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referenc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26091,Modifiability,variab,variable,26091,"- ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26236,Modifiability,variab,variable,26236,"identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, bef",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26350,Modifiability,variab,variable,26350,"gging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26591,Modifiability,variab,variable,26591," is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26641,Modifiability,variab,variable,26641," is synonymous with the; ``llvm.dbg.value`` IR intrinsic, and is written:. .. code-block:: text. DBG_VALUE $rax, $noreg, !123, !DIExpression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:26683,Modifiability,variab,variable,26683,"Expression(), debug-location !456. The operands to which respectively:. 1. Identifies a machine location such as a register, immediate, or frame index,. 2. Is either $noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR_REF`` s position records where the variable takes on the; designated value in the same way. More information about how these const",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:27634,Modifiability,variab,variable,27634,"$noreg, or immediate value zero if an extra level of indirection is to be added to the first operand,. 3. Identifies a ``DILocalVariable`` metadata node,. 4. Specifies an expression qualifying the variable location, either inline or as a metadata node reference,. While the source location identifies the ``DILocation`` for the scope of the; variable. The second operand (``IsIndirect``) is deprecated and to be deleted.; All additional qualifiers for the variable location should be made through the; expression metadata. .. _instruction-referencing-locations:. Instruction referencing locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This experimental feature aims to separate the specification of variable; *values* from the program point where a variable takes on that value. Changes; in variable value occur in the same manner as ``DBG_VALUE`` meta instructions; but using ``DBG_INSTR_REF``. Variable values are identified by a pair of; instruction number and operand number. Consider the example below:. .. code-block:: text. $rbp = MOV64ri 0, debug-instr-number 1, debug-location !12; DBG_INSTR_REF !123, !DIExpression(DW_OP_LLVM_arg, 0), dbg-instr-ref(1, 0), debug-location !456. Instruction numbers are directly attached to machine instructions with an; optional ``debug-instr-number`` attachment, before the optional; ``debug-location`` attachment. The value defined in ``$rbp`` in the code; above would be identified by the pair ``<1, 0>``. The 3rd operand of the ``DBG_INSTR_REF`` above records the instruction; and operand number ``<1, 0>``, identifying the value defined by the ``MOV64ri``.; The first two operands to ``DBG_INSTR_REF`` are identical to ``DBG_VALUE_LIST``,; and the ``DBG_INSTR_REF`` s position records where the variable takes on the; designated value in the same way. More information about how these constructs are used is available in; :doc:`InstrRefDebugInfo`. The related documents :doc:`SourceLevelDebugging` and; :doc:`HowToUpdateDebugInfo` may be useful as well.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4421,Performance,perform,performed,4421," whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4508,Performance,load,load,4508," the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the momen",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4641,Performance,load,load,4641,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5191,Performance,load,loader,5191,"external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6784,Performance,load,load,6784,"structions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body: |; bb.0.entry:; liveins: $rdi. $eax = MOV32rm $rdi, 1, _, 0, _; $eax = INC32r killed $eax, implicit-def dead $eflags; MOV32mr killed $rdi, 1, _, 0, _, $eax; CALL64pcrel32 @foo <regmask...>; RETQ $eax; ... The document above consists of attributes that represent the various; properties and data structures in a machine function. The attribute ``name`` is required, and its value should be identical to the; name of a function that this machine function is based on. The attribute ``body`` is a `YAML block literal string`_. Its value represent",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3103,Security,access,accessible,3103,"n, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3352,Security,expose,exposesReturnsTwice,3352,"est.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:523,Testability,test,testing,523,"========================================; Machine IR (MIR) Format Reference Manual; ========================================. .. contents::; :local:. .. warning::; This is a work in progress. Introduction; ============. This document is a reference manual for the Machine IR (MIR) serialization; format. MIR is a human readable serialization format that is used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, yo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1145,Testability,test,testing,1145,"cal:. .. warning::; This is a work in progress. Introduction; ============. This document is a reference manual for the Machine IR (MIR) serialization; format. MIR is a human readable serialization format that is used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1197,Testability,test,tests,1197,"ent is a reference manual for the Machine IR (MIR) serialization; format. MIR is a human readable serialization format that is used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run mult",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1364,Testability,test,tests,1364,"used to represent; LLVM's :ref:`machine specific intermediate representation; <machine code representation>`. The MIR serialization format is designed to be used for testing the code; generation passes in LLVM. Overview; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1574,Testability,test,tests,1574,"view; ========. The MIR serialization format uses a YAML container. YAML is a standard; data serialization language, and the full YAML language spec can be read at; `yaml.org; <http://www.yaml.org/spec/1.2/spec.html#Introduction>`_. A MIR file is split up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1813,Testability,test,test,1813,"up into a series of `YAML documents`_. The first document; can contain an optional embedded LLVM IR module, and the rest of the documents; contain the serialized machine functions. .. _YAML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARG",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:1925,Testability,test,test,1925,"ML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2136,Testability,test,test,2136,"ML documents: http://www.yaml.org/spec/1.2/spec.html#id2800132. MIR Testing Guide; =================. You can use the MIR format for testing in two different ways:. - You can write MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2190,Testability,test,test,2190,"ite MIR tests that invoke a single code generation pass using the; ``-run-pass`` option in llc. - You can use llc's ``-stop-after`` option with existing or new LLVM assembly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2358,Testability,test,test,2358,"embly; tests and check the MIR output of a specific code generation pass. Testing Individual Code Generation Passes; -----------------------------------------. The ``-run-pass`` option in llc allows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `expose",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2492,Testability,test,test,2492,"llows you to create MIR tests that invoke just; a single code generation pass. When this option is used, llc will parse an; input MIR file, run the specified code generation pass(es), and output the; resulting MIR code. You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:2790,Testability,test,test,2790," You can generate an input MIR file for the test by using the ``-stop-after`` or; ``-stop-before`` option in llc. For example, if you would like to write a test; for the post register allocation pseudo instruction expansion pass, you can; specify the machine copy propagation pass in the ``-stop-after`` option, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing late",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3257,Testability,test,test,3257," index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; functi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3758,Testability,test,testing,3758,"he MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3949,Testability,test,test,3949," line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4371,Testability,test,test,4371,"ice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which st",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4559,Testability,test,test,4559,"t care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5936,Testability,test,test,5936,"y operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5972,Testability,test,tests,5972,"ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5997,Testability,test,test,5997,"ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6172,Testability,test,tests,6172,". _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:6183,Testability,test,test,6183,". _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on the state of the target specific ``MachineFunctionInfo`` or; ``MachineConstantPoolValue`` subclasses can't use the MIR format at the moment. High Level Structure; ====================. .. _embedded-module:. Embedded Module; ---------------. When the first YAML document contains a `YAML block literal string`_, the MIR; parser will treat this string as an LLVM assembly language string that; represents an embedded LLVM IR module.; Here is an example of a YAML document that contains an LLVM module:. .. code-block:: llvm. define i32 @inc(i32* %x) {; entry:; %0 = load i32, i32* %x; %1 = add i32 %0, 1; store i32 %1, i32* %x; ret i32 %1; }. .. _YAML block literal string: http://www.yaml.org/spec/1.2/spec.html#id2795688. Machine Functions; -----------------. The remaining YAML documents contain the machine functions. This is an example; of such YAML document:. .. code-block:: text. ---; name: inc; tracksRegLiveness: true; liveins:; - { reg: '$rdi' }; callSites:; - { bb: 0, offset: 3, fwdArgRegs:; - { arg: 0, reg: '$edi' } }; body:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3136,Usability,simpl,simplified,3136,"n, as it; runs just before the pass that we are trying to test:. ``llc -stop-after=machine-cp bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:3162,Usability,simpl,simplify-mir,3162,"p bug-trigger.ll -o test.mir``. If the same pass is run multiple times, a run index can be included; after the name with a comma. ``llc -stop-after=dead-mi-elimination,1 bug-trigger.ll -o test.mir``. After generating the input MIR file, you'll have to add a run line that uses; the ``-run-pass`` option to it. In order to test the post register allocation; pseudo instruction expansion pass on X86-64, a run line like the one shown; below can be used:. ``# RUN: llc -o - %s -mtriple=x86_64-- -run-pass=postrapseudos | FileCheck %s``. The MIR files are target dependent, so they have to be placed in the target; specific test directories (``lib/CodeGen/TARGETNAME``). They also need to; specify a target triple or a target architecture either in the run line or in; the embedded LLVM IR module. Simplifying MIR files; ^^^^^^^^^^^^^^^^^^^^^. The MIR code coming out of ``-stop-after``/``-stop-before`` is very verbose;; Tests are more accessible and future proof when simplified:. - Use the ``-simplify-mir`` option with llc. - Machine function attributes often have default values or the test works just; as well with default values. Typical candidates for this are: `alignment:`,; `exposesReturnsTwice`, `legalized`, `regBankSelected`, `selected`.; The whole `frameInfo` section is often unnecessary if there is no special; frame usage in the function. `tracksRegLiveness` on the other hand is often; necessary for some passes that care about block livein lists. - The (global) `liveins:` list is typically only interesting for early; instruction selection passes and can be removed when testing later passes.; The per-block `liveins:` on the other hand are necessary if; `tracksRegLiveness` is true. - Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equiv",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:4877,Usability,simpl,simply,4877,"Branch probability data in block `successors:` lists can be dropped if the; test doesn't depend on it. Example:; `successors: %bb.1(0x40000000), %bb.2(0x40000000)` can be replaced with; `successors: %bb.1, %bb.2`. - MIR code contains a whole IR module. This is necessary because there are; no equivalents in MIR for global variables, references to external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized righ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:24602,Usability,simpl,simplest,24602,"an be added or customized by overriding InstrInfo's hook; ``createMIROperandComment()``. Debug-Info constructs; ---------------------. Most of the debugging information in a MIR file is to be found in the metadata; of the embedded module. Within a machine function, that metadata is referred to; by various constructs to describe source locations and variable locations. Source locations; ^^^^^^^^^^^^^^^^. Every MIR instruction may optionally have a trailing reference to a; ``DILocation`` metadata node, after all operands and symbols, but before; memory operands:. .. code-block:: text. $rbp = MOV64rr $rdi, debug-location !12. The source location attachment is synonymous with the ``!dbg`` metadata; attachment in LLVM-IR. The absence of a source location attachment will be; represented by an empty ``DebugLoc`` object in the machine instruction. Fixed variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^. There are several ways of specifying variable locations. The simplest is; describing a variable that is permanently located on the stack. In the stack; or fixedStack attribute of the machine function, the variable, scope, and; any qualifying location modifier are provided:. .. code-block:: text. - { id: 0, name: offset.addr, offset: -24, size: 8, alignment: 8, stack-id: default,; 4 debug-info-variable: '!1', debug-info-expression: '!DIExpression()',; debug-info-location: '!2' }. Where:. - ``debug-info-variable`` identifies a DILocalVariable metadata node,. - ``debug-info-expression`` adds qualifiers to the variable location,. - ``debug-info-location`` identifies a DILocation metadata node. These metadata attributes correspond to the operands of a ``llvm.dbg.declare``; IR intrinsic, see the :ref:`source level debugging<format_common_intrinsics>`; documentation. Varying variable locations; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Variables that are not always on the stack or change location are specified; with the ``DBG_VALUE`` meta machine instruction. It is synonymous with the; ``llvm.dbg.valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2157,Availability,avail,available,2157,"erification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1139,Integrability,message,message,1139,"expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:280,Performance,optimiz,optimizer,280,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:650,Performance,optimiz,optimizer,650,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1186,Performance,perform,perform,1186,"expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1546,Performance,perform,perform,1546,"something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1959,Performance,perform,performance,1959,"g input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+===========================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2182,Performance,optimiz,optimization,2182,"erification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2336,Performance,optimiz,optimization,2336,"re branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation added during by the LLVM backend |; +----------------+--------------------------------------------------------------------------------------+; | CS-IR | Context Sensitive IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:613,Safety,detect,detect,613,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:958,Usability,simpl,simple,958,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MisExpect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:836,Availability,echo,echo,836,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2014,Availability,down,download,2014,"s is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3155,Availability,down,down,3155," code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a syml",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5378,Availability,error,error,5378,"but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We shou",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5726,Availability,error,errors,5726,"rget name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; *******************",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12286,Availability,ping,ping,12286,"-------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13580,Availability,error,errors,13580,"e-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even whe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14662,Availability,failure,failure,14662,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14780,Availability,error,error,14780,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:15000,Availability,error,error,15000,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:613,Deployability,install,install,613,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1765,Deployability,install,install,1765,"arning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-proj",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2576,Deployability,install,installed,2576,"figure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run unde",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5774,Deployability,update,update,5774,"me we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10178,Deployability,update,updated,10178,"ign goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10788,Deployability,update,update,10788,"o open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11468,Deployability,patch,patch,11468,"the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11929,Deployability,patch,patches,11929,"ing a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12714,Deployability,patch,patches,12714,"oal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a varie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12836,Deployability,patch,patches,12836,"at do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13545,Deployability,patch,patch,13545," like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broke",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13714,Deployability,patch,patch,13714,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13736,Deployability,configurat,configurations,13736,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13768,Deployability,configurat,configurations,13768,"ly a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confus",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13943,Deployability,configurat,configurations,13943,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13976,Deployability,configurat,configuration,13976,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14151,Deployability,patch,patch,14151,"loper; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14408,Deployability,continuous,continuously,14408,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11627,Energy Efficiency,sustainab,sustainable,11627,"Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the directio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13214,Energy Efficiency,power,power,13214,"ave to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5384,Integrability,message,message,5384,"but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We shou",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5758,Integrability,message,messages,5758,"rget name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; *******************",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5786,Integrability,message,message,5786,"me we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5906,Integrability,message,message,5906," changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5985,Integrability,message,message,5985,":. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6902,Integrability,message,message,6902,"ang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:8547,Integrability,message,message,8547," LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9961,Integrability,depend,depending,9961,"er; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11371,Integrability,message,message,11371,"change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1549,Modifiability,config,configure,1549,"or,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2363,Modifiability,config,configure,2363,"ou'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3376,Modifiability,variab,variables,3376," to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools lik",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11665,Modifiability,maintainab,maintainable,11665,"Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the directio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13736,Modifiability,config,configurations,13736,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13768,Modifiability,config,configurations,13768,"ly a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confus",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13943,Modifiability,config,configurations,13943,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13976,Modifiability,config,configuration,13976,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2951,Performance,perform,performing,2951," one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in bui",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3469,Performance,optimiz,optimized,3469,"ices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12426,Performance,optimiz,optimize,12426,"quest will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2529,Safety,detect,detected,2529,"figure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run unde",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9021,Security,access,access,9021,"n suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11266,Security,access,access,11266,".org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11441,Security,access,access,11441,"the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12654,Security,access,access,12654,"be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12768,Security,access,access,12768,"oal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a varie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13009,Security,access,access,13009," `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a whi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13191,Security,access,access,13191,"ime consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:423,Testability,test,tests,423,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:869,Testability,test,test,869,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:897,Testability,test,test,897,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1696,Testability,test,tests,1696,",. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:2962,Testability,test,test,2962," one large repository; (""the monorepo""). It may take a while to download!. .. code:: console. $ git clone https://github.com/llvm/llvm-project.git. This will create a directory ""llvm-project"" with all of the source; code. (Checking out anonymously is OK - pushing commits uses a different; mechanism, as we'll see later.). Configure your workspace; ------------------------. Before we can build the code, we must configure exactly how to build it; by running CMake. CMake combines information from three sources:. - explicit choices you make (is this a debug build?). - settings detected from your system (where are libraries installed?). - project structure (which files are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in bui",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3661,Testability,test,test,3661,"are part of 'clang'?). First, create a directory to build in. Usually, this is; llvm-project/build. .. code:: console. $ mkdir llvm-project/build; $ cd llvm-project/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to buil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:3769,Testability,assert,assertions,3769,"/build. Now, run CMake:. .. code:: console. $ cmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=clang. If all goes well, you'll see a lot of ""performing test"" lines, and; finally:. .. code:: console. Configuring done; Generating done; Build files have been written to: /path/llvm-project/build. And you should see a build.ninja file. Let's break down that last command a little:. - **-G Ninja**: we're going to use ninja to build; please create; build.ninja. - **../llvm**: this is the path to the source of the ""main"" LLVM; project. - The two **-D** flags set CMake variables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The fi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:4355,Testability,test,testing,4355,"iables, which override; CMake/project defaults:. - **CMAKE_BUILD_TYPE=Release**: build in optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the er",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:4464,Testability,test,test,4464,"optimized mode, which is; (surprisingly) the fastest option. If you want to run under a debugger, you should use the default Debug; (which is totally unoptimized, and will lead to >10x slower test; runs) or RelWithDebInfo which is a halfway point.; **CMAKE_BUILD_TYPE** affects code generation only, assertions are; on by default regardless! **LLVM_ENABLE_ASSERTIONS=Off** disables; them. - **LLVM_ENABLE_PROJECTS=clang**: this lists the LLVM subprojects; you are interested in building, in addition to LLVM itself. Multiple; projects can be listed, separated by semicolons, such as ""clang;; lldb"".In this example, we'll be making a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:5127,Testability,test,tests,5127," a change to Clang, so we; should build it. Finally, create a symlink (or a copy) of; llvm-project/build/compile-commands.json into llvm-project/:. .. code:: console. $ ln -s build/compile_commands.json ../. (This isn't strictly necessary for building and testing, but allows; tools like clang-tidy, clang-query, and clangd to work in your source; tree). Build and test; --------------. Finally, we can build the code! It's important to do this first, to; ensure we're in a good state before making changes. But what to build?; In ninja, you specify a **target**. If we just want to build the clang; binary, our target name is ""clang"" and we run:. .. code:: console. $ ninja clang. The first time we build will be very slow - Clang + LLVM is a lot of; code. But incremental builds are fast: ninja will only rebuild the parts; that have changed. When it finally finishes you should have a working; clang binary. Try running:. .. code:: console. $ bin/clang --version. There's also a target for building and running all the clang tests:. .. code:: console. $ ninja check-clang. This is a common pattern in LLVM: check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To ver",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6224,Testability,test,test,6224," check-llvm is all the checks for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something simila",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6239,Testability,test,test,6239,"s for core,; other projects have targets like check-lldb. Making changes; ==============. Edit; ----. We need to find the file containing the error message. .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6381,Testability,test,tests,6381,". .. code:: console. $ git grep ""all paths through this function"" ..; ../clang/include/clang/Basic/DiagnosticSemaKinds.td: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; itera",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6525,Testability,test,tests,6525,"d: ""all paths through this function will call itself"">,. The string that appears in DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6583,Testability,test,tests,6583,"n DiagnosticSemaKinds.td is the one that is; printed by Clang. \*.td files define tables - in this case it's a list; of warnings and errors clang can emit and their messages. Let's update; the message in your favorite editor:. .. code:: console. $ vi ../clang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6814,Testability,test,test,6814,"ang/include/clang/Basic/DiagnosticSemaKinds.td. Find the message (it should be under; ``warn_infinite_recursive_function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6935,Testability,test,tests,6935,"function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:6974,Testability,test,tests,6974,"function``). Change the message to ""in order to; understand recursion, you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7053,Testability,test,test,7053," you must first understand recursion"". Test again; ----------. To verify our change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7092,Testability,test,test,7092," change, we can build clang and manually check that it; works. .. code:: console. $ ninja clang; $ bin/clang -Wall ~/test.cc; /path/test.cc:1:124: warning: in order to understand recursion, you must; first understand recursion [-Winfinite-recursion]. We should also run the tests to make sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7347,Testability,test,tests,7347,"ke sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future rea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7463,Testability,test,test,7463,"ke sure we didn't break something. .. code:: console. $ ninja check-clang. Notice that it is much faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future rea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7497,Testability,test,tests,7497,"faster to build this time, but the tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ g",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7521,Testability,test,tests,7521,"he tests take; just as long to run. Ninja doesn't know which tests might be affected,; so it runs them all. .. code:: console. ********************; Testing Time: 408.84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7688,Testability,test,test,7688,".84s; ********************; Failing Tests (1):; Clang :: SemaCXX/warn-infinite-recursion.cpp. Well, that makes sense… and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7791,Testability,test,test,7791,"and the test output suggests it's looking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:7842,Testability,test,tests,7842,"oking for; the old string ""call itself"" and finding our new message instead.; Note that more tests may fail in a similar way as new tests are; added time to time. Let's fix it by updating the expectation in the test. .. code:: console. $ vi ../clang/test/SemaCXX/warn-infinite-recursion.cpp. Everywhere we see `// expected-warning{{call itself}}` (or something similar; from the original warning text), let's replace it with; `// expected-warning{{to understand recursion}}`. Now we could run **all** the tests again, but this is a slow way to; iterate on a change! Instead, let's find a way to re-run just the; specific test. There are two main types of tests in LLVM:. - **lit tests** (e.g. SemaCXX/warn-infinite-recursion.cpp). These are fancy shell scripts that run command-line tools and verify the; output. They live in files like; clang/**test**/FixIt/dereference-addressof.c. Re-run like this:. .. code:: console. $ bin/llvm-lit -v ../clang/test/SemaCXX/warn-infinite-recursion.cpp. - **unit tests** (e.g. ToolingTests/ReplacementTest.CanDeleteAllText). These are C++ programs that call LLVM functions and verify the results.; They live in suites like ToolingTests. Re-run like this:. .. code:: console. $ ninja ToolingTests && tools/clang/unittests/Tooling/ToolingTests; --gtest_filter=ReplacementTest.CanDeleteAllText. Commit locally; --------------. We'll save the change to a local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://git",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9255,Testability,log,log,9255," local git branch. This lets us work on other; things while the change is being reviewed. Changes should have a; description, to explain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:11687,Testability,test,tested,11687,"Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the directio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13704,Testability,test,test,13704,"ce you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:13936,Testability,test,tested,13936,"ours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:14786,Testability,log,logs,14786,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:15006,Testability,log,logs,15006,"icy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.org), as discussion often happens; there if a new patch causes problems. Post-commit errors; ------------------. Once your change is submitted it will be picked up by automated build; bots that will build and test your patch in a variety of configurations. You can see all configurations and their current state in a waterfall; view at http://lab.llvm.org/buildbot/#/waterfall. The waterfall view is good; to get a general overview over the tested configurations and to see; which configuration have been broken for a while. The console view at http://lab.llvm.org/buildbot/#/console helps to get a; better understanding of the build results of a specific patch. If you; want to follow along how your change is affecting the build bots, **this; should be the first place to look at** - the colored bubbles correspond; to projects in the waterfall. If you see a broken build, do not despair - some build bots are; continuously broken; if your change broke the build, you will see a red; bubble in the console view, while an already broken build will show an; orange bubble. Of course, even when the build was already broken, a new; change might introduce a hidden new failure. | When you want to see more details how a specific build is broken,; click the red bubble.; | If post-commit error logs confuse you, do not worry too much -; everybody on the project is aware that this is a bit unwieldy, so; expect people to jump in and help you understand what's going on!. buildbots, overview of bots, getting error logs. Reverts; -------. If in doubt, revert immediately, and re-land later after investigation; and fix. Conclusion; ==========. llvm is a land of contrasts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:119,Usability,guid,guide,119,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:355,Usability,simpl,simple,355,"==============; MyFirstTypoFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may tak",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:1012,Usability,clear,clear,1012,"oFix; ==============. .. contents::; :local:. Introduction; ============. This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project. We'll be making a; change to Clang, but the steps for other parts of LLVM are the same.; Even though the change we'll be making is simple, we're going to cover; steps like building LLVM, running the tests, and code review. This is; good practice, and you'll be prepared for making larger changes. We'll assume you:. - know how to use an editor,. - have basic C++ knowledge,. - know how to install software on your system,. - are comfortable with the command line,. - have basic knowledge of git. The change we're making; -----------------------. Clang has a warning for infinite recursion:. .. code:: console. $ echo ""void foo() { foo(); }"" > ~/test.cc; $ clang -c -Wall ~/test.cc; input.cc:1:14: warning: all paths through this function will call; itself [-Winfinite-recursion]. This is clear enough, but not exactly catchy. Let's improve the wording; a little:. .. code:: console. input.cc:1:14: warning: to understand recursion, you must first; understand recursion [-Winfinite-recursion]. Dependencies; ------------. We're going to need some tools:. - git: to check out the LLVM source code,. - a C++ compiler: to compile LLVM source code. You'll want `a recent; version <https://llvm.org/docs/GettingStarted.html#host-c-toolchain-both-compiler-and-standard-library>`__; of Clang, GCC, or Visual Studio. - CMake: used to configure how LLVM should be built on your system,. - ninja: runs the C++ compiler to (re)build specific parts of LLVM,. - python: to run the LLVM tests,. As an example, on Ubuntu:. .. code:: console. $ sudo apt-get install git clang cmake ninja-build python arcanist. Building LLVM; =============. Checkout; --------. The source code is stored `on; Github <https://github.com/llvm/llvm-project>`__ in one large repository; (""the monorepo""). It may take a while to download!. .",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:9311,Usability,simpl,simple,9311,"xplain to reviewers and future readers of the code why; the change was made. .. code:: console. $ git checkout -b myfirstpatch; $ git commit -am ""[Diagnostic] Clarify -Winfinite-recursion message"". Now we're ready to send this change out into the world! By the way,; There is an unwritten convention of using tag for your commit. Tags; usually represent modules that you intend to modify. If you don't know; the tags for your modules, you can look at the commit history :; https://github.com/llvm/llvm-project/commits/main. Code review; ===========. Finding a reviewer; ------------------. Changes can be reviewed by anyone in the LLVM community who has commit; access.For larger and more complicated changes, it's important that the; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10098,Usability,simpl,simply,10098,"e; reviewer has experience with the area of LLVM and knows the design goals; well. The author of a change will often assign a specific reviewer (git; blame and git log can be useful to find one). As our change is fairly simple, we'll add the cfe-commits mailing list; as a subscriber; anyone who works on clang can likely pick up the; review. (For changes outside clang, llvm-commits is the usual list. See; `http://lists.llvm.org/ <http://lists.llvm.org/mailman/listinfo>`__ for; all the \*-commits mailing lists). Uploading a change for review; -----------------------------. LLVM code reviews happen through pull-request on GitHub, see; :ref:`GitHub <github-reviews>` documentation for how to open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:10781,Usability,simpl,simply,10781,"o open; a pull-request on GitHub. Review process; --------------. When you open a pull-request, some automation will add a comment and; notify different member of the projects depending on the component you; changed.; Within a few days, someone should start the review. They may add; themselves as a reviewer, or simply start leaving comments. You'll get; another email any time the review is updated. The details are in the; `https://llvm.org/docs/CodeReview/ <https://llvm.org/docs/CodeReview.html>`__. Comments; ~~~~~~~~. The reviewer can leave comments on the change, and you can reply. Some; comments are attached to specific lines, and appear interleaved with the; code. You can either reply to these, or address them and mark them as; ""done"". Note that in-line replies are **not** sent straight away! They; become ""draft"" comments and you must click ""Submit"" at the bottom of the; page. Updating your change; ~~~~~~~~~~~~~~~~~~~~. If you make changes in response to a reviewer's comments, simply update; your branch with more commits and push to your fork. It may be a good; idea to answer the comments from the reviewer explicitly. Accepting a revision; ~~~~~~~~~~~~~~~~~~~~. When the reviewer is happy with the change, they will **Accept** the; revision. They may leave some more minor comments that you should; address, but at this point the review is complete. It's time to get it; merged!. Commit by proxy; ---------------. As this is your first change, you won't have access to merge it; yourself yet. The reviewer **doesn't know this**, so you need to tell; them! Leave a message on the review like:. Thanks @somellvmdev. I don't have commit access, can you land this; patch for me?. The pull-request will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst:12618,Usability,feedback,feedback,12618,"quest will be closed and you will be notified by GitHub. Review expectations; -------------------. In order to make LLVM a long-term sustainable effort, code needs to be; maintainable and well tested. Code reviews help to achieve that goal.; Especially for new contributors, that often means many rounds of reviews; and push-back on design decisions that do not fit well within the; overall architecture of the project. For your first patches, this means:. - be kind, and expect reviewers to be kind in return - LLVM has a `Code; of Conduct <https://llvm.org/docs/CodeOfConduct.html>`__;. - be patient - understanding how a new feature fits into the; architecture of the project is often a time consuming effort, and; people have to juggle this with other responsibilities in their; lives; **ping the review once a week** when there is no response;. - if you can't agree, generally the best way is to do what the reviewer; asks; we optimize for readability of the code, which the reviewer is; in a better position to judge; if this feels like it's not the right; option, you can contact the cfe-dev mailing list to get more feedback; on the direction;. Commit access; =============. Once you've contributed a handful of patches to LLVM, start to think; about getting commit access yourself. It's probably a good idea if:. - you've landed 3-5 patches of larger scope than ""fix a typo"". - you'd be willing to review changes that are closely related to yours. - you'd like to keep contributing to LLVM. Getting commit access; ---------------------. LLVM uses Git for committing changes. The details are in the `developer; policy; document <https://llvm.org/docs/DeveloperPolicy.html#obtaining-commit-access>`__. With great power; ----------------. Actually, this would be a great time to read the rest of the `developer; policy <https://llvm.org/docs/DeveloperPolicy.html>`__, too. At minimum,; you need to be subscribed to the relevant commits list before landing; changes (e.g. llvm-commits@lists.llvm.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MyFirstTypoFix.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7795,Availability,down,down,7795,"============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR ana",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19044,Availability,error,error,19044,"s of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19949,Availability,avail,available,19949,"debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing effor",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20184,Availability,avail,available,20184,"nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:1254,Deployability,pipeline,pipeline,1254,"he-new-pass-manager/>`_. Just Tell Me How To Run The Default Optimization Pipeline With The New Pass Manager; ===================================================================================. .. code-block:: c++. // Create the analysis managers.; // These must be declared in this order so that they are destroyed in the; // correct order due to inter-analysis-manager references.; LoopAnalysisManager LAM;; FunctionAnalysisManager FAM;; CGSCCAnalysisManager CGAM;; ModuleAnalysisManager MAM;. // Create the new pass manager builder.; // Take a look at the PassBuilder constructor parameters for more; // customization, e.g. specifying a TargetMachine or various debugging; // options.; PassBuilder PB;. // Register all the basic analyses with the managers.; PB.registerModuleAnalyses(MAM);; PB.registerCGSCCAnalyses(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5251,Deployability,pipeline,pipeline,5251,"s1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5368,Deployability,pipeline,pipeline,5368,"M;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipelin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5615,Deployability,pipeline,pipeline,5615,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5863,Deployability,pipeline,pipeline,5863," to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6202,Deployability,pipeline,pipeline,6202,"ss manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6327,Deployability,pipeline,pipeline,6327,"level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6460,Deployability,pipeline,pipeline,6460,"ay want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6516,Deployability,pipeline,pipelines,6516,"ntrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9369,Deployability,update,updates,9369," function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9735,Deployability,pipeline,pipeline,9735,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11088,Deployability,update,updated,11088,"tation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12156,Deployability,update,update,12156,"alyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12480,Deployability,update,update,12480,"ay to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12543,Deployability,update,updaters,12543,"ay to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12915,Deployability,update,update,12915,"is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:12994,Deployability,update,updated,12994,"are what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c++. // We've made no transformations that can affect any analyses.; return PreservedAnalyses::all();. // We've made transformations and don't want to bother to update any analyses.; return PreservedAnalyses::none();. // We've specifically updated the dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20243,Deployability,pipeline,pipeline,20243,"nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20762,Deployability,pipeline,pipeline,20762,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20934,Deployability,pipeline,pipeline,20934,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21077,Deployability,pipeline,pipeline,21077,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21486,Deployability,pipeline,pipelines,21486,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21708,Deployability,pipeline,pipeline,21708,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:2076,Energy Efficiency,adapt,adaptor,2076,"es(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePass is a loop pass; FPM.addPass(createFunctionToLoopPassAdaptor(LoopRotatePass()));. The IR hierarchy in terms of the new PM is Module -> (CGSCC ->) Function ->; Loop, where going through a CGSCC is optional. .. code-block:: c++. FunctionPassManager FPM;; // loop -> function; FPM.addPass(createFunctionToLoopPassAdaptor(LoopFooPass()));. CGSCCPassManager CGPM;; // loop -> function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(FunctionFooPass()));. ModulePassManager MPM;; // loop -> function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:3865,Energy Efficiency,adapt,adaptors,3865,"function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass()));. // loop -> function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass()))));; // function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(FunctionFooPass())));. A pass manager of a specific IR unit is also a pass of that kind. For; example, a ``FunctionPassManager`` is a function pass, meaning it can be; added to a ``ModulePassManager``:. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:18641,Energy Efficiency,adapt,adaptor,18641,"ysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19654,Energy Efficiency,adapt,adaptor,19654,"ction(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contai",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:2048,Integrability,wrap,wrapped,2048,"es(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePass is a loop pass; FPM.addPass(createFunctionToLoopPassAdaptor(LoopRotatePass()));. The IR hierarchy in terms of the new PM is Module -> (CGSCC ->) Function ->; Loop, where going through a CGSCC is optional. .. code-block:: c++. FunctionPassManager FPM;; // loop -> function; FPM.addPass(createFunctionToLoopPassAdaptor(LoopFooPass()));. CGSCCPassManager CGPM;; // loop -> function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(FunctionFooPass()));. ModulePassManager MPM;; // loop -> function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5344,Integrability,inject,inject,5344,"M;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipelin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5571,Integrability,inject,injecting,5571,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6179,Integrability,inject,inject,6179,"ss manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10841,Integrability,depend,depend,10841," potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:16476,Integrability,depend,depends,16476,"servedSet<AllAnalysesOn<Function>>());; return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>(); || PAC.preservedSet<CFGAnalyses>());; }. says that if the ``PreservedAnalyses`` specifically preserves; ``FooAnalysis``, or if ``PreservedAnalyses`` preserves all analyses (implicit; in ``PAC.preserved()``), or if ``PreservedAnalyses`` preserves all function; analyses, or ``PreservedAnalyses`` preserves all analyses that only care; about the CFG, the ``FooAnalysisResult`` should not be invalidated. If an analysis is stateless and generally shouldn't be invalidated, use the; following:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; // Check whether the analysis has been explicitly invalidated. Otherwise, it's; // stateless and remains preserved.; auto PAC = PA.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:16861,Integrability,depend,dependencies,16861,"ses`` preserves all function; analyses, or ``PreservedAnalyses`` preserves all analyses that only care; about the CFG, the ``FooAnalysisResult`` should not be invalidated. If an analysis is stateless and generally shouldn't be invalidated, use the; following:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; // Check whether the analysis has been explicitly invalidated. Otherwise, it's; // stateless and remains preserved.; auto PAC = PA.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that depe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17751,Integrability,depend,depend,17751,"llAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17853,Integrability,depend,dependent,17853,"llAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:18605,Integrability,wrap,wrap,18605,"ysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19050,Integrability,message,messages,19050,"s of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20837,Integrability,depend,dependent,20837,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:2076,Modifiability,adapt,adaptor,2076,"es(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePass is a loop pass; FPM.addPass(createFunctionToLoopPassAdaptor(LoopRotatePass()));. The IR hierarchy in terms of the new PM is Module -> (CGSCC ->) Function ->; Loop, where going through a CGSCC is optional. .. code-block:: c++. FunctionPassManager FPM;; // loop -> function; FPM.addPass(createFunctionToLoopPassAdaptor(LoopFooPass()));. CGSCCPassManager CGPM;; // loop -> function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> cgscc; CGPM.addPass(createCGSCCToFunctionPassAdaptor(FunctionFooPass()));. ModulePassManager MPM;; // loop -> function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:3865,Modifiability,adapt,adaptors,3865,"function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass())));; // function -> module; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionFooPass()));. // loop -> function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(createFunctionToLoopPassAdaptor(LoopFooPass()))));; // function -> cgscc -> module; MPM.addPass(createModuleToPostOrderCGSCCPassAdaptor(createCGSCCToFunctionPassAdaptor(FunctionFooPass())));. A pass manager of a specific IR unit is also a pass of that kind. For; example, a ``FunctionPassManager`` is a function pass, meaning it can be; added to a ``ModulePassManager``:. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6475,Modifiability,plugin,plugins,6475,"ntrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6588,Modifiability,plugin,plugins,6588,"hat allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6628,Modifiability,plugin,plugin,6628,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6643,Modifiability,plugin,plugin,6643,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6668,Modifiability,plugin,plugin,6668,"er PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provid",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6724,Modifiability,plugin,plugin,6724,"anager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes wh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:18641,Modifiability,adapt,adaptor,18641,"ysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires explicit pass nesting. For example, to run a; function pass, then a module pass, we need to wrap the function pass in a module; adaptor:. .. code-block:: shell. $ opt -passes='function(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:19654,Modifiability,adapt,adaptor,19654,"ction(no-op-function),no-op-module' /tmp/a.ll -S. A more complete example, and ``-debug-pass-manager`` to show the execution; order:. .. code-block:: shell. $ opt -passes='no-op-module,cgscc(no-op-cgscc,function(no-op-function,loop(no-op-loop))),function(no-op-function,loop(no-op-loop))' /tmp/a.ll -S -debug-pass-manager. Improper nesting can lead to error messages such as. .. code-block:: shell. $ opt -passes='no-op-function,no-op-module' /tmp/a.ll -S; opt: unknown function pass 'no-op-module'. The nesting is: module (-> cgscc) -> function -> loop, where the CGSCC nesting is optional. There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contai",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:21344,Modifiability,extend,extend,21344,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:1241,Performance,optimiz,optimization,1241,"he-new-pass-manager/>`_. Just Tell Me How To Run The Default Optimization Pipeline With The New Pass Manager; ===================================================================================. .. code-block:: c++. // Create the analysis managers.; // These must be declared in this order so that they are destroyed in the; // correct order due to inter-analysis-manager references.; LoopAnalysisManager LAM;; FunctionAnalysisManager FAM;; CGSCCAnalysisManager CGAM;; ModuleAnalysisManager MAM;. // Create the new pass manager builder.; // Take a look at the PassBuilder constructor parameters for more; // customization, e.g. specifying a TargetMachine or various debugging; // options.; PassBuilder PB;. // Register all the basic analyses with the managers.; PB.registerModuleAnalyses(MAM);; PB.registerCGSCCAnalyses(CGAM);; PB.registerFunctionAnalyses(FAM);; PB.registerLoopAnalyses(LAM);; PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);. // Create the pass manager.; // This one corresponds to a typical -O2 optimization pipeline.; ModulePassManager MPM = PB.buildPerModuleDefaultPipeline(OptimizationLevel::O2);. // Optimize the IR!; MPM.run(MyModule, MAM);. The C API also supports most of this, see ``llvm-c/Transforms/PassBuilder.h``. Adding Passes to a Pass Manager; ===============================. For how to write a new PM pass, see :doc:`this page <WritingAnLLVMNewPMPass>`. To add a pass to a new PM pass manager, the important thing is to match the; pass type and the pass manager type. For example, a ``FunctionPassManager``; can only contain function passes:. .. code-block:: c++. FunctionPassManager FPM;; // InstSimplifyPass is a function pass; FPM.addPass(InstSimplifyPass());. If you want to add a loop pass that runs on all loops in a function to a; ``FunctionPassManager``, the loop pass must be wrapped in a function pass; adaptor that goes through all the loops in the function and runs the loop; pass on each one. .. code-block:: c++. FunctionPassManager FPM;; // LoopRotatePa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4640,Performance,cache,cache,4640,"ion pass; FPM.addPass(InstSimplifyPass());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-bloc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4782,Performance,optimiz,optimization,4782,"r(std::move(FPM)));. Generally you want to group CGSCC/function/loop passes together in a pass; manager, as opposed to adding adaptors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:4884,Performance,optimiz,optimized,4884,"tors for each pass to the containing upper; level pass manager. For example,. .. code-block:: c++. ModulePassManager MPM;; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass1()));; MPM.addPass(createModuleToFunctionPassAdaptor(FunctionPass2()));; MPM.run();. will run ``FunctionPass1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5272,Performance,optimiz,optimization,5272,"s1`` on each function in a module, then run; ``FunctionPass2`` on each function in the module. In contrast,. .. code-block:: c++. ModulePassManager MPM;. FunctionPassManager FPM;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6567,Performance,load,loading,6567,"hat allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6618,Performance,load,load-pass-plugin,6618,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6655,Performance,load,loads,6655,"er PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provid",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6958,Performance,cache,cache,6958,"entation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7378,Performance,cache,cache,7378,"TargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``Function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7486,Performance,cache,cached,7486,"parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR anal",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8656,Performance,cache,cached,8656,"scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9722,Performance,optimiz,optimization,9722,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9799,Performance,concurren,concurrency,9799,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9871,Performance,concurren,concurrency,9871,"st reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10002,Performance,cache,cached,10002,"cans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10123,Performance,concurren,concurrency,10123,"cans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10875,Performance,concurren,concurrency,10875," over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has beco",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14081,Performance,cache,cached,14081,"ysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14198,Performance,cache,cached,14198,";. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14438,Performance,cache,cached,14438,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14530,Performance,cache,cached,14530,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14618,Performance,cache,cached,14618,"xy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<FooAnalysis>();; // the default would be:; // return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>());; return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>(); || PAC.preservedSet<CFGAnalyses",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20749,Performance,optimiz,optimization,20749,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20921,Performance,optimiz,optimization,20921,"reated. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``TargetMachine::adjustPassManager()`` function that was used to extend a; legacy PM with passes on a per target basis has been removed. It was mainly; used from opt, but since support for using the default pipelines has been; removed in opt the function isn't needed any longer. In the new PM such; adjustments are done by using ``TargetMachine::registerPassBuilderCallbacks()``. Currently there are efforts to make the codegen pipeline work with the new; PM.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5344,Security,inject,inject,5344,"M;; FPM.addPass(FunctionPass1());; FPM.addPass(FunctionPass2());. MPM.addPass(createModuleToFunctionPassAdaptor(std::move(FPM)));. will run ``FunctionPass1`` and ``FunctionPass2`` on the first function in a; module, then run both passes on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipelin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5542,Security,expose,exposes,5542,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:5571,Security,inject,injecting,5571,"asses on the second function in the module, and so on.; This is better for cache locality around LLVM data structures. This similarly; applies for the other IR types, and in some cases can even affect the quality; of optimization. For example, running all loop passes on a loop may cause a; later loop to be able to be optimized more than if each loop pass were run; separately. Inserting Passes into Default Pipelines; =======================================. Rather than manually adding passes to a pass manager, the typical way of; creating a pass manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6179,Security,inject,inject,6179,"ss manager is to use a ``PassBuilder`` and call something like; ``PassBuilder::buildPerModuleDefaultPipeline()`` which creates a typical; pipeline for a given optimization level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6285,Security,sanitiz,sanitizer,6285,"level. Sometimes either frontends or backends will want to inject passes into the; pipeline. For example, frontends may want to add instrumentation, and target; backends may want to add passes that lower custom intrinsics. For these; cases, ``PassBuilder`` exposes callbacks that allow injecting passes into; certain parts of the pipeline. For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8398,Security,access,access,8398,"'s ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8416,Security,access,access,8416,"ger to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:8754,Security,access,access,8754,"scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``FunctionAnalysisManager`` that a function pass; would have access to. To get access to an outer level IR analysis, you can; call. .. code-block:: c++. const auto &MAMProxy =; AM.getResult<ModuleAnalysisManagerCGSCCProxy>(InitialC, CG);; FooAnalysisResult *AR = MAMProxy.getCachedResult<FooAnalysis>(M);. Asking for a cached and immutable outer level IR analysis works via; ``getCachedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10475,Security,access,accessing,10475,"cture for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10566,Security,access,access,10566,"ly degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10597,Security,access,accessing,10597,"ly degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11241,Security,access,access,11241,"hey could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11374,Security,access,accessible,11374,"lidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as wh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14044,Security,access,accessing,14044,"ysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17170,Security,access,accessible,17170,"lock:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; // Check whether the analysis has been explicitly invalidated. Otherwise, it's; // stateless and remains preserved.; auto PAC = PA.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11745,Usability,simpl,simply,11745,"e function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:11758,Usability,clear,clear,11758,"e function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are valid, they are; manually updated in the loop passes to ensure that invalidation is not; necessary. There is a set of common function analyses that loop passes and; analyses have access to which is passed into loop passes as a; ``LoopStandardAnalysisResults`` parameter. Other mutable function analyses are; not accessible from loop passes. As with any caching mechanism, we need some way to tell analysis managers; when results are no longer valid. Much of the analysis manager complexity; comes from trying to invalidate as few analysis results as possible to keep; compile times as low as possible. There are two ways to deal with potentially invalid analysis results. One is; to simply force clear the results. This should generally only be used when; the IR that the result is keyed on becomes invalid. For example, a function; is deleted, or a CGSCC has become invalid due to call graph changes. The typical way to invalidate analysis results is for a pass to declare what; types of analyses it preserves and what types it does not. When transforming; IR, a pass either has the option to update analyses alongside the IR; transformation, or tell the analysis manager that analyses are no longer; valid and should be invalidated. If a pass wants to keep some specific; analysis up to date, such as when updating it would be faster than; invalidating and recalculating it, the analysis itself may have methods to; update it for specific transformations, or there may be helper updaters like; ``DomTreeUpdater`` for a ``DominatorTree``. Otherwise to mark some analysis; as no longer valid, the pass can return a ``PreservedAnalyses`` with the; proper analyses invalidated. .. code-block:: c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14001,Usability,clear,clear,14001,"dominator tree alongside any transformations, but other analysis results may be invalid.; PreservedAnalyses PA;; PA.preserve<DominatorAnalysis>();; return PA;. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14247,Usability,clear,clear,14247," still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14428,Usability,clear,clear,14428,"s's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis can implement; ``invalidate()`` to be more conservative when it comes to invalidation. For; example,. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &) {; auto PAC = PA.getChecker<Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17437,Usability,clear,clear,17437,"A.getChecker<FooAnalysis>();; return !PAC.preservedWhenStateless();; }. If an analysis depends on other analyses, those analyses also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:17495,Usability,simpl,simply,17495,"es also need to be; checked if they are invalidated:. .. code-block:: c++. bool FooAnalysisResult::invalidate(Function &F, const PreservedAnalyses &PA,; FunctionAnalysisManager::Invalidator &Inv) {; auto PAC = PA.getChecker<FooAnalysis>();; if (!PAC.preserved() && !PAC.preservedSet<AllAnalysesOn<Function>>()); return true;. // Check transitive dependencies.; return Inv.invalidate<BarAnalysis>(F, PA) ||; Inv.invalidate<BazAnalysis>(F, PA);; }. Combining invalidation and analysis manager proxies results in some; complexity. For example, when we invalidate all analyses in a module pass,; we have to make sure that we also invalidate function analyses accessible via; any existing inner proxies. The inner proxy's ``invalidate()`` first checks; if the proxy itself should be invalidated. If so, that means the proxy may; contain pointers to IR that is no longer valid, meaning that the inner proxy; needs to completely clear all relevant analysis results. Otherwise the proxy; simply forwards the invalidation to the inner analysis manager. Generally for outer proxies, analysis results from the outer analysis manager; should be immutable, so invalidation shouldn't be a concern. However, it is; possible for some inner analysis to depend on some outer analysis, and when; the outer analysis is invalidated, we need to make sure that dependent inner; analyses are also invalidated. This actually happens with alias analysis; results. Alias analysis is a function-level analysis, but there are; module-level implementations of specific types of alias analysis. Currently; ``GlobalsAA`` is the only module-level alias analysis and it generally is not; invalidated so this is not so much of a concern. See; ``OuterAnalysisManagerProxy::Result::registerOuterAnalysisInvalidation()``; for more details. Invoking ``opt``; ================. .. code-block:: shell. $ opt -passes='pass1,pass2' /tmp/a.ll -S; # -p is an alias for -passes; $ opt -p pass1,pass2 /tmp/a.ll -S. The new PM typically requires exp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:20275,Usability,simpl,simply,20275,"There are a couple of special cases for easier typing:. * If the first pass is not a module pass, a pass manager of the first pass is; implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-function' /tmp/a.ll -S; $ opt -passes='function(no-op-function,no-op-function)' /tmp/a.ll -S. * If there is an adaptor for a pass that lets it fit in the previous pass; manager, that is implicitly created. * For example, the following are equivalent. .. code-block:: shell. $ opt -passes='no-op-function,no-op-loop' /tmp/a.ll -S; $ opt -passes='no-op-function,loop(no-op-loop)' /tmp/a.ll -S. For a list of available passes and analyses, including the IR unit (module,; CGSCC, function, loop) they operate on, run. .. code-block:: shell. $ opt --print-passes. or take a look at ``PassRegistry.def``. To make sure an analysis named ``foo`` is available before a pass, add; ``require<foo>`` to the pass pipeline. This adds a pass that simply requests; that the analysis is run. This pass is also subject to proper nesting. For; example, to make sure some function analysis is already computed for all; functions before a module pass:. .. code-block:: shell. $ opt -passes='function(require<my-function-analysis>),my-module-pass' /tmp/a.ll -S. Status of the New and Legacy Pass Managers; ==========================================. LLVM currently contains two pass managers, the legacy PM and the new PM. The; optimization pipeline (aka the middle-end) uses the new PM, whereas the backend; target-dependent code generation uses the legacy PM. The legacy PM somewhat works with the optimization pipeline, but this is; deprecated and there are ongoing efforts to remove its usage. Some IR passes are considered part of the backend codegen pipeline even if; they are LLVM IR passes (whereas all MIR passes are codegen passes). This; includes anything added via ``TargetPassConfig`` hooks, e.g.; ``TargetPassConfig::addCodeGenPrepare()``. The ``Targ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NewPassManager.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:12139,Availability,down,downloads,12139," <LangRef.html#module-flags-metadata>`; for details.). Executing PTX; =============. The most common way to execute PTX assembly on a GPU device is to use the CUDA; Driver API. This API is a low-level interface to the GPU driver and allows for; JIT compilation of PTX code to native GPU machine code. Initializing the Driver API:. .. code-block:: c++. CUdevice device;; CUcontext context;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a compute device context; cuCtxCreate(&context, 0, device);. JIT compiling a PTX string to a device binary:. .. code-block:: c++. CUmodule module;; CUfunction function;. // JIT compile a null-terminated PTX string; cuModuleLoadData(&module, (void*)PTXString);. // Get a handle to the ""myfunction"" kernel function; cuModuleGetFunction(&function, module, ""myfunction"");. For full examples of executing PTX assembly, please see the `CUDA Samples; <https://developer.nvidia.com/cuda-downloads>`_ distribution. Common Issues; =============. ptxas complains of undefined function: __nvvm_reflect; -----------------------------------------------------. When linking with libdevice, the ``NVVMReflect`` pass must be used. See; :ref:`libdevice` for more information. Tutorial: A Simple Compute Kernel; =================================. To start, let us take a look at a simple compute kernel written directly in; LLVM IR. The kernel implements vector addition, where each thread computes one; element of the output vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19225,Availability,error,error,19225,"data node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8808,Deployability,pipeline,pipeline,8808,"LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended tha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9306,Deployability,pipeline,pipeline,9306,"ompilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; Mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9851,Deployability,pipeline,pipeline,9851,"tely after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:22812,Deployability,install,installed,22812,"ing kernel\n"";. // Kernel launch; checkCudaErrors(cuLaunchKernel(function, gridSizeX, gridSizeY, gridSizeZ,; blockSizeX, blockSizeY, blockSizeZ,; 0, NULL, KernelParams, NULL));. // Retrieve device data; checkCudaErrors(cuMemcpyDtoH(&hostC[0], devBufferC, sizeof(float)*16));. std::cout << ""Results:\n"";; for (unsigned i = 0; i != 16; ++i) {; std::cout << hostA[i] << "" + "" << hostB[i] << "" = "" << hostC[i] << ""\n"";; }. // Clean up after ourselves; delete [] hostA;; delete [] hostB;; delete [] hostC;. // Clean-up; checkCudaErrors(cuMemFree(devBufferA));; checkCudaErrors(cuMemFree(devBufferB));; checkCudaErrors(cuMemFree(devBufferC));; checkCudaErrors(cuModuleUnload(cudaModule));; checkCudaErrors(cuCtxDestroy(context));. return 0;; }. You will need to link with the CUDA driver and specify the path to cuda.h. .. code-block:: text. # clang++ sample.cpp -o sample -O2 -g -I/usr/local/cuda-5.5/include -lcuda. We don't need to specify a path to ``libcuda.so`` since this is installed in a; system location by the driver, not the CUDA toolkit. If everything goes as planned, you should see the following output when; running the compiled program:. .. code-block:: text. Using CUDA Device [0]: GeForce GTX 680; Device Compute Capability: 3.0; Launching kernel; Results:; 0 + 0 = 0; 1 + 2 = 3; 2 + 4 = 6; 3 + 6 = 9; 4 + 8 = 12; 5 + 10 = 15; 6 + 12 = 18; 7 + 14 = 21; 8 + 16 = 24; 9 + 18 = 27; 10 + 20 = 30; 11 + 22 = 33; 12 + 24 = 36; 13 + 26 = 39; 14 + 28 = 42; 15 + 30 = 45. .. note::. You will likely see a different device identifier based on your hardware. Tutorial: Linking with Libdevice; ================================. In this tutorial, we show a simple example of linking LLVM IR with the; libdevice library. We will use the same kernel as the previous tutorial,; except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25421,Deployability,configurat,configuration,25421,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9927,Energy Efficiency,schedul,schedule,9927,"list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================. The value of this flag is determined by the ""nvvm-reflect-ftz"" module flag.; The following sets the ftz flag to 1. .. code-block:: llvm. !llvm.module.flag = !{!0}; !0 = !{i32 4, !""nvvm-reflect-ftz"", i32 1}. (``i32 4`` indic",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:3231,Integrability,interface,interface,3231,"========; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". These are overloaded intrinsics. You can use these on any pointer types. .. code-block:: llvm. declare i8* @llvm.nvvm.ptr.global.to.gen.p0i8.p1i8(i8 addrspace(1)*); declare i8* @llvm.nvvm.ptr.shared.to.gen.p0i8.p3i8(i8 ad",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:3423,Integrability,interface,interface,3423,"s space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". These are overloaded intrinsics. You can use these on any pointer types. .. code-block:: llvm. declare i8* @llvm.nvvm.ptr.global.to.gen.p0i8.p1i8(i8 addrspace(1)*); declare i8* @llvm.nvvm.ptr.shared.to.gen.p0i8.p3i8(i8 addrspace(3)*); declare i8* @llvm.nvvm.ptr.constant.to.gen.p0i8.p4i8(i8 addrspace(4)*); declare i8* @llvm.nvvm.ptr.local.to.gen.p0i8.p5i8(i8 addrspace(5)*). Overview:; """""""""""""""""". The '``llvm.nvvm.ptr.*.to.gen``' intrinsics convert a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8221,Integrability,depend,depends,8221,""""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Elimi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:11348,Integrability,interface,interface,11348,"----------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================. The value of this flag is determined by the ""nvvm-reflect-ftz"" module flag.; The following sets the ftz flag to 1. .. code-block:: llvm. !llvm.module.flag = !{!0}; !0 = !{i32 4, !""nvvm-reflect-ftz"", i32 1}. (``i32 4`` indicates that the value set here overrides the value in another; module we link with. See the `LangRef <LangRef.html#module-flags-metadata>`; for details.). Executing PTX; =============. The most common way to execute PTX assembly on a GPU device is to use the CUDA; Driver API. This API is a low-level interface to the GPU driver and allows for; JIT compilation of PTX code to native GPU machine code. Initializing the Driver API:. .. code-block:: c++. CUdevice device;; CUcontext context;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a compute device context; cuCtxCreate(&context, 0, device);. JIT compiling a PTX string to a device binary:. .. code-block:: c++. CUmodule module;; CUfunction function;. // JIT compile a null-terminated PTX string; cuModuleLoadData(&module, (void*)PTXString);. // Get a handle to the ""myfunction"" kernel function; cuModuleGetFunction(&function, module, ""myfunction"");. For full examples of executing PTX assembly, please see the `CUDA Samples; <https://developer.nvidia.com/cuda-downloads>`_ distribution. Common Issues; =============. ptxas complains of undefined function: __nvvm_reflect; -----------------------------------------------------. When linking with libdevice, the ``NVVMReflect`` pass must be used. See; :ref:`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:2313,Modifiability,variab,variable,2313,"nel function calling a device function in LLVM IR. The; function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is not. .. code-block:: llvm. define float @my_fmad(float %x, float %y, float %z) {; %mul = fmul float %x, %y; %add = fadd float %mul, %z; ret float %add; }. define void @my_kernel(float* %ptr) {; %val = load float, float* %ptr; %ret = call float @my_fmad(float %val, float %val, float %val); store float %ret, float* %ptr; ret void; }. !nvvm.annotations = !{!1}; !1 = !{void (float*)* @my_kernel, !""kernel"", i32 1}. When compiled, the PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:2947,Modifiability,variab,variables,2947,"he PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:3093,Modifiability,variab,variables,3093,"ack-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3 ]. LLVM IR functions can read and write to this array, and host-side code can; copy data to it by name with the CUDA Driver API. Note that since address space 0 is the generic space, it is illegal to have; global variables in address space 0. Address space 0 is the default address; space in LLVM, so the ``addrspace(N)`` annotation is *required* for global; variables. Triples; -------. The NVPTX target uses the module triple to select between 32/64-bit code; generation and the driver-compiler interface to use. The triple architecture; can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit PTX). The; operating system should be one of ``cuda`` or ``nvcl``, which determines the; interface used by the generated code to communicate with the driver. Most; users will want to use ``cuda`` as the operating system, which makes the; generated PTX compatible with the CUDA Driver API. Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``. Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``. .. _nvptx_intrinsics:. NVPTX Intrinsics; ================. Address Space Conversion; ------------------------. '``llvm.nvvm.ptr.*.to.gen``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". These are overloaded intrinsics. You can use these on any pointer types. .. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9528,Modifiability,variab,variables,9528,"al; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; contro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25421,Modifiability,config,configuration,25421,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25809,Modifiability,variab,variables,25809," float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param_1];; mul.wide.s32 %rl4, %r3, 4;; add.s64 %rl5, %rl2, %rl4;; ld.param.u64 %rl6, [kernel_param_2];; add.s64 %rl7, %rl3, %rl4;; add.s64 %rl1, %rl6, %rl4;; ld.global.f32 %f1, [%rl5];; ld.global.f32 %f2, [%rl7];; setp.eq.f32 %p1, %f1, 0f3F800000;; setp.eq.f32 %p2, %f2, 0f00000000;; or.pred %p3, %p1, %p2;; @%p3 bra BB0_1;; bra.uni BB0_2;; BB0_1:; mov.f32 %f110, 0f3F800000;; st.global.f32 [%rl1], %f110;; r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:1690,Performance,load,load,1690,"ons; ===========. Marking Functions as Kernels; ----------------------------. In PTX, there are two types of functions: *device functions*, which are only; callable by device code, and *kernel functions*, which are callable by host; code. By default, the back-end will emit device functions. Metadata is used to; declare a function as a kernel function. This metadata is attached to the; ``nvvm.annotations`` named metadata object, and has the following format:. .. code-block:: text. !0 = !{<function-ref>, metadata !""kernel"", i32 1}. The first parameter is a reference to the kernel function. The following; example shows a kernel function calling a device function in LLVM IR. The; function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is not. .. code-block:: llvm. define float @my_fmad(float %x, float %y, float %z) {; %mul = fmul float %x, %y; %add = fadd float %mul, %z; ret float %add; }. define void @my_kernel(float* %ptr) {; %val = load float, float* %ptr; %ret = call float @my_fmad(float %val, float %val, float %val); store float %ret, float* %ptr; ret void; }. !nvvm.annotations = !{!1}; !1 = !{void (float*)* @my_kernel, !""kernel"", i32 1}. When compiled, the PTX kernel functions are callable by host-side code. .. _address_spaces:. Address Spaces; --------------. The NVPTX back-end uses the following address space mapping:. ============= ======================; Address Space Memory Space; ============= ======================; 0 Generic; 1 Global; 2 Internal Use; 3 Shared; 4 Constant; 5 Local; ============= ======================. Every global variable and pointer type is assigned to one of these address; spaces, with 0 being the default address space. Intrinsics are provided which; can be used to convert pointers between the generic and non-generic address; spaces. As an example, the following IR will define an array ``@g`` that resides in; global device memory. .. code-block:: llvm. @g = internal addrspace(1) global [4 x i32] [ i32 0, i32 1, i32 2, i32 3",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:7740,Performance,perform,performance,7740,"====================================; ``threadId`` ``@llvm.nvvm.read.ptx.sreg.tid.*``; ``blockIdx`` ``@llvm.nvvm.read.ptx.sreg.ctaid.*``; ``blockDim`` ``@llvm.nvvm.read.ptx.sreg.ntid.*``; ``gridDim`` ``@llvm.nvvm.read.ptx.sreg.nctaid.*``; ============ =====================================. Barriers; --------. '``llvm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8794,Performance,optimiz,optimization,8794,"LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended tha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9293,Performance,optimiz,optimization,9293,"ompilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; Mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:9838,Performance,optimiz,optimization,9838,"tely after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:10697,Performance,optimiz,optimized,10697," code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended that ``NVVMReflect`` is executed early in the; optimization pipeline before dead-code elimination. The NVPTX TargetMachine knows how to schedule ``NVVMReflect`` at the beginning; of your pass manager; just use the following code when setting up your pass; manager and the PassBuilder will use ``registerPassBuilderCallbacks`` to let; NVPTXTargetMachine::registerPassBuilderCallbacks add the pass to the; pass manager:. .. code-block:: c++. std::unique_ptr<TargetMachine> TM = ...;; PassBuilder PB(TM);; ModulePassManager MPM;; PB.parsePassPipeline(MPM, ...);. Reflection Parameters; ---------------------. The libdevice library currently uses the following reflection parameters to; control code generation:. ==================== ======================================================; Flag Description; ==================== ======================================================; ``__CUDA_FTZ=[0,1]`` Use optimized code paths that flush subnormals to zero; ==================== ======================================================. The value of this flag is determined by the ""nvvm-reflect-ftz"" module flag.; The following sets the ftz flag to 1. .. code-block:: llvm. !llvm.module.flag = !{!0}; !0 = !{i32 4, !""nvvm-reflect-ftz"", i32 1}. (``i32 4`` indicates that the value set here overrides the value in another; module we link with. See the `LangRef <LangRef.html#module-flags-metadata>`; for details.). Executing PTX; =============. The most common way to execute PTX assembly on a GPU device is to use the CUDA; Driver API. This API is a low-level interface to the GPU driver and allows for; JIT compilation of PTX code to native GPU machine code. Initializing the Driver API:. .. code-block:: c++. CUdevice device;; CUcontext context;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:13626,Performance,load,load,13626," vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = A + B; %valC = fadd float %valA, %valB. ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:. .. code-block:: text. # llc -mcpu=sm_20 kernel.ll -o kernel.ptx. .. note::. If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32``; in the module data layout string and use ``nvptx-nvidia-cuda`` as the; target triple. The output we get from ``llc`` (as of LLVM 3.4):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .f32 %f<4>;; .reg .s32 %r<2>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:13682,Performance,load,load,13682," vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = A + B; %valC = fadd float %valA, %valB. ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:. .. code-block:: text. # llc -mcpu=sm_20 kernel.ll -o kernel.ptx. .. note::. If you want to generate 32-bit code, change ``p:64:64:64`` to ``p:32:32:32``; in the module data layout string and use ``nvptx-nvidia-cuda`` as the; target triple. The output we get from ``llc`` (as of LLVM 3.4):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .f32 %f<4>;; .reg .s32 %r<2>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:17440,Performance,load,load,17440,"read.ptx.sreg.ntid.{x,y,z}`` blockDim.{x,y,z}; ``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}`` gridDim.{x,y,z}; ``void @llvm.nvvm.barrier0()`` __syncthreads(); ================================================ ====================. Address Spaces; ^^^^^^^^^^^^^^. You may have noticed that all of the pointer types in the LLVM IR example had; an explicit address space specifier. What is address space 1? NVIDIA GPU; devices (generally) have four types of memory:. - Global: Large, off-chip memory; - Shared: Small, on-chip memory shared among all threads in a CTA; - Local: Per-thread, private memory; - Constant: Read-only memory shared across all threads. These different types of memory are represented in LLVM IR as address spaces.; There is also a fifth address space used by the NVPTX code generator that; corresponds to the ""generic"" address space. This address space can represent; addresses in any other address space (with a few exceptions). This allows; users to write IR functions that can load/store memory using the same; instructions. Intrinsics are provided to convert pointers between the generic; and non-generic address spaces. See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more information. Kernel Metadata; ^^^^^^^^^^^^^^^. In PTX, a function can be either a `kernel` function (callable from the host; program), or a `device` function (callable only from GPU code). You can think; of `kernel` functions as entry-points in the GPU program. To mark an LLVM IR; function as a `kernel` function, we make use of special LLVM metadata. The; NVPTX back-end will look for a named metadata node called; ``nvvm.annotations``. This named metadata must contain a list of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:18970,Performance,load,loading,18970,"function as a `kernel` function, we make use of special LLVM metadata. The; NVPTX back-end will look for a named metadata node called; ``nvvm.annotations``. This named metadata must contain a list of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initializatio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19111,Performance,load,load,19111,"ist of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19212,Performance,perform,perform,19212,"data node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19420,Performance,load,loaded,19420,"r the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;; checkCudaErrors(cuDeviceComputeCapability(&devMajor, &devMinor, device));; std::cout << ""Device Compute Capability: ""; << devMajor << ""."" << devMinor << ""\n"";; if (devMajor < 2) {; std::cerr << ""ERROR: Device 0 is not SM 2.0 or g",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:24572,Performance,load,load,24572,"xcept that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:24628,Performance,load,load,24628,"xcept that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25006,Performance,perform,perform,25006," declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25236,Performance,perform,performed,25236,"d = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25350,Performance,perform,performed,25350,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:6507,Security,access,access,6507," These intrinsics modify the pointer value to be a valid pointer in the target; non-generic address space. Reading PTX Special Registers; -----------------------------. '``llvm.nvvm.read.ptx.sreg.*``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare i32 @llvm.nvvm.read.ptx.sreg.tid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.tid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.tid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.ntid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.ntid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.ntid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.x(); declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.y(); declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.z(); declare i32 @llvm.nvvm.read.ptx.sreg.warpsize(). Overview:; """""""""""""""""". The '``@llvm.nvvm.read.ptx.sreg.*``' intrinsics provide access to the PTX; special registers, in particular the kernel launch bounds. These registers; map in the following way to CUDA builtins:. ============ =====================================; CUDA Builtin PTX Special Register Intrinsic; ============ =====================================; ``threadId`` ``@llvm.nvvm.read.ptx.sreg.tid.*``; ``blockIdx`` ``@llvm.nvvm.read.ptx.sreg.ctaid.*``; ``blockDim`` ``@llvm.nvvm.read.ptx.sreg.ntid.*``; ``gridDim`` ``@llvm.nvvm.read.ptx.sreg.nctaid.*``; ============ =====================================. Barriers; --------. '``llvm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Link",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19692,Testability,assert,assert,19692,"ns``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;; checkCudaErrors(cuDeviceComputeCapability(&devMajor, &devMinor, device));; std::cout << ""Device Compute Capability: ""; << devMajor << ""."" << devMinor << ""\n"";; if (devMajor < 2) {; std::cerr << ""ERROR: Device 0 is not SM 2.0 or greater\n"";; return 1;; }. std::ifstream t(""kernel.ptx"");; if (!t.is_open()) {; std::cerr << ""kernel.ptx not found\n"";; return 1;; }; std::string str((std::istreambuf_iterator<char>(t)),; std::istreambuf_iterator<char>());. // Create driver context; check",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8080,Usability,guid,guide,8080,"vm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:12522,Usability,simpl,simple,12522,"text;. // Initialize the driver API; cuInit(0);; // Get a handle to the first compute device; cuDeviceGet(&device, 0);; // Create a compute device context; cuCtxCreate(&context, 0, device);. JIT compiling a PTX string to a device binary:. .. code-block:: c++. CUmodule module;; CUfunction function;. // JIT compile a null-terminated PTX string; cuModuleLoadData(&module, (void*)PTXString);. // Get a handle to the ""myfunction"" kernel function; cuModuleGetFunction(&function, module, ""myfunction"");. For full examples of executing PTX assembly, please see the `CUDA Samples; <https://developer.nvidia.com/cuda-downloads>`_ distribution. Common Issues; =============. ptxas complains of undefined function: __nvvm_reflect; -----------------------------------------------------. When linking with libdevice, the ``NVVMReflect`` pass must be used. See; :ref:`libdevice` for more information. Tutorial: A Simple Compute Kernel; =================================. To start, let us take a look at a simple compute kernel written directly in; LLVM IR. The kernel implements vector addition, where each thread computes one; element of the output vector C from the input vectors A and B. To make this; easier, we also assume that only a single CTA (thread block) will be launched,; and that it will be one dimensional. The Kernel; ----------. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19081,Usability,simpl,simple,19081,"ist of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:23493,Usability,simpl,simple,23493,"Module));; checkCudaErrors(cuCtxDestroy(context));. return 0;; }. You will need to link with the CUDA driver and specify the path to cuda.h. .. code-block:: text. # clang++ sample.cpp -o sample -O2 -g -I/usr/local/cuda-5.5/include -lcuda. We don't need to specify a path to ``libcuda.so`` since this is installed in a; system location by the driver, not the CUDA toolkit. If everything goes as planned, you should see the following output when; running the compiled program:. .. code-block:: text. Using CUDA Device [0]: GeForce GTX 680; Device Compute Capability: 3.0; Launching kernel; Results:; 0 + 0 = 0; 1 + 2 = 3; 2 + 4 = 6; 3 + 6 = 9; 4 + 8 = 12; 5 + 10 = 15; 6 + 12 = 18; 7 + 14 = 21; 8 + 16 = 24; 9 + 18 = 27; 10 + 20 = 30; 11 + 22 = 33; 12 + 24 = 36; 13 + 26 = 39; 14 + 28 = 42; 15 + 30 = 45. .. note::. You will likely see a different device identifier based on your hardware. Tutorial: Linking with Libdevice; ================================. In this tutorial, we show a simple example of linking LLVM IR with the; libdevice library. We will use the same kernel as the previous tutorial,; except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``.; Libdevice provides an ``__nv_powf`` function that we will use. .. code-block:: llvm. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64""; target triple = ""nvptx64-nvidia-cuda"". ; Intrinsic to read X component of thread ID; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind; ; libdevice function; declare float @__nv_powf(float, float). define void @kernel(float addrspace(1)* %A,; float addrspace(1)* %B,; float addrspace(1)* %C) {; entry:; ; What is my ID?; %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:3675,Availability,redundant,redundant,3675,"gh LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4132,Availability,down,down,4132,"verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11858,Deployability,pipeline,pipeline,11858,"ossible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11900,Deployability,release,release,11900,"le to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``LLVMBuildLoad``) are no; longer supported. The following typed pointer functionality is still to be removed:. * Vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4380,Energy Efficiency,reduce,reduces,4380," pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9954,Integrability,interface,interface,9954," %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:10943,Integrability,interface,interface,10943,"nd will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in pro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11176,Integrability,interface,interface,11176,"ldInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11313,Integrability,interface,interface,11313," LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2934,Modifiability,evolve,evolved,2934,"s some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11372,Modifiability,plugin,plugin-opt,11372,"LVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are used (if you want to; override the default) using ``LLVMContext::setOpaquePointers``. Temporarily disabling opaque pointers; =====================================. In LLVM 15, opaque pointers are enabled by default, but it it still possible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:12731,Modifiability,plugin,plugin-opt,12731,"NG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``LLVMBuildLoad``) are no; longer supported. The following typed pointer functionality is still to be removed:. * Various APIs that are no longer relevant with opaque pointers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:707,Performance,load,load,707,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:744,Performance,load,load,744,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:891,Performance,load,load,891,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:960,Performance,load,load,960,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:1005,Performance,load,load,1005,"==; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2458,Performance,optimiz,optimization,2458,"ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2606,Performance,optimiz,optimization,2606,"il/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; ope",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2627,Performance,optimiz,optimization,2627,"rth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:2915,Performance,optimiz,optimizations,2915,"s some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4280,Performance,optimiz,optimizations,4280,"complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all poin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6636,Performance,optimiz,optimizations,6636,"i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6686,Performance,load,load,6686,"i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6826,Performance,load,loads,6826,"y converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:8776,Performance,load,load,8776,"aquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a byval argument, use ``getParamByValType()``. Similar; method exists for other ABI-affecting attributes that need to know the; element type, such as byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. F",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9008,Performance,load,load,9008,"s byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9097,Performance,load,load,9097," can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9195,Performance,load,load,9195,"ot naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9507,Performance,load,load,9507,"`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:11845,Performance,optimiz,optimization,11845,"ossible to; use typed pointers using a number of opt-in flags. For users of the clang driver interface, it is possible to temporarily restore; the old default using the ``-DCLANG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:1939,Safety,safe,safe,1939,"s of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks to make sure that the Clang frontend matched; its frontend values/operations with the corresponding LLVM IR. However, as other; languages like C++ adopted LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimization",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:3189,Safety,detect,detect,3189," LLVM, the community realized that pointee types were; more of a hindrance for LLVM development and that the extra type checking with; some frontends wasn't worth it. LLVM's type system was `originally designed; <https://llvm.org/pubs/2003-05-01-GCCSummit2003.html>`_ to support high-level; optimization. However, years of LLVM implementation experience have demonstrated; that the pointee type system design does not effectively support; optimization. Memory optimization algorithms, such as SROA, GVN, and AA,; generally need to look through LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:3675,Safety,redund,redundant,3675,"gh LLVM's struct types and reason about the; underlying memory offsets. The community realized that pointee types hinder LLVM; development, rather than helping it. Some of the initially proposed high-level; optimizations have evolved into `TBAA; <https://llvm.org/docs/LangRef.html#tbaa-metadata>`_ due to limitations with; representing higher-level language information directly via SSA values. Pointee types provide some value to frontends because the IR verifier uses types; to detect straightforward type confusion bugs. However, frontends also have to; deal with the complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6782,Safety,avoid,avoid,6782,"terType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6607,Security,access,access,6607,"i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6809,Security,access,accesses,6809,"terType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9246,Security,access,accessed,9246,"ot naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9610,Security,access,accessed,9610,"ent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be possible to call ``LLVMGetElementType()``; on a pointer type. It is possible to control whether opaque pointers are u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:5907,Testability,test,testing,5907,"nguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * U",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:5984,Testability,test,test,5984,"nsition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:6151,Testability,test,test,6151,"=========. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:7187,Testability,assert,assertions,7187," ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * Use ``getLoadStoreType()`` to handle both of the above in one call.; * For getelementptr instructions, use ``getSourceElementType()``.; * For calls, use ``getFunctionType()``.; * For allocas, use ``getAllocatedType()``.; * For globals, use ``getValueType()``.; * For consistency assertions, use; ``PointerType::isOpaqueOrPointeeTypeEquals()``.; * To create a pointer type in a different address space, use; ``PointerType::getWithSamePointeeType()``.; * To check that two pointers have the same element type, use; ``PointerType::hasSameElementTypeAs()``.; * While it is preferred to write code in a way that accepts both typed and; opaque pointers, ``Type::isOpaquePointerTy()`` and; ``PointerType::isOpaque()`` can be used to handle opaque pointers specially.; ``PointerType::getNonOpaquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a byval argument, use ``getParamByValType()``. Similar; method exists for other ABI-affecting attributes that need to know the; element type, such as byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not natura",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:8933,Testability,test,test,8933,"s byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9055,Testability,test,test,9055," can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:12248,Testability,test,tested,12248,"NG_ENABLE_OPAQUE_POINTERS=OFF`` cmake option,; or by passing ``-Xclang -no-opaque-pointers`` to a single clang invocation. For users of the clang cc1 interface, ``-no-opaque-pointers`` can be passed.; Note that the ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake option has no effect on; the cc1 interface. Usage for LTO can be disabled by passing ``-Wl,-plugin-opt=no-opaque-pointers``; to the clang driver. For users of LLVM as a library, opaque pointers can be disabled by calling; ``setOpaquePointers(false)`` on the ``LLVMContext``. For users of LLVM tools like opt, opaque pointers can be disabled by passing; ``-opaque-pointers=0``. Version Support; ===============. **LLVM 14:** Supports all necessary APIs for migrating to opaque pointers and deprecates/removes incompatible APIs. However, using opaque pointers in the optimization pipeline is **not** fully supported. This release can be used to make out-of-tree code compatible with opaque pointers, but opaque pointers should **not** be enabled in production. **LLVM 15:** Opaque pointers are enabled by default. Typed pointers are still; supported. **LLVM 16:** Opaque pointers are enabled by default. Typed pointers are; supported on a best-effort basis only and not tested. **LLVM 17:** Only opaque pointers are supported. Typed pointers are not; supported. Transition State; ================. As of July 2023:. Typed pointers are **not** supported on the ``main`` branch. The following typed pointer functionality has been removed:. * The ``CLANG_ENABLE_OPAQUE_POINTERS`` cmake flag is no longer supported.; * The ``-no-opaque-pointers`` cc1 clang flag is no longer supported.; * The ``-opaque-pointers`` opt flag is no longer supported.; * The ``-plugin-opt=no-opaque-pointers`` LTO flag is no longer supported.; * C APIs that do not support opaque pointers (like ``LLVMBuildLoad``) are no; longer supported. The following typed pointer functionality is still to be removed:. * Various APIs that are no longer relevant with opaque pointers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:5875,Usability,simpl,simplifies,5875,"nguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all pointer types have a pointee type and opaque pointers cannot be used.; In opaque pointers mode (the default), all pointers are opaque. The opaque; pointer mode can be disabled using ``-opaque-pointers=0`` in; LLVM tools like ``opt``, or ``-Xclang -no-opaque-pointers`` in clang.; Additionally, opaque pointer mode is automatically disabled for IR and bitcode; files that explicitly mention ``i8*`` style typed pointers. In opaque pointer mode, all typed pointers used in IR, bitcode, or created; using ``PointerType::get()`` and similar APIs are automatically converted into; opaque pointers. This simplifies migration and allows testing existing IR with; opaque pointers. .. code-block:: llvm. define i8* @test(i8* %p) {; %p2 = getelementptr i8, i8* %p, i64 1; ret i8* %p2; }. ; Is automatically converted into the following if -opaque-pointers; ; is enabled:. define ptr @test(ptr %p) {; %p2 = getelementptr i8, ptr %p, i64 1; ret ptr %p2; }. Migration Instructions; ======================. In order to support opaque pointers, two types of changes tend to be necessary.; The first is the removal of all calls to ``PointerType::getElementType()`` and; ``Type::getPointerElementType()``. In the LLVM middle-end and backend, this is usually accomplished by inspecting; the type of relevant operations instead. For example, memory access related; analyses and optimizations should use the types encoded in the load and store; instructions instead of querying the pointer type. Here are some common ways to avoid pointer element type accesses:. * For loads, use ``getType()``.; * For stores, use ``getValueOperand()->getType()``.; * U",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:100,Availability,error,errors,100,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:452,Availability,down,down,452,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1519,Availability,failure,failure,1519,"rect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5782,Availability,redundant,redundant,5782,"nning pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMCo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1919,Integrability,message,message,1919,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2312,Integrability,wrap,wrapper,2312,"t.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:6903,Integrability,depend,depends,6903,"used exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to ch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2728,Modifiability,plug-in,plug-in,2728,"t command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be asso",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2978,Modifiability,plugin,plugin-opt,2978,"t could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3214,Modifiability,plugin,plugin-opt,3214,"alue will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and inc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3589,Modifiability,variab,variable,3589,". clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:87,Performance,optimiz,optimization,87,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:284,Performance,optimiz,optimization,284,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:506,Performance,optimiz,optimization,506,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:742,Performance,perform,performing,742,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:753,Performance,optimiz,optimizations,753,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:832,Performance,perform,perform,832,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1155,Performance,optimiz,optimization,1155," ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1887,Performance,perform,perform,1887,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1899,Performance,optimiz,optimizations,1899,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:1963,Performance,optimiz,optimization,1963,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2055,Performance,optimiz,optimization,2055,"tLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gol",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2078,Performance,optimiz,optimizations,2078,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2120,Performance,optimiz,optimization,2120,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2139,Performance,perform,performed,2139,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2179,Performance,optimiz,optimizations,2179,"ax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed v",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2668,Performance,optimiz,optimizations,2668,"t command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be asso",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3534,Performance,optimiz,optimizations,3534,". clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3620,Performance,optimiz,optimization,3620,"llowing syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3747,Performance,perform,performed,3747,"llowing syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3763,Performance,optimiz,optimization,3763,"llowing syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4291,Performance,optimiz,optimizations,4291,"-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4359,Performance,optimiz,optimizations,4359,"-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4924,Performance,perform,performs,4924,"on of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:8752,Performance,perform,performed,8752,"-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable because the behavior should be the same in either; case. The majority of LLVM passes which can be skipped have already been instrumented; in the manner described above. If you are adding a new pass or believe you; have found a pass which is not being included in the opt-bisect process but; should be, you can add it as described above. Adding Finer Granularity; ========================. Once the pass in which an incorrect transformation is performed has been; determined, it may be useful to perform further analysis in order to determine; which specific transformation is causing the problem. Debug counters; can be used for this purpose.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:8804,Performance,perform,perform,8804,"-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable because the behavior should be the same in either; case. The majority of LLVM passes which can be skipped have already been instrumented; in the manner described above. If you are adding a new pass or believe you; have found a pass which is not being included in the opt-bisect process but; should be, you can add it as described above. Adding Finer Granularity; ========================. Once the pass in which an incorrect transformation is performed has been; determined, it may be useful to perform further analysis in order to determine; which specific transformation is causing the problem. Debug counters; can be used for this purpose.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:627,Safety,safe,safely,627,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5782,Safety,redund,redundant,5782,"nning pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMCo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:7246,Security,access,accessed,7246," Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (g); BISECT: NOT running pass (22) Speculatively execute instructions if target has divergent branches on function (g); ... etc. ... Pass Skipping Implementation; ============================. The -opt-bisect-limit implementation depends on individual passes opting in to; the opt-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5158,Testability,test,test-opt,5158,"table and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running p",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5191,Testability,test,test,5191," one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5424,Testability,test,test,5424," a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5526,Testability,test,test,5526,"cation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5598,Testability,test,test,5598," skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g)",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:5739,Testability,test,test,5739,FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE on function (g); BISECT: running pass (4) Infer set function attributes on module (test.ll); BISECT: running pass (5) Interprocedural Sparse Conditional Constant Propagation on module (test.ll); BISECT: running pass (6) Global Variable Optimizer on module (test.ll); BISECT: running pass (7) Promote Memory to Register on function (g); BISECT: running pass (8) Dead Argument Elimination on module (test.ll); BISECT: running pass (9) Combine redundant instructions on function (g); BISECT: running pass (10) Simplify the CFG on function (g); BISECT: running pass (11) Remove unused exception handling info on SCC (<<null function>>); BISECT: running pass (12) Function Integration/Inlining on SCC (<<null function>>); BISECT: running pass (13) Deduce function attributes on SCC (<<null function>>); BISECT: running pass (14) Remove unused exception handling info on SCC (f); BISECT: running pass (15) Function Integration/Inlining on SCC (f); BISECT: running pass (16) Deduce function attributes on SCC (f); BISECT: NOT running pass (17) Remove unused exception handling info on SCC (g); BISECT: NOT running pass (18) Function Integration/Inlining on SCC (g); BISECT: NOT running pass (19) Deduce function attributes on SCC (g); BISECT: NOT running pass (20) SROA on function (g); BISECT: NOT running pass (21) Early CSE on function (,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/OptBisect.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:335,Availability,avail,available,335,"===============================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; se",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4932,Availability,error,error,4932,"of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setN",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5619,Availability,error,error,5619,"sing their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) that uses constructs like symbol linkage and visibility, and weak [3]_; and common symbol definitions. To see how this works, imagine a program ``foo`` which links against a pair; of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5672,Availability,failure,failures,5672,"sing their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) that uses constructs like symbol linkage and visibility, and weak [3]_; and common symbol definitions. To see how this works, imagine a program ``foo`` which links against a pair; of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:6957,Availability,error,error,6957,"tLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) that uses constructs like symbol linkage and visibility, and weak [3]_; and common symbol definitions. To see how this works, imagine a program ``foo`` which links against a pair; of dynamic libraries: ``libA`` and ``libB``. On the command line, building this; program might look like:. .. code-block:: bash. $ clang++ -shared -o libA.dylib a1.cpp a2.cpp; $ clang++ -shared -o libB.dylib b1.cpp b2.cpp; $ clang++ -o myapp myapp.cpp -L. -lA -lB; $ ./myapp. In ORC, this would translate into API calls on a hypothetical CXXCompilingLayer; (with error checking omitted for brevity) as:. .. code-block:: c++. ExecutionSession ES;; RTDyldObjectLinkingLayer ObjLinkingLayer(; ES, []() { return std::make_unique<SectionMemoryManager>(); });; CXXCompileLayer CXXLayer(ES, ObjLinkingLayer);. // Create JITDylib ""A"" and add code to it using the CXX layer.; auto &LibA = ES.createJITDylib(""A"");; CXXLayer.add(LibA, MemoryBuffer::getFile(""a1.cpp""));; CXXLayer.add(LibA, MemoryBuffer::getFile(""a2.cpp""));. // Create JITDylib ""B"" and add code to it using the CXX layer.; auto &LibB = ES.createJITDylib(""B"");; CXXLayer.add(LibB, MemoryBuffer::getFile(""b1.cpp""));; CXXLayer.add(LibB, MemoryBuffer::getFile(""b2.cpp""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:8517,Availability,error,error,8517,"bB = ES.createJITDylib(""B"");; CXXLayer.add(LibB, MemoryBuffer::getFile(""b1.cpp""));; CXXLayer.add(LibB, MemoryBuffer::getFile(""b2.cpp""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look up the JIT'd main, cast it to a function pointer, then call it.; auto MainSym = ExitOnErr(ES.lookup({&MainJD}, ""main""));; auto *Main = MainSym.getAddress().toPtr<int(*)(int, char *[])>();. int Result = Main(...);. This example tells us nothing about *how* or *when* compilation will happen.; That will depend on the implementation of the hypothetical CXXCompilingLayer.; The same linker-based symbol resolution rules will apply regardless of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:8595,Availability,error,error,8595,"p""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look up the JIT'd main, cast it to a function pointer, then call it.; auto MainSym = ExitOnErr(ES.lookup({&MainJD}, ""main""));; auto *Main = MainSym.getAddress().toPtr<int(*)(int, char *[])>();. int Result = Main(...);. This example tells us nothing about *how* or *when* compilation will happen.; That will depend on the implementation of the hypothetical CXXCompilingLayer.; The same linker-based symbol resolution rules will apply regardless of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a quer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10884,Availability,error,error,10884,"es a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11451,Availability,error,error,11451," notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12872,Availability,failure,failure,12872,"nt APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20914,Availability,error,error,20914,"n ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3111,Deployability,release,release,3111,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5393,Deployability,configurat,configuration,5393,"ule will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17205,Deployability,update,update,17205,". code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally wa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24436,Deployability,release,released,24436,"D = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materiali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:36046,Deployability,release,released,36046,"ProcessControl: Improvements to in-tree support for out-of-process; execution**. The ``TargetProcessControl`` API provides various operations on the JIT; target process (the one which will execute the JIT'd code), including; memory allocation, memory writes, function execution, and process queries; (e.g. for the target triple). By targeting this API new components can be; developed which will work equally well for in-process and out-of-process; JITing. 2. **ORC RPC based TargetProcessControl implementation**. An ORC RPC based implementation of the ``TargetProcessControl`` API is; currently under development to enable easy out-of-process JITing via; file descriptors / sockets. 3. **Core State Machine Cleanup**. The core ORC state machine is currently implemented between JITDylib and; ExecutionSession. Methods are slowly being moved to `ExecutionSession`. This; will tidy up the code base, and also allow us to support asynchronous removal; of JITDylibs (in practice deleting an associated state object in; ExecutionSession and leaving the JITDylib instance in a defunct state until; all references to it have been released). Near Future Work; ----------------. 1. **ORC JIT Runtime Libraries**. We need a runtime library for JIT'd code. This would include things like; TLS registration, reentry functions, registration code for language runtimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21069,Energy Efficiency,reduce,reduce,21069,"ram representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol string",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22099,Energy Efficiency,reduce,reduce,22099,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22162,Energy Efficiency,efficient,efficient,22162,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2633,Integrability,wrap,wrapper,2633,"ne running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3064,Integrability,depend,dependency,3064,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3160,Integrability,depend,dependencies,3160,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:8234,Integrability,depend,depend,8234,"auto &LibA = ES.createJITDylib(""A"");; CXXLayer.add(LibA, MemoryBuffer::getFile(""a1.cpp""));; CXXLayer.add(LibA, MemoryBuffer::getFile(""a2.cpp""));. // Create JITDylib ""B"" and add code to it using the CXX layer.; auto &LibB = ES.createJITDylib(""B"");; CXXLayer.add(LibB, MemoryBuffer::getFile(""b1.cpp""));; CXXLayer.add(LibB, MemoryBuffer::getFile(""b2.cpp""));. // Create and specify the search order for the main JITDylib. This is; // equivalent to a ""links against"" relationship in a command-line link.; auto &MainJD = ES.createJITDylib(""main"");; MainJD.addToLinkOrder(&LibA);; MainJD.addToLinkOrder(&LibB);; CXXLayer.add(MainJD, MemoryBuffer::getFile(""main.cpp""));. // Look up the JIT'd main, cast it to a function pointer, then call it.; auto MainSym = ExitOnErr(ES.lookup({&MainJD}, ""main""));; auto *Main = MainSym.getAddress().toPtr<int(*)(int, char *[])>();. int Result = Main(...);. This example tells us nothing about *how* or *when* compilation will happen.; That will depend on the implementation of the hypothetical CXXCompilingLayer.; The same linker-based symbol resolution rules will apply regardless of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two oth",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9453,Integrability,synchroniz,synchronization,9453,"less of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9923,Integrability,wrap,wrappers,9923,"a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9966,Integrability,wrap,wrap,9966,"a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11601,Integrability,wrap,wrappers,11601," once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11962,Integrability,wrap,wraps,11962,"avior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12517,Integrability,synchroniz,synchronization,12517," in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:16485,Integrability,interface,interface,16485,"tions in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from mul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19302,Integrability,interface,interfaces,19302,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20931,Integrability,synchroniz,synchronization,20931,"n ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21287,Integrability,wrap,wrapper,21287,"iled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:25633,Integrability,wrap,wrapped,25633,"OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materialize`` and ``discard``. The; ``materialize`` function will be called for you when any symbol provided by the unit is looked up,; and it should just call the ``emit`` function on your layer, passing in the given; ``MaterializationResponsibility`` and the wrapped program representation. The ``discard`` function; will be called if some weak symbol provided by your unit is not needed (because the JIT found an; overriding definition). You can use this to drop your definition early, or just ignore it and let; the linker drops the definition later. Here is an example of an ASTLayer:. .. code-block:: c++. // ... In you JIT class; AstLayer astLayer;; // ... class AstMaterializationUnit : public orc::MaterializationUnit {; public:; AstMaterializationUnit(AstLayer &l, Ast &ast); : llvm::orc::MaterializationUnit(l.getInterface(ast)), astLayer(l),; ast(ast) {};. llvm::StringRef getName() const override {; return ""AstMaterializationUnit"";; }. void materialize(std::unique_ptr<orc::MaterializationResponsibility> r) override {; astLayer.emit(std::move(r), ast);; };. private:; void discard(const llvm::orc::JITDylib &jd, const llvm::orc::SymbolStringPtr &",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:27889,Integrability,wrap,wrappers,27889,"ResourceTrackerSP &rt, Ast &ast) {; return rt->getJITDylib().define(std::make_unique<AstMaterializationUnit>(*this, ast), rt);; }. void emit(std::unique_ptr<orc::MaterializationResponsibility> mr, Ast &ast) {; // compileAst is just function that compiles the given AST and returns; // a `llvm::orc::ThreadSafeModule`; baseLayer.emit(std::move(mr), compileAst(ast));; }. llvm::orc::MaterializationUnit::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33115,Integrability,interface,interface,33115,"ion:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. JD.define(; absoluteSymbols({; { Mangle(""puts""), ExecutorAddr::fromPtr(&puts)},; { Mangle(""gets""), ExecutorAddr::fromPtr(&getS)}; }));. Using absoluteSymbols is reasonable if the set of symbols to be reflected is; small and fixed. On the other hand, if the set of symbols is large or variable; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(D",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4236,Modifiability,extend,extends,4236,"ogram representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:5393,Modifiability,config,configuration,5393,"ule will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit or throw an exception into JIT'd code.; throw JITFailed();; }. auto JIT = LLLazyJITBuilder(); .setNumCompileThreads(4); .setLazyCompileFailureAddr(; ExecutorAddr::fromPtr(&handleLazyCompileFailure)); .create();. // ... For users wanting to get started with LLJIT a minimal example program can be; found at ``llvm/examples/HowToUseLLJIT``. Design Overview; ===============. ORC's JIT program model aims to emulate the linking and symbol resolution; rules used by the static and dynamic linkers. This allows ORC to JIT; arbitrary LLVM IR, including IR produced by an ordinary static compiler (e.g.; clang) t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12340,Modifiability,layers,layers,12340," in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17397,Modifiability,inherit,inherit,17397,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19452,Modifiability,layers,layers,19452,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19530,Modifiability,layers,layers,19530,"calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19785,Modifiability,layers,layers,19785,"e(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20135,Modifiability,layers,layers,20135," from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve loo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20199,Modifiability,layers,layers,20199," 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:20272,Modifiability,layers,layers,20272," The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21183,Modifiability,layers,layers,21183,"`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21483,Modifiability,layers,layers,21483," See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to app",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:32539,Modifiability,variab,variable,32539," Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. JD.define(; absoluteSymbols({; { Mangle(""puts""), ExecutorAddr::fromPtr(&puts)},; { Mangle(""gets""), ExecutorAddr::fromPtr(&getS)}; }));. Using absoluteSymbols is reasonable if the set of symbols to be reflected is; small and fixed. On the other hand, if the set of symbols is large or variable; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:999,Performance,perform,performance,999,"===========================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; sessio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:1068,Performance,optimiz,optimizations,1068,"=========================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default make",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2913,Performance,concurren,concurrently,2913,"**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of reloc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:2984,Performance,concurren,concurrently,2984,"ssion object (``ExecutionSession``). Compiling eagerly by default makes it; easy to use ORC as an in-memory compiler for an existing JIT (similar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a sy",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3516,Performance,concurren,concurrent,3516," compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLL",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3957,Performance,perform,performed,3957,"hed my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.take",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4717,Performance,load,loaded,4717,"ents to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure()",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:9483,Performance,concurren,concurrent,9483,"less of that; implementation, however. For example, if a1.cpp and a2.cpp both define a; function ""foo"" then ORCv2 will generate a duplicate definition error. On the; other hand, if a1.cpp and b1.cpp both define ""foo"" there is no error (different; dynamic libraries may define the same symbol). If main.cpp refers to ""foo"", it; should bind to the definition in LibA rather than the one in LibB, since; main.cpp is part of the ""main"" dylib, and the main dylib links against LibA; before LibB. Many JIT clients will have no need for this strict adherence to the usual; ahead-of-time linking rules, and should be able to get by just fine by putting; all of their code in a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11183,Performance,queue,queue,11183,"e; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:13660,Performance,cache,cached,13660,"sociated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. ..",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17414,Performance,concurren,concurrency,17414,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17497,Performance,concurren,concurrently,17497,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17754,Performance,concurren,concurrent,17754,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17779,Performance,concurren,concurrent,17779,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19240,Performance,concurren,concurrent,19240,"e reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19367,Performance,concurren,concurrency,19367,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19441,Performance,concurren,concurrent,19441,"le:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine h",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21112,Performance,perform,performance,21112,"ram representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`` determines *where* the definitions; reside, the layers determine *how* the definitions will be compiled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol string",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21369,Performance,concurren,concurrently,21369,"iled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22086,Performance,perform,performance,22086,"ookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage rel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22394,Performance,perform,perform,22394,"lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``Ex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:22831,Performance,perform,perform,22831,"urceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ES;; /// ...; auto MainSymbolName = ES.intern(""main"");. If you wish to perform lookup using the C/IR name of a symbol you will also; need to apply the platform linker-mangling before interning the string. On; Linux this mangling is a no-op, but on other platforms it usually involves; adding a prefix to the string (e.g. '_' on Darwin). The mangling scheme is; based on the DataLayout for the target. Given a DataLayout and an; ExecutionSession, you can create a MangleAndInterner function object that; will perform both jobs for you:. .. code-block:: c++. ExecutionSession ES;; const DataLayout &DL = ...;; MangleAndInterner Mangle(ES, DL);. // ... // Portable IR-symbol-name lookup:; auto Sym = ES.lookup({&MainJD}, Mangle(""main""));. How to create JITDylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = .",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24347,Performance,perform,performed,24347,"D = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materiali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:28293,Performance,concurren,concurrent,28293,"::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29775,Performance,load,loadModuleOnContext,29775,"::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30514,Performance,concurren,concurrent,30514,"hed to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAnd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30818,Performance,concurren,concurrency,30818,"feModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30847,Performance,load,loading,30847,"feModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:31178,Performance,load,loading,31178," result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = g",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33544,Performance,load,loadModule,33544,"e; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symboli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:34383,Performance,load,loadModule,34383,"; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symbolic resolution; using the JIT symbol tables should be preferred: it keeps the IR and objects; readable and reusable in subsequent JIT sessions. Hardcoded addresses are; difficult to read, and usually only good for one session. Roadmap; =======. ORC is still undergoing active development. Some current and future works are; listed below. Current Work; ------------. 1. **TargetProcessControl: Improvements to in-tree support for out-of-process; execution**. The ``TargetProcessControl`` API provides various operations on the JIT; target process (the one which will execute the JIT'd code), including; memory allocation, memory writes, function execution, and process queries; (e.g. for the target triple). By targeting this API new components can be; developed which will work equally well for in-process and out-of-process; JITing. 2. **OR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37043,Performance,concurren,concurrent,37043," need a runtime library for JIT'd code. This would include things like; TLS registration, reentry functions, registration code for language runtimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37270,Performance,latency,latency,37270,"untimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37294,Performance,throughput,throughput,37294,"untimes; (e.g. Objective C and Swift) and other JIT specific runtime code. This should; be built in a similar manner to compiler-rt (possibly even as part of it). 2. **Remote jit_dlopen / jit_dlclose**. To more fully mimic the environment that static programs operate in we would; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:3208,Safety,safe,safe,3208,"milar to how; MCJIT is commonly used). However ORC also provides built-in support for lazy; compilation via lazy-reexports (see :ref:`Laziness`). **Support for Custom Compilers and Program Representations**; Clients can supply custom compilers for each symbol that they define in their; JIT session. ORC will run the user-supplied compiler when the a definition of; a symbol is needed. ORC is actually fully language agnostic: LLVM IR is not; treated specially, and is supported via the same wrapper mechanism (the; ``MaterializationUnit`` class) that is used for custom compilers. **Concurrent JIT'd code** and **Concurrent Compilation**; JIT'd code may be executed in multiple threads, may spawn new threads, and may; re-enter the ORC (e.g. to request lazy compilation) concurrently from multiple; threads. Compilers launched my ORC can run concurrently (provided the client; sets up an appropriate dispatcher). Built-in dependency tracking ensures that; ORC does not release pointers to JIT'd code or data until all dependencies; have also been JIT'd and they are safe to call or use. **Removable Code**; Resources for JIT'd program representations. **Orthogonality** and **Composability**; Each of the features above can be used independently. It is possible to put; ORC components together to make a non-lazy, in-process, single threaded JIT; or a lazy, out-of-process, concurrent JIT, or anything in between. LLJIT and LLLazyJIT; ===================. ORC provides two basic JIT classes off-the-shelf. These are useful both as; examples of how to assemble ORC components to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4788,Safety,detect,detect,4788,"uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure() {; // JIT'd code will jump here if lazy compilation fails, giving us an; // opportunity to exit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10359,Safety,safe,safe,10359,"rs compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. -",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:12335,Safety,safe,safe,12335," in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17702,Safety,safe,safely,17702,"symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10375,Security,access,access,10375,"rs compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. -",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14249,Security,access,access,14249,"foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15396,Security,access,access,15396,"library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:21360,Security,access,accessed,21360,"iled.; Linkage relationships between ``JITDylibs`` determine how inter-module; references are resolved, and symbol resolvers are no longer used. See the; section `Design Overview`_ for more details. Unless multiple JITDylibs are needed to model linkage relationships, ORCv1; clients should place all code in a single JITDylib.; MCJIT clients should use LLJIT (see `LLJIT and LLLazyJIT`_), and can place; code in LLJIT's default created main JITDylib (See; ``LLJIT::getMainJITDylib()``). 2. All JIT stacks now need an ``ExecutionSession`` instance. ExecutionSession; manages the string pool, error reporting, synchronization, and symbol; lookup. 3. ORCv2 uses uniqued strings (``SymbolStringPtr`` instances) rather than; string values in order to reduce memory overhead and improve lookup; performance. See the subsection `How to manage symbol strings`_. 4. IR layers require ThreadSafeModule instances, rather than; std::unique_ptr<Module>s. ThreadSafeModule is a wrapper that ensures that; Modules that use the same LLVMContext are not accessed concurrently.; See `How to use ThreadSafeModule and ThreadSafeContext`_. 5. Symbol lookup is no longer handled by layers. Instead, there is a; ``lookup`` method on JITDylib that takes a list of JITDylibs to scan. .. code-block:: c++. ExecutionSession ES;; JITDylib &JD1 = ...;; JITDylib &JD2 = ...;. auto Sym = ES.lookup({&JD1, &JD2}, ES.intern(""_main""));. 6. The removeModule/removeObject methods are replaced by; ``ResourceTracker::remove``.; See the subsection `How to remove code`_. For code examples and suggestions of how to use the ORCv2 APIs, please see; the section `How-tos`_. How-tos; =======. How to manage symbol strings; ----------------------------. Symbol strings in ORC are uniqued to improve lookup performance, reduce memory; overhead, and allow symbol names to function as efficient keys. To get the; unique ``SymbolStringPtr`` for a string value, call the; ``ExecutionSession::intern`` method:. .. code-block:: c++. ExecutionSession ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:28304,Security,access,access,28304,"::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29195,Security,access,accessible,29195,"g scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29499,Security,access,accessing,29499,"memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29920,Security,access,access,29920,"_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:31686,Security,access,access,31686," constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the same as any other symbol defined within the; execution session (i.e. they are findable via `ExecutionSession::lookup`, and; so visible to the JIT linker during linking). One way to reflect external symbols is to add them manually using the; absoluteSymbols function:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. JD.define(; absoluteSymbols({; { Mangle(""puts""), ExecutorAddr::fromPtr(&puts)},; { Mangle(""gets""), ExecutorAddr::fromPtr(&getS)}; }));. Using absoluteSymbols is reasonable if the set of symbols to be reflected is; small and fixed. On the other hand, if the set of symbols is large or variable; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33728,Security,expose,expose,33728,"ever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symbolic resolution; using the JIT symbol tables should be preferred: it keeps the IR and objects; readable and reusable in subsequent JIT sessions. Hardcoded addresses are; difficult to read, and usually onl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:10169,Testability,stub,stubs,10169,"a single JITDylib. However, clients who want to JIT code; for languages/projects that traditionally rely on ahead-of-time linking (e.g.; C++) will find that this feature makes life much easier. Symbol lookup in ORC serves two other important functions, beyond providing; addresses for symbols: (1) It triggers compilation of the symbol(s) searched for; (if they have not been compiled already), and (2) it provides the; synchronization mechanism for concurrent compilation. The pseudo-code for the; lookup process is:. .. code-block:: none. construct a query object from a query set and query handler; lock the session; lodge query against requested symbols, collect required materializers (if any); unlock the session; dispatch materializers (if any). In this context a materializer is something that provides a working definition; of a symbol upon request. Usually materializers are just wrappers for compilers,; but they may also wrap a jit-linker directly (if the program representation; backing the definitions is an object file), or may even be a class that writes; bits directly into memory (for example, if the definitions are; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14507,Testability,log,log,14507,"ing established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT stand",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14539,Testability,log,log,14539,"; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14580,Testability,log,log,14580,"; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14724,Testability,log,log,14724,"sions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ...",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:14764,Testability,log,log,14764,"sions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically build absolute; symbol mappings for you. However the absoluteSymbols function is still useful; for making non-global objects in your JIT visible to JIT'd code. For example,; imagine that your JIT standard library needs access to your JIT object to make; some calls. We could bake the address of your object into the library, but then; it would need to be recompiled for each session:. .. code-block:: c++. // From standard library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ...",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15408,Testability,log,log,15408,"library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15429,Testability,log,log,15429,"library for JIT'd code:. class MyJIT {; public:; void log(const char *Msg);; };. void log(const char *Msg) { ((MyJIT*)0x1234)->log(Msg); }. We can turn this into a symbolic reference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15630,Testability,log,log,15630,"eference in the JIT standard library:. .. code-block:: c++. extern MyJIT *__MyJITInstance;. void log(const char *Msg) { __MyJITInstance->log(Msg); }. And then make our JIT object visible to the JIT standard library with an; absolute symbol definition when the JIT is started:. .. code-block:: c++. MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:15902,Testability,log,log,15902," MyJIT J = ...;. auto &JITStdLibJD = ... ;. JITStdLibJD.define(absoluteSymbols(SymbolMap({; { Mangle(""__MyJITInstance""),; { ExecutorAddr::fromPtr(&J), JITSymbolFlags() } }; });. Aliases and Reexports; ---------------------. Aliases and reexports allow you to define new symbols that map to existing; symbols. This can be useful for changing linkage relationships between symbols; across sessions without having to recompile code. For example, imagine that; JIT'd code has access to a log function, ``void log(const char*)`` for which; there are two implementations in the JIT standard library: ``log_fast`` and; ``log_detailed``. Your JIT can choose which one of these definitions will be; used when the ``log`` symbol is referenced by setting up an alias at JIT startup; time:. .. code-block:: c++. auto &JITStdLibJD = ... ;. auto LogImplementationSymbol =; Verbose ? Mangle(""log_detailed"") : Mangle(""log_fast"");. JITStdLibJD.define(; symbolAliases(SymbolAliasMap({; { Mangle(""log""),; { LogImplementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materializ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:16945,Testability,stub,stub,16945,"lementationSymbol; JITSymbolFlags::Exported | JITSymbolFlags::Callable } }; });. The ``symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that so",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:16965,Testability,stub,stub,16965,"`symbolAliases`` function allows you to define aliases within a single; JITDylib. The ``reexports`` function provides the same functionality, but; operates across JITDylib boundaries. E.g. .. code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17066,Testability,stub,stub,17066,". code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally wa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17217,Testability,stub,stub,17217,". code-block:: c++. auto &JD1 = ... ;; auto &JD2 = ... ;. // Make 'bar' in JD2 an alias for 'foo' from JD1.; JD2.define(; reexports(JD1, SymbolAliasMap({; { Mangle(""bar""), { Mangle(""foo""), JITSymbolFlags::Exported } }; });. The reexports utility can be handy for composing a single JITDylib interface by; re-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally wa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:627,Usability,simpl,simple,627,"===============================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; se",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:13054,Usability,simpl,simply,13054,"e) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions they provide, and for unwrapping the program; representation and passing it back to the layer when compilation is required; (this ownership shuffle makes writing thread-safe layers easier, since the; ownership of the program representation will be passed back on the stack,; rather than having to be fished out of a Layer member, which would require; synchronization). - *MaterializationResponsibility* - When a MaterializationUnit hands a program; representation back to the layer it comes with an associated; MaterializationResponsibility object. This object tracks the definitions; that must be materialized and provides a way to notify the JITDylib once they; are either successfully materialized or a failure occurs. Absolute Symbols, Aliases, and Reexports; ========================================. ORC makes it easy to define symbols with absolute addresses, or symbols that; are simply aliases of other symbols:. Absolute Symbols; ----------------. Absolute symbols are symbols that map directly to addresses without requiring; further materialization, for example: ""foo"" = 0x1234. One use case for; absolute symbols is allowing resolution of process symbols. E.g. .. code-block:: c++. JD.define(absoluteSymbols(SymbolMap({; { Mangle(""printf""),; { ExecutorAddr::fromPtr(&printf),; JITSymbolFlags::Callable } }; });. With this mapping established code added to the JIT can refer to printf; symbolically rather than requiring the address of printf to be ""baked in"".; This in turn allows cached versions of the JIT'd code (e.g. compiled objects); to be re-used across JIT sessions as the JIT'd code no longer changes, only the; absolute symbol definition does. For process and library symbols the DynamicLibrarySearchGenerator utility (See; :ref:`How to Add Process and Library Symbols to JITDylibs; <ProcessAndLibrarySymbols>`) can be used to automatically",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24105,Usability,clear,clear,24105,"ylibs and set up linkage relationships; --------------------------------------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24131,Usability,clear,cleared,24131,"----------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` objec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24164,Usability,usab,usable,24164,"----------------------------. In ORC, all symbol definitions reside in JITDylibs. JITDylibs are created by; calling the ``ExecutionSession::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` objec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24258,Usability,clear,clears,24258,"::createJITDylib`` method with a unique name:. .. code-block:: c++. ExecutionSession ES;; auto &JD = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37616,Usability,simpl,simplify,37616,"ld; like JIT'd code to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to have been; used. .. [3] Weak definitions are currently handled correctly within dylibs, but if; multiple dylibs provide a weak definition of a symbol then each will end; up with its own definition (similar to how weak definitions are handled; in Windows DLLs). This will be fix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:37985,Usability,simpl,simply,37985," to be able to ""dlopen"" and ""dlclose"" JITDylibs, running all of; their initializers/deinitializers on the current thread. This would require; support from the runtime library described above. 3. **Debugging support**. ORC currently supports the GDBRegistrationListener API when using RuntimeDyld; as the underlying JIT linker. We will need a new solution for JITLink based; platforms. Further Future Work; -------------------. 1. **Speculative Compilation**. ORC's support for concurrent compilation allows us to easily enable; *speculative* JIT compilation: compilation of code that is not needed yet,; but which we have reason to believe will be needed in the future. This can be; used to hide compile latency and improve JIT throughput. A proof-of-concept; example of speculative compilation with ORC has already been developed (see; ``llvm/examples/SpeculativeJIT``). Future work on this is likely to focus on; re-using and improving existing profiling support (currently used by PGO) to; feed speculation decisions, as well as built-in tools to simplify use of; speculative compilation. .. [1] Formats/architectures vary in terms of supported features. MachO and; ELF tend to have better support than COFF. Patches very welcome!. .. [2] The ``LazyEmittingLayer``, ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` do not have counterparts in the new; system. In the case of ``LazyEmittingLayer`` it was simply no longer; needed: in ORCv2, deferring compilation until symbols are looked up is; the default. The removal of ``RemoteObjectClientLayer`` and; ``RemoteObjectServerLayer`` means that JIT stacks can no longer be split; across processes, however this functionality appears not to have been; used. .. [3] Weak definitions are currently handled correctly within dylibs, but if; multiple dylibs provide a weak definition of a symbol then each will end; up with its own definition (similar to how weak definitions are handled; in Windows DLLs). This will be fixed in the future.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/ORCv2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1120,Availability,avail,available,1120,"ure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1283,Availability,down,down,1283,"These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ==========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1347,Availability,avail,available,1347,"em. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1644,Availability,avail,available,1644,"pps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:463,Deployability,release,release,463,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:542,Deployability,install,installed,542,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:921,Deployability,install,install,921,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:148,Modifiability,config,configure,148,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:999,Modifiability,config,configure,999,"==================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Sh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1978,Modifiability,inherit,inherit,1978,"pps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:678,Performance,optimiz,optimized,678,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:934,Performance,optimiz,optimized,934,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1240,Performance,optimiz,optimization,1240,"These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ==========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1449,Performance,optimiz,optimizing,1449," so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/ind",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1538,Performance,optimiz,optimized,1538,"ps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ============. ``--enable-libffi``; Depend on `libffi <http://sources.redhat.com/libffi/>`_ to allow the LLVM; interpreter to call external functions. ``--with-oprofile``. Depend on `libopagent; <http://oprofile.sourceforge.net/doc/devel/index.html>`_ (>=version 0.9.4); to let the LLVM JIT tell oprofile about function addresses and line; numbers.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:692,Testability,assert,assertions,692,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:832,Testability,assert,assertions,832,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1042,Testability,assert,assertions,1042,"================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enabl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Packaging.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2593,Availability,avail,available,2593,"Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2890,Availability,avail,available,2890,"----------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" fil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:3446,Availability,avail,available,3446,"tors. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:3993,Availability,avail,available,3993,"ram:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:4387,Availability,avail,available,4387,"---------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:4767,Availability,avail,available,4767,"functions that contain the specified substring; will be printed. ``dot-dom``: Print dominance tree of function to ""dot"" file; -----------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph. This graph can then be processed with the :program:`dot` tool to; convert it to postscript or some other suitable format. ``dot-dom-only``: Print dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:5184,Availability,avail,available,5184,"int dominance tree of function to ""dot"" file (with no function bodies); ------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the dominator tree into a ``.dot``; graph, omitting the function bodies. This graph can then be processed with the; :program:`dot` tool to convert it to postscript or some other suitable format. ``dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information An",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9055,Availability,avail,available,9055,"is that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; --------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9111,Availability,error,error,9111,"is that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; --------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9272,Availability,avail,available,9272,"ion query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9341,Availability,error,error,9341,"ion query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarch",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9495,Availability,avail,available,9495,"eadable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categori",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9566,Availability,error,error,9566,"eadable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categori",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:15042,Availability,avail,available,15042,"er. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It should eventually be removed. ``constmerge``: Merge Duplicate Global Constants; ------------------------------------------------. Merges duplicate global constants together into a single constant that is; shared. This is useful because some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:15932,Availability,redundant,redundant,15932,"e some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:16863,Availability,alive,alive,16863,"-------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformati",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17447,Availability,redundant,redundant,17447,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17488,Availability,redundant,redundant,17488," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19091,Availability,down,down,19091,"s guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instru",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19317,Availability,redundant,redundant,19317,"lized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant powe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23098,Availability,redundant,redundant,23098,"ssors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibilit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:29566,Availability,avail,available,29566," be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory reference",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:37391,Availability,avail,available,37391,"ss readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-debug-info``: Strip debug info for unused symbols; --------------------------------------------------------------. .. FIXME: this description is the same as for -strip. performs code stripping. this transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-prototypes``: Strip Unused Function Prototypes; -----------------------------------------------------------. This pass loops over all of the functions in the input module, looking for dead; declarations and removes them. Dead declarations are declarations of functions; for which no implementation is available (i.e., declarations for unused library; functions). ``strip-debug-declare``: Strip all ``llvm.dbg.declare`` intrinsics; ------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal globals and functions; #. debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-nondebug``: Strip all symbols, except dbg symbols, from a module; ------------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal glob",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9739,Deployability,pipeline,pipelined,9739,"his pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:14300,Deployability,update,update,14300," done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first order. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It should eventually be removed. ``constmerge``: Merge Duplicate Global Constants; ------------------------------------------------. Merges duplicate global constants together into a single constant that is; shared. This is useful because some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargeli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27834,Deployability,update,updates,27834,"simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global des",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1510,Energy Efficiency,adapt,adapted,1510,"for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; ----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11522,Energy Efficiency,allocate,allocated,11522," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20336,Energy Efficiency,power,power-of-two,20336,"-----------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21062,Energy Efficiency,reduce,reduce,21062,"<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple pr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24056,Energy Efficiency,reduce,reduce,24056,"eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25894,Energy Efficiency,reduce,reduce,25894,"s are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39359,Energy Efficiency,efficient,efficient,39359,"ping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal globals and functions; #. debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``tailcallelim``: Tail Call Elimination; ---------------------------------------. This file transforms calls of the current function (self recursion) followed by; a return instruction with a branch to the entry of the function, creating a; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2073,Integrability,depend,dependences,2073,"sses; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:8177,Integrability,depend,depends,8177,"anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-reada",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:8268,Integrability,interface,interface,8268,"appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11092,Integrability,depend,dependence,11092,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11134,Integrability,depend,dependencies,11134,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11196,Integrability,depend,dependencies,11196,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18681,Integrability,depend,dependence,18681,"is transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25641,Integrability,wrap,wrapper,25641,"if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28979,Integrability,wrap,wrapper,28979,"ks best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:31714,Integrability,rout,routine,31714," and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5) ⇒ x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1510,Modifiability,adapt,adapted,1510,"for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; ----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:6151,Modifiability,variab,variables,6151,"-------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information Analysis; ----------------------------------------------------. Interface for lazy computation of value constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass wil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:10619,Modifiability,variab,variables,10619,"n to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:10833,Modifiability,variab,variable,10833," are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Eliminat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11533,Modifiability,variab,variables,11533," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11696,Modifiability,variab,variables,11696,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17157,Modifiability,variab,variables,17157,"o not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17274,Modifiability,variab,variables,17274,"addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17689,Modifiability,variab,variables,17689,"igned to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the ind",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17903,Modifiability,variab,variable,17903,"e alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17981,Modifiability,variab,variable,17981," to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transf",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18057,Modifiability,variab,variable,18057,"----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transforme",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18727,Modifiability,variab,variable,18727,"is transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21600,Modifiability,variab,variables,21600,"nsformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple predecessors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25032,Modifiability,variab,variable,25032,"t optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25152,Modifiability,variab,variable,25152,"y of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26109,Modifiability,variab,variable,26109," then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28597,Modifiability,variab,variables,28597,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30743,Modifiability,rewrite,rewrite,30743," generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this functio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39278,Modifiability,variab,variable,39278,"ping. Specifically, it can delete:. #. names for virtual registers; #. symbols for internal globals and functions; #. debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``tailcallelim``: Tail Call Elimination; ---------------------------------------. This file transforms calls of the current function (self recursion) followed by; a return instruction with a branch to the entry of the function, creating a; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:218,Performance,optimiz,optimization,218,"====================================; LLVM's Analysis and Transform Passes; ====================================. .. contents::; :local:. Introduction; ============. This document serves as a high level summary of the optimization features that; LLVM provides. Optimizations are implemented as Passes that traverse some; portion of a program to either collect information or transform the program.; The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:7237,Performance,optimiz,optimization,7237,"ue constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass will warn about it anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:7312,Performance,optimiz,optimization,7312,"ue constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TODO comments, but those; aren't comprehensive either. Second, many conditions cannot be checked; statically. This pass does no dynamic instrumentation, so it can't check for; all possible problems. Another limitation is that it assumes all code will be executed. A store; through a null pointer in a basic block which is never reached is harmless, but; this pass will warn about it anyway. Optimization passes may make conditions that this pass checks for more or less; obvious. If an optimization pass appears to be introducing a warning, it may; be that the optimization pass is merely exposing an existing condition in the; code. This code may be run before :ref:`instcombine <passes-instcombine>`. In many; cases, instcombine checks for the same kinds of things and turns instructions; with undefined behavior into unreachable (or equivalent). Because of this,; this pass makes some effort to look through bitcasts and so on. ``loops``: Natural Loop Information; -----------------------------------. This analysis is used to identify natural loops and determine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:12689,Performance,load,loaded,12689,"fe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block pl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13053,Performance,load,loaded,13053,"that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first orde",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17381,Performance,perform,performs,17381,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17479,Performance,perform,performs,17479," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17498,Performance,load,load,17498," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:18991,Performance,perform,performed,18991,"ormed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant op",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19120,Performance,optimiz,optimization,19120,"s guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instru",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19786,Performance,perform,performed,19786,"p is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19983,Performance,perform,performed,19983,"tions have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21236,Performance,optimiz,optimization,21236,"X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple predecessors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forwa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23238,Performance,optimiz,optimizations,23238,"f the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23501,Performance,perform,performs,23501,"the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23855,Performance,load,loads,23855,"dary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23977,Performance,optimiz,optimizations,23977,"(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and store",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24205,Performance,optimiz,optimizations,24205,"eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24330,Performance,load,loads,24330,"rminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24388,Performance,load,load,24388,"ariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24802,Performance,load,loads,24802,"s, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24959,Performance,load,loads,24959,"t optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:25981,Performance,perform,performs,25981," then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26704,Performance,perform,performs,26704,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28610,Performance,load,loads,28610,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28683,Performance,perform,performance,28683,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30584,Performance,load,loads,30584,"able in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30751,Performance,load,loads,30751," generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this functio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30953,Performance,perform,performs,30953,"come dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:32189,Performance,perform,performs,32189,"ntroduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5) ⇒ x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:33341,Performance,load,load,33341,"ciates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5) ⇒ x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory references. It is intended to be the; inverse of :ref:`mem2reg <passes-mem2reg>`. By converting to ``load``; instructions, the only values live across basic blocks are ``alloca``; instructions and ``load`` instructions before ``phi`` nodes. It is intended; that this should make CFG hacking much easier. To make later hacking easier,; the entry block is split into two, such that all introduced ``alloca``; instructions (and nothing else) are in the entry block. ``sroa``: Scalar Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:33439,Performance,load,load,33439,"ciates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5) ⇒ x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory references. It is intended to be the; inverse of :ref:`mem2reg <passes-mem2reg>`. By converting to ``load``; instructions, the only values live across basic blocks are ``alloca``; instructions and ``load`` instructions before ``phi`` nodes. It is intended; that this should make CFG hacking much easier. To make later hacking easier,; the entry block is split into two, such that all introduced ``alloca``; instructions (and nothing else) are in the entry block. ``sroa``: Scalar Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:36706,Performance,perform,performs,36706,"(...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-debug-info``: Strip debug info for unused symbols; --------------------------------------------------------------. .. FIXME: this description is the same as for -strip. performs code stripping. this transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. note that this transformation makes code much less readable, so it should only; be used in situations where the strip utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``strip-dead-prototypes``: Strip Unused Function Prototypes; -----------------------------------------------------------. This pass loops over all of the functions in the input module, looking for dead; declarations and removes them. Dead declarations are declarations of functions; for which no implementation is available (i.e., declarations for unused library; functions). ``strip-debug-declare``: Strip all ``llvm.dbg.declare`` intrinsics; ------------------------------------------------------------------. .. FIXME: this description is the same as for -strip. This pass implements code stripping. Specifically, it can delete:. #. names",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39386,Performance,perform,performed,39386,"ld only; be used in situations where the 'strip' utility would be used, such as reducing; code size or making it harder to reverse engineer code. ``tailcallelim``: Tail Call Elimination; ---------------------------------------. This file transforms calls of the current function (self recursion) followed by; a return instruction with a branch to the entry of the function, creating a; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:40763,Performance,optimiz,optimization,40763," value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41025,Performance,optimiz,optimization,41025,"d Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped togeth",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41274,Performance,perform,performing,41274,"s only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41285,Performance,optimiz,optimizing,41285,"s only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41507,Performance,perform,performed,41507,"om the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2066,Safety,detect,detect,2066,"sses; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:10131,Safety,detect,detects,10131,"; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11385,Safety,safe,safety,11385,"s; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11561,Safety,safe,safe,11561," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11653,Safety,avoid,avoid,11653,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11691,Safety,safe,safe,11691,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:15932,Safety,redund,redundant,15932,"e some passes (i.e., TraceValues) insert a lot of; string constants into the program, regardless of whether or not an existing; string is available. .. _passes-dce:. ``dce``: Dead Code Elimination; ------------------------------. Dead code elimination is similar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17447,Safety,redund,redundant,17447,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17488,Safety,redund,redundant,17488," not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-bl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19317,Safety,redund,redundant,19317,"lized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant powe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23098,Safety,redund,redundant,23098,"ssors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibilit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23721,Safety,safe,safe,23721,"--------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28793,Safety,safe,safe,28793," should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; --------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2096,Security,access,accesses,2096,"sses; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11578,Security,access,access,11578," and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. I",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11639,Security,sanitiz,sanitizers,11639,"with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of ali",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:16168,Security,access,access,16168,"milar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global varia",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26205,Security,access,access,26205,"--------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and do not contribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:39816,Security,access,access,39816,"; loop. This pass also implements the following extensions to the basic; algorithm:. #. Trivial instructions between the call and return do not prevent the; transformation from taking place, though currently the analysis cannot; support moving any really useful instructions (only dead ones).; #. This pass transforms functions that are prevented from being tail recursive; by an associative expression to use an accumulator variable, thus compiling; the typical naive factorial or fib implementation into efficient code.; #. TRE is performed if the function returns void, if the return returns the; result returned by the call, or if the function returns a run-time constant; on all exits from the function. It is possible, though unlikely, that the; return returns something else (like constant 0), and can still be TRE'd. It; can be TRE'd if *all other* return instructions in the function return the; exact same value.; #. If it can prove that callees do not access their caller stack frame, they; are marked as eligible for tail call elimination (by the code generator). Utility Passes; ==============. This section describes the LLVM Utility Passes. ``deadarghaX0r``: Dead Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numb",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41412,Security,access,access,41412,"--------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:42823,Security,secur,security,42823,"add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruction that returns a value that does; not agree with the function return value type.; #. Function call argument types match the function prototype.; #. All other things that are tested by asserts spread about the code. Note that this does not provide full security verification (like Java), but; instead just tries to ensure that code is well-formed. .. _passes-view-cfg:. ``view-cfg``: View CFG of function; ----------------------------------. Displays the control flow graph using the GraphViz tool.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-cfg-only``: View CFG of function (with no function bodies); -----------------------------------------------------------------. Displays the control flow graph using the GraphViz tool, but omitting function; bodies.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-dom``: View dominance tree of function; ---------------------------------------------. Displays the dominator tree using the GraphViz tool. ``view-dom-only``: View dominance tree of fun",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:11123,Testability,test,tests,11123,"ntry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; -------------------------------------------------. Simple alias analysis implemented in terms of ``ScalarEvolution`` queries. This differs from traditional loop dependence analysis in that it tests for; dependencies within a single iteration of a loop, rather than dependencies; between different iterations. ``ScalarEvolution`` has a more complete understanding of pointer arithmetic; than ``BasicAliasAnalysis``' collection of ad-hoc analyses. ``stack-safety``: Stack Safety Analysis; ---------------------------------------. The ``StackSafety`` analysis can be used to determine if stack allocated; variables can be considered safe from memory access bugs. This analysis' primary purpose is to be used by sanitizers to avoid unnecessary; instrumentation of safe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inline",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20221,Testability,log,logical,20221,"-------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:31728,Testability,log,log,31728," and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5) ⇒ x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:31799,Testability,log,log,31799," and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instruction in them.; Additionally, it keeps track of which node is the new exit node of the CFG. ``partial-inliner``: Partial Inliner; ------------------------------------. This pass performs partial inlining, typically by inlining an ``if`` statement; that surrounds the body of the function. ``reassociate``: Reassociate expressions; ----------------------------------------. This pass reassociates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5) ⇒ x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41059,Testability,test,testing,41059,"d Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped togeth",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41563,Testability,log,logicals,41563," anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:42745,Testability,test,tested,42745,"other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruction that returns a value that does; not agree with the function return value type.; #. Function call argument types match the function prototype.; #. All other things that are tested by asserts spread about the code. Note that this does not provide full security verification (like Java), but; instead just tries to ensure that code is well-formed. .. _passes-view-cfg:. ``view-cfg``: View CFG of function; ----------------------------------. Displays the control flow graph using the GraphViz tool.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-cfg-only``: View CFG of function (with no function bodies); -----------------------------------------------------------------. Displays the control flow graph using the GraphViz tool, but omitting function; bodies.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-dom``: View dominance tree of function; ---------------------------------------------.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:42755,Testability,assert,asserts,42755,"other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped together.; #. PHI nodes must have at least one entry.; #. All basic blocks should only end with terminator insts, not contain them.; #. The entry node to a function must not have predecessors.; #. All Instructions must be embedded into a basic block.; #. Functions cannot take a void-typed parameter.; #. Verify that a function's argument list agrees with its declared type.; #. It is illegal to specify a name for a void value.; #. It is illegal to have an internal global value with no initializer.; #. It is illegal to have a ``ret`` instruction that returns a value that does; not agree with the function return value type.; #. Function call argument types match the function prototype.; #. All other things that are tested by asserts spread about the code. Note that this does not provide full security verification (like Java), but; instead just tries to ensure that code is well-formed. .. _passes-view-cfg:. ``view-cfg``: View CFG of function; ----------------------------------. Displays the control flow graph using the GraphViz tool.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-cfg-only``: View CFG of function (with no function bodies); -----------------------------------------------------------------. Displays the control flow graph using the GraphViz tool, but omitting function; bodies.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are displayed. All functions that contain the specified substring; will be displayed. ``view-dom``: View dominance tree of function; ---------------------------------------------.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1263,Usability,simpl,simple,1263,"mplemented as Passes that traverse some; portion of a program to either collect information or transform the program.; The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:1358,Usability,simpl,simply,1358," The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2221,Usability,simpl,simple,2221,"This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; wil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:2402,Usability,simpl,simple,2402,"as queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ---------------------------. Dependence analysis framework, which is used to detect dependences in memory; accesses. ``domfrontier``: Dominance Frontier Construction; ------------------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominator frontiers. ``domtree``: Dominator Tree Construction; ----------------------------------------. This pass is a simple dominator construction algorithm for finding forward; dominators. ``dot-callgraph``: Print Call Graph to ""dot"" file; -------------------------------------------------. This pass, only available in ``opt``, prints the call graph into a ``.dot``; graph. This graph can then be processed with the ""dot"" tool to convert it to; postscript or some other suitable format. ``dot-cfg``: Print CFG of function to ""dot"" file; ------------------------------------------------. This pass, only available in ``opt``, prints the control flow graph into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format.; Additionally the ``-cfg-func-name=<substring>`` option can be used to filter the; functions that are printed. All functions that contain the specified substring; will be printed. ``dot-cfg-only``: Print CFG of function to ""dot"" file (with no function bodies); -------------------------------------------------------------------------------. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:5525,Usability,simpl,simple,5525,"`dot-post-dom``: Print postdominance tree of function to ""dot"" file; --------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information Analysis; ----------------------------------------------------. Interface for lazy computation of value constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. Firs",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:5715,Usability,simpl,simple,5715,"is pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph. This graph can then be processed with the :program:`dot` tool; to convert it to postscript or some other suitable format. ``dot-post-dom-only``: Print postdominance tree of function to ""dot"" file (with no function bodies); ---------------------------------------------------------------------------------------------------. This pass, only available in ``opt``, prints the post dominator tree into a; ``.dot`` graph, omitting the function bodies. This graph can then be processed; with the :program:`dot` tool to convert it to postscript or some other suitable; format. ``globals-aa``: Simple mod/ref analysis for globals; ---------------------------------------------------. This simple pass provides alias and mod/ref information for global values that; do not have their address taken, and keeps track of whether functions read or; write memory (are ""pure""). For this simple (but very common) case, we can; provide pretty accurate and useful information. ``instcount``: Counts the various types of ``Instruction``\ s; -------------------------------------------------------------. This pass collects the count of all instructions and reports them. ``iv-users``: Induction Variable Users; --------------------------------------. Bookkeeping for ""interesting"" users of expressions computed from induction; variables. ``lazy-value-info``: Lazy Value Information Analysis; ----------------------------------------------------. Interface for lazy computation of value constraint information. ``lint``: Statically lint-checks LLVM IR; ----------------------------------------. This pass statically checks for common and easily-identified constructs which; produce undefined or likely unintended behavior in LLVM IR. It is not a guarantee of correctness, in two ways. First, it isn't; comprehensive. There are checks which could be done statically which are not; yet implemented. Some of these are indicated by TO",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:8781,Usability,simpl,simple,8781,"termine the loop depth of; various nodes of the CFG. Note that the loops identified may actually be; several natural loops that share the same header node... not just a single; natural loop. ``memdep``: Memory Dependence Analysis; --------------------------------------. An analysis that determines, for a given memory operation, what preceding; memory operations it depends on. It builds on alias analysis information, and; tries to provide a lazy, caching interface to a common kind of alias; information query. ``module-debuginfo``: Decodes module-level debug info; -----------------------------------------------------. This pass decodes the debug info metadata in a module and prints in a; (sufficiently-prepared-) human-readable form. For example, run this pass from ``opt`` along with the ``-analyze`` option, and; it'll print to standard output. ``postdomtree``: Post-Dominator Tree Construction; -------------------------------------------------. This pass is a simple post-dominator construction algorithm for finding; post-dominators. ``print-alias-sets``: Alias Set Printer; ---------------------------------------. Yet to be written. ``print-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:9942,Usability,simpl,simply,9942,"-callgraph``: Print a call graph; ---------------------------------------. This pass, only available in ``opt``, prints the call graph to standard error; in a human-readable form. ``print-callgraph-sccs``: Print SCCs of the Call Graph; ------------------------------------------------------. This pass, only available in ``opt``, prints the SCCs of the call graph to; standard error in a human-readable form. ``print-cfg-sccs``: Print SCCs of each function CFG; ---------------------------------------------------. This pass, only available in ``opt``, printsthe SCCs of each function CFG to; standard error in a human-readable fom. ``print-function``: Print function to stderr; --------------------------------------------. The ``PrintFunctionPass`` class is designed to be pipelined with other; ``FunctionPasses``, and prints out the functions of the module as they are; processed. ``print-module``: Print module to stderr; ----------------------------------------. This pass simply prints out the entire module when it is executed. ``regions``: Detect single entry single exit regions; ----------------------------------------------------. The ``RegionInfo`` pass detects single entry single exit regions in a function,; where a region is defined as any subgraph that is connected to the remaining; graph at only two spots. Furthermore, a hierarchical region tree is built. .. _passes-scalar-evolution:. ``scalar-evolution``: Scalar Evolution Analysis; -----------------------------------------------. The ``ScalarEvolution`` analysis can be used to analyze and categorize scalar; expressions in loops. It specializes in recognizing general induction; variables, representing them with the abstract and opaque ``SCEV`` class.; Given this analysis, trip counts of loops and other important properties can be; obtained. This analysis is primarily useful for induction variable substitution and; strength reduction. ``scev-aa``: ScalarEvolution-based Alias Analysis; ----------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:12805,Usability,simpl,simplification,12805,"--------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13658,Usability,simpl,simple,13658,"ment is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first order. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It shoul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:13673,Usability,guid,guided,13673,"ment is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block placement algorithm. The; idea is to put frequently executed blocks together at the start of the function; and hopefully increase the number of fall-through conditional branches. If; there is no profile information for a particular function, this pass basically; orders blocks in depth-first order. ``break-crit-edges``: Break critical edges in CFG; -------------------------------------------------. Break all of the critical edges in the CFG by inserting a dummy basic block.; It may be ""required"" by passes that cannot deal with critical edges. This; transformation obviously invalidates the CFG, but can update forward dominator; (set, immediate dominators, tree, and frontier) information. ``codegenprepare``: Optimize for code generation; ------------------------------------------------. This pass munges the code in the input function to better prepare it for; SelectionDAG-based code generation. This works around limitations in its; basic-block-at-a-time approach. It shoul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:16076,Usability,simpl,simple,16076,"milar to dead instruction elimination, but it; rechecks instructions that were used by removed instructions to see if they; are newly dead. ``deadargelim``: Dead Argument Elimination; ------------------------------------------. This pass deletes dead arguments from internal functions. Dead argument; elimination removes arguments which are directly dead, as well as arguments; only passed into function calls as dead arguments of other functions. This; pass also deletes dead arguments in a similar way. This pass is often useful as a cleanup pass to run after aggressive; interprocedural passes, which add possibly-dead arguments. ``dse``: Dead Store Elimination; -------------------------------. A trivial dead store elimination that only considers basic-block local; redundant stores. .. _passes-function-attrs:. ``function-attrs``: Deduce function attributes; ----------------------------------------------. A simple interprocedural pass which walks the call-graph, looking for functions; which do not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global varia",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17143,Usability,simpl,simple,17143,"o not access or only read non-local memory, and marking them; ``readnone``/``readonly``. In addition, it marks function arguments (of; pointer type) ""``nocapture``"" if a call to the function does not create any; copies of the pointer value that outlive the call. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17742,Usability,simpl,simpler,17742,"igned to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the ind",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19426,Usability,simpl,simple,19426,"s like:. .. code-block:: c++. for (i = 7; i*i < 1000; ++i). into. .. code-block:: c++. for (i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19512,Usability,simpl,simplification,19512,"(i = 0; i != 25; ++i). * Any use outside of the loop of an expression derived from the indvar is; changed to compute the derived value outside of the loop, eliminating the; dependence on the exit value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:19690,Usability,simpl,simple,19690,"it value of the induction variable. If the only purpose; of the loop is to compute the exit value of some derived expression, this; transformation will make the loop dead. This transformation should be followed by strength reduction after all of the; desired loop transformations have been performed. Additionally, on targets; where it is profitable, the loop could be transformed to count down to zero; (the ""do loop"" optimization). ``inline``: Function Integration/Inlining; -----------------------------------------. Bottom-up inlining of functions into callees. .. _passes-instcombine:. ``instcombine``: Combine redundant instructions; -----------------------------------------------. Combine instructions to form fewer, simple instructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20418,Usability,simpl,simplify,20418,"tructions. This pass does not; modify the CFG. This pass is where algebraic simplification happens. This pass combines things like:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; ---------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20606,Usability,simpl,simply,20606,"e:. .. code-block:: llvm. %Y = add i32 %X, 1; %Z = add i32 %Y, 1. into:. .. code-block:: llvm. %Z = add i32 %X, 2. This is a simple worklist driven algorithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:20661,Usability,simpl,simplified,20661,"orithm. This pass guarantees that the following canonicalizations are performed on the; program:. #. If a binary operator has a constant operand, it is moved to the right-hand; side.; #. Bitwise operators with constant operands are always grouped so that shifts; are performed first, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21018,Usability,simpl,simple,21018,"t, then ``or``\ s, then ``and``\ s, then ``xor``\ s.; #. Compare instructions are converted from ``<``, ``>``, ``≤``, or ``≥`` to; ``=`` or ``≠`` if possible.; #. All ``cmp`` instructions on boolean values are replaced with logical; operations.; #. ``add X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23285,Usability,simpl,simpler,23285,"f the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23955,Usability,simpl,simplifies,23955,"(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and store",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26429,Usability,simpl,simple,26429,"ribute to the computation of; the function's return value. .. _passes-loop-extract:. ``loop-extract``: Extract loops into new functions; --------------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; head",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26589,Usability,simpl,simplify,26589,"---------------------------------------------. A pass wrapper around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26607,Usability,simpl,simplify,26607,"er around the ``ExtractLoop()`` scalar transformation to extract; each top-level loop into its own new function. If the loop is the *only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simpl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26772,Usability,simpl,simpler,26772,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26838,Usability,simpl,simpler,26838,"only* loop; in a given function, it is not touched. This is a pass most useful for; debugging via bugpoint. ``loop-reduce``: Loop Strength Reduction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:26966,Usability,simpl,simplify,26966,"uction; ----------------------------------------. This pass performs a strength reduction on array references inside loops that; have as one or more of their components the loop induction variable. This is; accomplished by creating a new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; ----",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27114,Usability,simpl,simplifies,27114,"new value to hold the initial value of the array; access for the first iteration, and then creating a new GEP instruction in the; loop to increment the value by the appropriate amount. .. _passes-loop-rotate:. ``loop-rotate``: Rotate Loops; -----------------------------. A simple loop rotation transformation. A summary of it can be found in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27456,Usability,simpl,simplifies,27456,"ound in; :ref:`Loop Terminology for Rotated Loops <loop-terminology-loop-rotate>`. .. _passes-loop-simplify:. ``loop-simplify``: Canonicalize natural loops; ---------------------------------------------. This pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27621,Usability,simpl,simplifycfg,27621,"his pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27641,Usability,simpl,simplifycfg,27641,"his pass performs several transformations to transform natural loops into a; simpler form, which makes subsequent analyses and transformations simpler and; more effective. A summary of it can be found in; :ref:`Loop Terminology, Loop Simplify Form <loop-terminology-loop-simplify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:27972,Usability,simpl,simple,27972,"lify>`. Loop pre-header insertion guarantees that there is a single, non-critical entry; edge from outside of the loop to the loop header. This simplifies a number of; analyses and transformations, such as :ref:`LICM <passes-licm>`. Loop exit-block insertion guarantees that all exit blocks from the loop (blocks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by cr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28265,Usability,simpl,simple,28265,"ks; which are outside of the loop that have predecessors inside of the loop) only; have predecessors from inside of the loop (and are thus dominated by the loop; header). This simplifies transformations such as store-sinking that are built; into LICM. This pass also guarantees that loops will have exactly one backedge. Note that the :ref:`simplifycfg <passes-simplifycfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:29597,Usability,simpl,simply,29597," be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory reference",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30065,Usability,simpl,simplifycfg,30065,"ctions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations rela",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:34717,Usability,simpl,simplifycfg,34717," Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks are dead unless proven otherwise; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:34733,Usability,simpl,simplifycfg,34733,"------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks are dead unless proven otherwise; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (...); A; C. This can increase the size of the c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:35368,Usability,simpl,simple-loop-unswitch,35368,"ise; * Assumes BasicBlocks are dead unless proven otherwise; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:35393,Usability,simpl,simple-loop-unswitch,35393,"e; * Proves values to be constant, and replaces them with constants; * Proves conditional branches to be unconditional. Note that this pass has a habit of making definitions be dead. It is a good; idea to run a :ref:`DCE <passes-dce>` pass sometime after running this pass. .. _passes-simplifycfg:. ``simplifycfg``: Simplify the CFG; ---------------------------------. Performs dead code elimination and basic block merging. Specifically:. * Removes basic blocks with no predecessors.; * Merges a basic block into its predecessor if there is only one and the; predecessor only has one successor.; * Eliminates PHI nodes for basic blocks with a single predecessor.; * Eliminates a basic block that only contains an unconditional branch. ``sink``: Code sinking; ----------------------. This pass moves instructions into successor blocks, when possible, so that they; aren't executed on paths where their results aren't needed. .. _passes-simple-loop-unswitch:. ``simple-loop-unswitch``: Unswitch loops; ----------------------------------------. This pass transforms loops that contain branches on loop-invariant conditions; to have multiple loops. For example, it turns the left into the right code:. .. code-block:: c++. for (...) if (lic); A for (...); if (lic) A; B; C; B else; C for (...); A; C. This can increase the size of the code exponentially (doubling it every time a; loop is unswitched) so we only unswitch if the resultant code will be smaller; than a threshold. This pass expects :ref:`LICM <passes-licm>` to be run before it to hoist; invariant conditions out of the loop, to make the unswitching opportunity; obvious. ``strip``: Strip all symbols from a module; ------------------------------------------. Performs code stripping. This transformation can delete:. * names for virtual registers; * symbols for internal globals and functions; * debug information. Note that this transformation makes code much less readable, so it should only; be used in situations where the strip utili",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Passes.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2339,Availability,avail,available,2339," be provided after a section name string; (e.g. ``!0 = !{""s1"", !1, !2}``), and a single constant tuple may be reused for; different sections (e.g. ``!0 = !{""s1"", !1, ""s2"", !1}``). Binary Encoding; ===============. *Instructions* result in emitting a single PC, and *functions* result in; emission of the start of the function and a 32-bit size. This is followed by; the auxiliary constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:4565,Availability,toler,tolerant,4565,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2958,Deployability,pipeline,pipeline,2958,"ve relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2009,Integrability,depend,depends,2009,"ction#1``, ``section#2``, ..., ``section#N`` in the; metadata causes the backend to emit the PC for the associated instruction or; function to all named sections. For each emitted PC in a section #N, the; constants ``aux-consts#N`` in the tuple ``!N`` will be emitted after the PC.; Multiple tuples with constant data may be provided after a section name string; (e.g. ``!0 = !{""s1"", !1, !2}``), and a single constant tuple may be reused for; different sections (e.g. ``!0 = !{""s1"", !1, ""s2"", !1}``). Binary Encoding; ===============. *Instructions* result in emitting a single PC, and *functions* result in; emission of the start of the function and a 32-bit size. This is followed by; the auxiliary constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Prop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2716,Performance,optimiz,optimizations,2716,"y constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:2924,Performance,optimiz,optimization,2924,"ve relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization pass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:3873,Performance,optimiz,optimization,3873,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:4677,Performance,optimiz,optimization,4677,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:1802,Safety,avoid,avoid,1802," ... ]; [ !""<section#2"">; [ , !2 ... ]; ... ]; }; !1 = !{ iXX <aux-consts#1>, ... }; !2 = !{ iXX <aux-consts#2>, ... }; ... The occurrence of ``section#1``, ``section#2``, ..., ``section#N`` in the; metadata causes the backend to emit the PC for the associated instruction or; function to all named sections. For each emitted PC in a section #N, the; constants ``aux-consts#N`` in the tuple ``!N`` will be emitted after the PC.; Multiple tuples with constant data may be provided after a section name string; (e.g. ``!0 = !{""s1"", !1, !2}``), and a single constant tuple may be reused for; different sections (e.g. ``!0 = !{""s1"", !1, ""s2"", !1}``). Binary Encoding; ===============. *Instructions* result in emitting a single PC, and *functions* result in; emission of the start of the function and a 32-bit size. This is followed by; the auxiliary constants that followed the respective section name in the; ``MD_pcsections`` metadata. To avoid relocations in the final binary, each PC address stored at ``entry``; is a relative relocation, computed as ``pc - entry``. To decode, a user has to; compute ``entry + *entry``. The size of each entry depends on the code model. With large and medium sized; code models, the entry size matches pointer size. For any smaller code model; the entry size is just 32 bits. Encoding Options; ----------------. Optional encoding options can be passed in the first ``MDString`` operator:; ``<section>!<options>``. The following options are available:. * ``C`` -- Compress constant integers of size 2-8 bytes as ULEB128; this; includes the function size (but excludes the PC entry). For example, ``foo!C`` will emit into section ``foo`` with all constants; encoded as ULEB128. Guarantees on Code Generation; =============================. Attaching ``!pcsections`` metadata to LLVM IR instructions *shall not* affect; optimizations or code generation outside the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarant",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3867,Availability,down,down,3867,"ion on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:11544,Availability,avail,available,11544,"-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a ❤️ to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repository. Please make sure that either:. * You set a git hash as ``sourceControlBaseRevision`` in Phabricator which is; available on the GitHub repository,; * **or** you define the dependencies of your patch in Phabricator,; * **or** your patch can be applied to the main branch. Only then can the build server apply the patch locally and run the builds and; tests. Accessing build results; ^^^^^^^^^^^^^^^^^^^^^^^; Phabricator will automatically trigger a build for every new patch you upload or; modify. Phabricator shows the build results at the top of the entry. Clicking on; the links (in the red box) will show more details:. .. image:: Phabricator_premerge_results.png. The CI will compile and run tests, run clang-format and clang-tidy on lines; changed. If a unit test failed, this is shown below the build status. You can also expand; the unit test to see the details:. .. image:: Phabricator_premerge_unit_tests.png. Opting Out; ^^^^^^^^^^. In case you want to opt-out entirely of pre-merge testing, add yourself to the; `OPT OUT project <https://reviews.llvm.org/project/view/83/>`_. If you decide; to opt-out, please let us know why, so we might b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:16360,Availability,down,down,16360,"abricator review. Check you are happy with the commit message and amend it if necessary.; For example, ensure the 'Author' property of the commit is set to the original author.; You can use a command to correct the author property if it is incorrect:. ::. git commit --amend --author=""John Doe <jdoe@llvm.org>"". Then, make sure the commit is up-to-date, and commit it. This can be done by running; the following:. ::. git pull --rebase https://github.com/llvm/llvm-project.git main; git show # Ensure the patch looks correct.; ninja check-$whatever # Rerun the appropriate tests if needed.; git push https://github.com/llvm/llvm-project.git HEAD:main. Abandoning a change; -------------------. If you decide you should not commit the patch, you should explicitly abandon; the review so that reviewers don't think it is still open. In the web UI,; scroll to the bottom of the page where normally you would enter an overall; comment. In the drop-down Action list, which defaults to ""Comment,"" you should; select ""Abandon Revision"" and then enter a comment explaining why. Click the; Submit button to finish closing the review. Status; ------. Please let us know whether you like it and what could be improved! We're still; working on setting up a bug tracker, but you can email klimek-at-google-dot-com; and chandlerc-at-gmail-dot-com and CC the llvm-dev mailing list with questions; until then. We also could use help implementing improvements. This sadly is; really painful and hard because the Phabricator codebase is in PHP and not as; testable as you might like. However, we've put exactly what we're deploying up; on an `llvm-reviews GitHub project`_ where folks can hack on it and post pull; requests. We're looking into what the right long-term hosting for this is, but; note that it is a derivative of an existing open source project, and so not; trivially a good fit for an official LLVM project. .. _LLVM's Phabricator: https://reviews.llvm.org; .. _`https://reviews.llvm.org`: https://revie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:402,Deployability,patch,patches,402,".. _phabricator-reviews:. =============================; Code Reviews with Phabricator; =============================. .. warning::. Phabricator is deprecated and will be switched to read-only mode in October; 2023, for new code contributions use :ref:`GitHub Pull Requests <github-reviews>`. .. contents::; :local:. If you prefer to use a web user interface for code reviews, you can now submit; your patches for Clang and LLVM at `LLVM's Phabricator`_ instance. While Phabricator is a useful tool for some, the relevant -commits mailing list; is the system of record for all LLVM code review. The mailing list should be; added as a subscriber on all reviews, and Phabricator users should be prepared; to respond to free-form comments in mail sent to the commits list. Sign up; -------. To get started with Phabricator, navigate to `https://reviews.llvm.org`_ and; click the power icon in the top right. You can register with a GitHub account,; a Google account, or you can create your own profile. Make *sure* that the email address registered with Phabricator is subscribed; to the relevant -commits mailing list. If you are not subscribed to the commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:1552,Deployability,patch,patches,1552,"st; is the system of record for all LLVM code review. The mailing list should be; added as a subscriber on all reviews, and Phabricator users should be prepared; to respond to free-form comments in mail sent to the commits list. Sign up; -------. To get started with Phabricator, navigate to `https://reviews.llvm.org`_ and; click the power icon in the top right. You can register with a GitHub account,; a Google account, or you can create your own profile. Make *sure* that the email address registered with Phabricator is subscribed; to the relevant -commits mailing list. If you are not subscribed to the commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2137,Deployability,patch,patch,2137,"he commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2214,Deployability,patch,patch,2214,"he commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2220,Deployability,update,update,2220,"he commit; list, all mail sent by Phabricator on your behalf will be held for moderation. Note that if you use your git user name as Phabricator user name,; Phabricator will automatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2270,Deployability,update,update,2270,"omatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2353,Deployability,update,update,2353,"omatically connect your submits to your Phabricator user in; the `Code Repository Browser`_. Requesting a review via the command line; ----------------------------------------. Phabricator has a tool called *Arcanist* to upload patches from; the command line. To get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2578,Deployability,patch,patches,2578,"get you set up, follow the; `Arcanist Quick Start`_ instructions. You can learn more about how to use arc to interact with; Phabricator in the `Arcanist User Guide`_.; The basic way of creating a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:2653,Deployability,patch,patches,2653,"ng a revision for the current commit in your local; repository is to run:. ::. arc diff HEAD~. Sometime you may want to create a draft revision to show the proof of concept; or for experimental purposes, In that case you can use the `--draft` option. It; will create a new draft revision. The good part is: it will not send mail to; llvm-commit mailing list, patch reviewers, and all other subscribers, buildbot; will also run on every patch update:. ::. arc diff --draft HEAD~. If you later update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3311,Deployability,patch,patch,3311,"r update your commit message, you need to add the `--verbatim`; option to have `arc` update the description on Phabricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3357,Deployability,patch,patch,3357,"habricator:. ::. arc diff --edit --verbatim. .. _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Di",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3403,Deployability,patch,patch,3403," _phabricator-request-review-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3451,Deployability,patch,patch,3451,"-web:. Requesting a review via the web interface; -----------------------------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3482,Deployability,patch,patch,3482,"-------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3585,Deployability,patch,patches,3585,"-------------------. The tool to create and review patches in Phabricator is called; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3612,Deployability,patch,patch,3612,"led; *Differential*. Note that you can upload patches created through git, but using `arc` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repos",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:3710,Deployability,patch,patch,3710,"c` on the; command line (see previous section) is preferred: it adds more metadata to; Phabricator which are useful for the pre-merge testing system and for; propagating attribution on commits when someone else has to push it for you. To make reviews easier, please always include **as much context as; possible** with your diff! Don't worry, Phabricator; will automatically send a diff with a smaller context in the review; email, but having the full file in the web interface will help the; reviewer understand your code. To get a full diff, use one of the following commands (or just use Arcanist; to upload your patch):. * ``git show HEAD -U999999 > mypatch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4353,Deployability,update,updated,4353,"tch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4361,Deployability,patch,patch,4361,"tch.patch``; * ``git diff -U999999 @{u} > mypatch.patch``; * ``git diff HEAD~1 -U999999 > mypatch.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4432,Deployability,update,updated,4432,"h.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4462,Deployability,update,updated,4462,"h.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:4470,Deployability,patch,patch,4470,"h.patch``. Before uploading your patch, please make sure it is formatted properly, as; described in :ref:`How to Submit a Patch <format patches>`. To upload a new patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the text diff or browse to the patch file. Leave this first Repository; field blank. (We'll fill in the Repository later, when sending the review.); Click *Create Diff*.; * Leave the drop down on *Create a new Revision...* and click *Continue*.; * Enter a descriptive title and summary. The title and summary are usually; in the form of a :ref:`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5054,Deployability,patch,patch,5054,":`commit message <commit messages>`.; * Add reviewers (see below for advice). (If you set the Repository field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5121,Deployability,patch,patch-series,5121,"itory field; correctly, llvm-commits or cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5147,Deployability,patch,patch,5147," cfe-commits will be subscribed automatically;; otherwise, you will have to manually subscribe them.); * In the Repository field, enter ""rG LLVM Github Monorepo"".; * Click *Save*. To submit an updated patch:. * Click *Differential*.; * Click *+ Create Diff*.; * Paste the updated diff or browse to the updated patch file. Click *Create Diff*.; * Select the review you want to from the *Attach To* dropdown and click; *Continue*.; * Leave the Repository field blank. (We previously filled out the Repository; for the review request.); * Add comments about the changes in the new diff. Click *Save*. Choosing reviewers: You typically pick one or two people as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5815,Deployability,patch,patch,5815,"le as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5842,Deployability,patch,patch,5842,"le as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5853,Deployability,patch,patches,5853,"le as initial reviewers.; This choice is not crucial, because you are merely suggesting and not requiring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:5867,Deployability,patch,patch,5867,"ring; them to participate. Many people will see the email notification on cfe-commits; or llvm-commits, and if the subject line suggests the patch is something they; should look at, they will. .. _creating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own lin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6160,Deployability,patch,patch,6160,"ating-a-patch-series:. Creating a patch series; -----------------------. Chaining reviews together requires some manual work. There are two ways to do it; (these are also described `here <https://moz-conduit.readthedocs.io/en/latest/arcanist-user.html#series-of-commits>`_; along with some screenshots of what to expect). .. _using-the-web-interface:. Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6461,Deployability,patch,patch-summaries,6461,"Using the web interface; ^^^^^^^^^^^^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. W",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6485,Deployability,patch,patch,6485,"^^^^^^^^^^^^. This assumes that you've already created a Phabricator review for each commit,; using `arc` or the web interface. * Go to what will be the last review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6648,Deployability,patch,patch,6648,"ast review in the series (the most recent).; * Click ""Edit Related Revisions"" then ""Edit Parent Revisions"".; * This will open a dialog where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creati",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6782,Deployability,patch,patch,6782," where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:6840,Deployability,patch,patch,6840," where you will enter the patch number of the parent patch; (or patches). The patch number is of the form D<number> and you can find it by; looking at the URL for the review e.g. reviews.llvm/org/D12345.; * Click ""Save Parent Revisions"" after entering them.; * You should now see a ""Stack"" tab in the ""Revision Contents"" section of the web; interface, showing the parent patch that you added. Repeat this with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; y",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7220,Deployability,update,update,7220,"s with each previous review until you reach the first in the series. This; one won't have a parent since it's the start of the series. If you prefer to start with the first in the series and go forward, you can use the; ""Edit Child Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7424,Deployability,update,updated,7424,"ild Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7440,Deployability,patch,patches,7440,"ild Revisions"" option instead. .. _using-patch-summaries:. Using patch summaries; ^^^^^^^^^^^^^^^^^^^^^. This applies to new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7483,Deployability,patch,patches,7483,"o new and existing reviews, uploaded with `arc` or the web interface. * Upload the first review and note its patch number, either with the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7670,Deployability,patch,patches,7670,"th the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial revie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7684,Deployability,patch,patch,7684,"th the web interface; or `arc`.; * For each commit after that, add the following line to the commit message or patch; summary: ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial revie",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:7842,Deployability,update,update,7842," ""Depends on D<num>"", where ""<num>"" is the patch number of the previous review.; This must be entirely on its own line, with a blank line before it.; For example::. [llvm] Example commit. Depends on D12345. * If you want a single review to have multiple parent reviews then; add more with ""and"", for example: ""Depends on D12344 and D12345"".; * Upload the commit with the web interface or `arc`; (``arc diff --verbatim`` to update an existing review).; * You will see a ""Stack"" tab in the ""Revision Contents"" section of the review; in the web interface, showing the parent review.; * Repeat these steps until you've uploaded or updated all the patches in; your series. When you push the patches, please remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial reviewer(s):. * Use ``git blame`` and the commit log to find names of people who have; recently modified the same area of code tha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:8515,Deployability,patch,patch,8515,"ase remove the ""Depends on"" lines from the; commit messages, since they add noise and duplicate git's implicit ordering. One frequently used workflow for creating a series of patches using patch summaries; is based on git's rebasing. These steps assume that you have a series of commits that; you have not posted for review, but can be adapted to update existing reviews. * git interactive rebase back to the first commit you want to upload for review::. git rebase -i HEAD~<number of commits you have written>. * Mark all commits for editing by changing ""pick"" to ""edit"" in the instructions; git shows.; * Start the rebase (usually by writing and closing the instructions).; * For the first commit:. - Upload the current commit for a review (with ``arc diff`` or the web; interface). - Continue to the next commit with ``git rebase --continue``. * For the rest:. - Add the ""Depends on..."" line using ``git commit --amend``. - Upload for review. - Continue the rebase. * Once the rebase is complete, you've created your patch series. .. _finding-potential-reviewers:. Finding potential reviewers; ---------------------------. Here are a couple of ways to pick the initial reviewer(s):. * Use ``git blame`` and the commit log to find names of people who have; recently modified the same area of code that you are modifying.; * Look in CODE_OWNERS.TXT to see who might be responsible for that area.; * If you've discussed the change on a dev list, the people who participated; might be appropriate reviewers. Even if you think the code owner is the busiest person in the world, it's still; okay to put them as a reviewer. Being the code owner means they have accepted; responsibility for making sure the review happens. Reviewing code with Phabricator; -------------------------------. Phabricator allows you to add inline comments as well as overall comments; to a revision. To add an inline comment, select the lines of code you want; to comment on by clicking and dragging the line numbers in the dif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:9837,Deployability,patch,patch,9837,"difying.; * Look in CODE_OWNERS.TXT to see who might be responsible for that area.; * If you've discussed the change on a dev list, the people who participated; might be appropriate reviewers. Even if you think the code owner is the busiest person in the world, it's still; okay to put them as a reviewer. Being the code owner means they have accepted; responsibility for making sure the review happens. Reviewing code with Phabricator; -------------------------------. Phabricator allows you to add inline comments as well as overall comments; to a revision. To add an inline comment, select the lines of code you want; to comment on by clicking and dragging the line numbers in the diff pane.; When you have added all your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main bran",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10422,Deployability,continuous,continuous,10422,"t; to comment on by clicking and dragging the line numbers in the diff pane.; When you have added all your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a ❤️ to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repos",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10433,Deployability,integrat,integration,10433,"t; to comment on by clicking and dragging the line numbers in the diff pane.; When you have added all your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a ❤️ to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repos",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10485,Deployability,patch,patches,10485,"your comments, scroll to the bottom of the page and; click the Submit button. You can add overall comments in the text box at the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a ❤️ to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repository. Please make sure that either:. * You set a git hash as ``sourceControlBaseRevision`` in Phabric",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst:10622,Deployability,patch,patch,10622,"t the bottom of the page.; When you're done, click the Submit button. Phabricator has many useful features, for example allowing you to select; diffs between different versions of the patch as it was reviewed in the; *Revision Update History*. Most features are self descriptive - explore, and; if you have a question, drop by on #llvm in IRC to get help. Note that as e-mail is the system of reference for code reviews, and some; people prefer it over a web interface, we do not generate automated mail; when a review changes state, for example by clicking ""Accept Revision"" in; the web interface. Thus, please type LGTM into the comment box to accept; a change from Phabricator. .. _pre-merge-testing:. Pre-merge testing; -----------------. The pre-merge tests are a continuous integration (CI) workflow. The workflow; checks the patches uploaded to Phabricator before a user merges them to the main; branch - thus the term *pre-merge testing*. When a user uploads a patch to Phabricator, Phabricator triggers the checks and; then displays the results. This way bugs in a patch are contained during the; code review stage and do not pollute the main branch. Our goal with pre-merge testing is to report most true problems while strongly; minimizing the number of false positive reports. Our goal is that problems; reported are always actionable. If you notice a false positive, please report; it so that we can identify the cause. If you notice issues or have an idea on how to improve pre-merge checks, please; `create a new issue <https://github.com/google/llvm-premerge-checks/issues/new>`_; or give a ❤️ to an existing one. Requirements; ^^^^^^^^^^^^. To get a patch on Phabricator tested, the build server must be able to apply the; patch to the checked out git repository. Please make sure that either:. * You set a git hash as ``sourceControlBaseRevision`` in Phabricator which is; available on the GitHub repository,; * **or** you define the dependencies of your patch in Phabricator,; * **",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Phabricator.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Phabricator.rst
