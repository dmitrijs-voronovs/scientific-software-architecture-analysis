id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/367#issuecomment-716198032:229,Availability,error,error,229,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference?; * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367#issuecomment-716198032
https://github.com/google/deepvariant/issues/367#issuecomment-716198032:235,Integrability,message,message,235,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference?; * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367#issuecomment-716198032
https://github.com/google/deepvariant/issues/370#issuecomment-716678135:995,Testability,log,log,995,"HI,. Seven million realigned BAMs seems to be a right number. For your purposes you may use --regions parameter that would restrict make_examples to a specific region. For example --regions chr20:1000-1500 would generate BAM files for 500 bases. In addition, if you use --regions flag you may want to remove a shardining from output examples files name: make_examples.tfrecord.gz instead of make_examples.tfrecord@60.gz. And you don't need to run it with parallel, you just need to run one instance. --task parameter is also not needed when the output is not sharded.; Something like this:; ```; /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord.gz \; --sample_name sample --regions chr20:1000-1500 2> results/sample/deepvariant/tmp/make_examples.log; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370#issuecomment-716678135
https://github.com/google/deepvariant/issues/371#issuecomment-716733704:65,Modifiability,layers,layers,65,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-716733704
https://github.com/google/deepvariant/issues/371#issuecomment-716733704:295,Modifiability,layers,layers,295,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-716733704
https://github.com/google/deepvariant/issues/371#issuecomment-716733704:24,Security,Access,Accessing,24,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-716733704
https://github.com/google/deepvariant/issues/371#issuecomment-716733704:42,Testability,log,logit,42,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-716733704
https://github.com/google/deepvariant/issues/371#issuecomment-717204430:159,Integrability,interface,interface,159,"@AndrewCarroll 2 was exactly what I meant, thank you!. About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-717204430
https://github.com/google/deepvariant/issues/371#issuecomment-717204430:61,Security,access,accessing,61,"@AndrewCarroll 2 was exactly what I meant, thank you!. About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-717204430
https://github.com/google/deepvariant/issues/371#issuecomment-717204430:75,Testability,log,logit,75,"@AndrewCarroll 2 was exactly what I meant, thank you!. About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-717204430
https://github.com/google/deepvariant/issues/371#issuecomment-717581087:358,Testability,log,logits,358,"Hi @maricatovictor . At this time, we have not put effort into abstracting these components into a module. We'll take your request into account for future development. It is definitely something that we would like to do, but we likely have a few more pressing issues to address. Yes, right now you would need to fork the code to be able to work with the pre-logits. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371#issuecomment-717581087
https://github.com/google/deepvariant/issues/372#issuecomment-717644098:86,Availability,error,error,86,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372#issuecomment-717644098
https://github.com/google/deepvariant/issues/372#issuecomment-717644098:92,Integrability,message,message,92,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372#issuecomment-717644098
https://github.com/google/deepvariant/issues/374#issuecomment-723752207:46,Availability,down,down,46,"My current investigation shows that this went down the path here:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105; But FAI format doesn't get read here, so it got into the last branch of the switch:; ```; default:; errno = EFTYPE;; goto error;; ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723752207
https://github.com/google/deepvariant/issues/374#issuecomment-723752207:416,Availability,error,error,416,"My current investigation shows that this went down the path here:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105; But FAI format doesn't get read here, so it got into the last branch of the switch:; ```; default:; errno = EFTYPE;; goto error;; ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723752207
https://github.com/google/deepvariant/issues/374#issuecomment-723752207:559,Safety,detect,detection,559,"My current investigation shows that this went down the path here:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105; But FAI format doesn't get read here, so it got into the last branch of the switch:; ```; default:; errno = EFTYPE;; goto error;; ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723752207
https://github.com/google/deepvariant/issues/374#issuecomment-723753324:145,Availability,error,error,145,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723753324
https://github.com/google/deepvariant/issues/374#issuecomment-723753324:46,Deployability,update,update,46,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723753324
https://github.com/google/deepvariant/issues/374#issuecomment-723753324:65,Integrability,message,message,65,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723753324
https://github.com/google/deepvariant/issues/374#issuecomment-723753324:151,Integrability,message,message,151,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374#issuecomment-723753324
https://github.com/google/deepvariant/issues/376#issuecomment-720015500:228,Usability,simpl,simplify,228,"Hi @anands-repo ; Our codebase still supports TPU like before!; We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial.; If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know.; (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720015500
https://github.com/google/deepvariant/issues/376#issuecomment-720015500:531,Usability,feedback,feedback,531,"Hi @anands-repo ; Our codebase still supports TPU like before!; We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial.; If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know.; (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720015500
https://github.com/google/deepvariant/issues/376#issuecomment-720216342:140,Energy Efficiency,efficient,efficient,140,"@pichuan Thanks for confirming!. I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720216342
https://github.com/google/deepvariant/issues/376#issuecomment-720216342:255,Usability,feedback,feedback,255,"@pichuan Thanks for confirming!. I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/376#issuecomment-720216342
https://github.com/google/deepvariant/issues/377#issuecomment-785387024:4,Deployability,update,updates,4,Any updates regarding this?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377#issuecomment-785387024
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:442,Availability,checkpoint,checkpoints,442,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:515,Availability,checkpoint,checkpoint,515,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:595,Availability,checkpoint,checkpoint,595,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:695,Availability,checkpoint,checkpoint,695,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:750,Performance,concurren,concurrently,750,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:281,Testability,log,logs,281,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/378#issuecomment-721485772:304,Testability,log,log,304,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772
https://github.com/google/deepvariant/issues/379#issuecomment-723720026:60,Performance,perform,performs,60,"Hi, glad to see my issue is helping you.; Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one.; It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-723720026
https://github.com/google/deepvariant/issues/379#issuecomment-723807366:554,Usability,simpl,simple,554,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1.; 2. The bam-file read I mentioned: . - Starts at: 10001358; - Cigar: 78M, 5I, 18M. That means that we have; 10001358 ... 10001435 X X X X X 10001436 ... 10001453; M ... M I I I I I M ... M; Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, ; - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-723807366
https://github.com/google/deepvariant/issues/379#issuecomment-724880533:1264,Testability,benchmark,benchmarks,1264,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-724880533
https://github.com/google/deepvariant/issues/379#issuecomment-724880533:306,Usability,learn,learning,306,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379#issuecomment-724880533
https://github.com/google/deepvariant/issues/381#issuecomment-726521665:268,Availability,avail,available,268,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-726521665
https://github.com/google/deepvariant/issues/381#issuecomment-726521665:425,Availability,avail,available,425,"Hi @umahsn. 1. Training data comes from: the Human Pangenome Reference Consortium, [linked in this GitHub repo](https://github.com/human-pangenomics/HG002_Data_Freeze_v1.0). This includes HiFi sequencing for HG003 and HG004 which we contributed. It also includes data available on the [Genome in a Bottle FTP](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/) in the CCS folders for individuals. 2. Most of these datasets are available as FASTQ files. These are mapped with pbmm2 using the default parameters for HiFi. 3. For HG002 and HG004, we use v4.2 truth set. For the other samples, we use the v3.3.2 truth set. For HG001, HG005, HG006, and HG007, the truth set used is still v3.3.2 for training, since we do not yet have v4.2 for these samples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-726521665
https://github.com/google/deepvariant/issues/381#issuecomment-727285166:399,Availability,error,error,399,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166
https://github.com/google/deepvariant/issues/381#issuecomment-727285166:459,Availability,checkpoint,checkpoint,459,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166
https://github.com/google/deepvariant/issues/381#issuecomment-727285166:492,Availability,checkpoint,checkpoint,492,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166
https://github.com/google/deepvariant/issues/381#issuecomment-727285166:227,Deployability,release,released,227,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166
https://github.com/google/deepvariant/issues/381#issuecomment-727285166:58,Testability,benchmark,benchmark,58,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166
https://github.com/google/deepvariant/issues/381#issuecomment-727285166:770,Testability,benchmark,benchmark,770,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166
https://github.com/google/deepvariant/issues/381#issuecomment-727742145:680,Deployability,release,release,680,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727742145
https://github.com/google/deepvariant/issues/381#issuecomment-727742145:646,Testability,benchmark,benchmark,646,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-727742145
https://github.com/google/deepvariant/issues/381#issuecomment-728645413:398,Deployability,release,release,398,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413
https://github.com/google/deepvariant/issues/381#issuecomment-728645413:188,Performance,perform,performance,188,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413
https://github.com/google/deepvariant/issues/381#issuecomment-728645413:169,Testability,test,test,169,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413
https://github.com/google/deepvariant/issues/381#issuecomment-728645413:208,Testability,benchmark,benchmark,208,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413
https://github.com/google/deepvariant/issues/381#issuecomment-728645413:309,Testability,benchmark,benchmark,309,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:734,Availability,checkpoint,checkpoint,734,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:15,Deployability,release,release,15,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:559,Performance,perform,performance,559,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:656,Performance,perform,performance,656,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:701,Performance,tune,tune,701,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:68,Testability,benchmark,benchmark,68,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:173,Testability,benchmark,benchmarking,173,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/381#issuecomment-728683346:595,Testability,test,test,595,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346
https://github.com/google/deepvariant/issues/383#issuecomment-727745570:252,Modifiability,variab,variables,252,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570
https://github.com/google/deepvariant/issues/383#issuecomment-727745570:266,Performance,load,loaded,266,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570
https://github.com/google/deepvariant/issues/383#issuecomment-727745570:327,Usability,learn,learn,327,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570
https://github.com/google/deepvariant/issues/385#issuecomment-728239211:17,Availability,error,error,17,"@gevro Given the error information, can you try installing setuptools?; For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385#issuecomment-728239211
https://github.com/google/deepvariant/issues/385#issuecomment-728239211:48,Deployability,install,installing,48,"@gevro Given the error information, can you try installing setuptools?; For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385#issuecomment-728239211
https://github.com/google/deepvariant/issues/385#issuecomment-728242699:29,Availability,error,error,29,"I tried, but then I get this error:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>; import tensorflow as tf; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>; from tensorflow_core import *; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 40, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load; File ""<frozen importlib._bootstrap>"", line 947, in _find_and_load_unlocked; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__; module = self._load(); File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load; module = _importlib.import_module(self.__name__); File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py"", line 68, in <module>; from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/core/protobuf/meta_graph_pb2.py"", line 526, in <module>; serialized_options=None, file=DESCRIPTOR),; File ""/tmp/Bazel.runfiles_39ce4fku/runfiles/com_google_protobuf/python/google/protobuf/descriptor.py"", line 534, in __new__; return _message.default_pool.FindFieldByName(full_name); KeyError: ""Couldn't find field tensorflow.TensorInfo.CompositeTensor.type_spec""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385#issuecomment-728242699
https://github.com/google/deepvariant/issues/385#issuecomment-728242962:31,Security,access,accessing,31,"For some reason singularity is accessing my local python libs, instead of the singularity image's libraries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385#issuecomment-728242962
https://github.com/google/deepvariant/issues/385#issuecomment-730789300:336,Availability,error,error,336,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385#issuecomment-730789300
https://github.com/google/deepvariant/issues/386#issuecomment-728248806:101,Usability,simpl,simple,101,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386#issuecomment-728248806
https://github.com/google/deepvariant/issues/386#issuecomment-728300689:200,Availability,down,downstream,200,"Thank you! I'm still curious-- if RefCall entries are rejected by 'call_variants', why are they still reported in the final VCF? Do they enable some sort of helpful QC checks? If so, which? Since any downstream application would filter these out, I'm a bit unclear why an internal variant nomination process is included in the final output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/386#issuecomment-728300689
https://github.com/google/deepvariant/issues/388#issuecomment-734139746:65,Usability,feedback,feedback,65,"Thank you, we're taking a look at the example and expect to have feedback relatively soon (with some delay for holidays in the US).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388#issuecomment-734139746
https://github.com/google/deepvariant/issues/388#issuecomment-735306929:345,Usability,simpl,simpleRepeat,345,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer.; You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:; https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:; TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388#issuecomment-735306929
https://github.com/google/deepvariant/issues/390#issuecomment-737431101:224,Availability,error,error,224,"When I copy pasted your command, it was split up into two commands, with the `--ref` onwards showing up as a separate command. Could you try the following? This should show up as one command and you should no longer see the error. ```; BAMName=""VK446chrYx19""; INPUT_DIR=""${PWD}/""; OUTPUT_DIR=""${PWD}/${BAMName}_dv"". docker run \; -v ""${INPUT_DIR}:/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""1.0.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/Hg19ChrY.fa \; --reads=/input/${BAMName}.bam \; --output_vcf=/output/${BAMName}.vcf.gz \; --output_gvcf=/output/${BAMName}.g.vcf.gz \; --num_shards=24; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/390#issuecomment-737431101
https://github.com/google/deepvariant/issues/392#issuecomment-739048960:285,Availability,error,errors,285,"@wharvey31 when running `make_examples`, you'll need to add `--alt_aligned_pileup=diff_channels` for the PacBio model provided in v1.0.0. This will result in tf.Examples with two additional channels, so the shapes should match after that. If you did add this flag but are still seeing errors, please let me know, and I can help troubleshoot further. I would recommend using the `run_deepvariant` script when possible, as this will set various flags correctly for each model type (Illumina WGS, Illumina WES, PacBio, Hybrid). See [this quickstart doc](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for an example of how to use this script through Docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392#issuecomment-739048960
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:248,Availability,avail,available,248,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:451,Availability,echo,echo,451,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:591,Deployability,update,update,591,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:612,Deployability,install,install,612,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:628,Deployability,install,install-recommends,628,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:184,Performance,perform,perform,184,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-742247654:56,Testability,benchmark,benchmark,56,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654
https://github.com/google/deepvariant/pull/393#issuecomment-743129654:86,Testability,log,logging,86,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-743129654
https://github.com/google/deepvariant/pull/393#issuecomment-743129654:114,Testability,Benchmark,Benchmarked,114,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393#issuecomment-743129654
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:646,Deployability,Release,Releases,646,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:691,Deployability,update,update,691,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:754,Deployability,release,releases,754,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:207,Testability,test,tested,207,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:283,Testability,test,test,283,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:354,Testability,test,tested,354,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:852,Testability,test,test,852,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742111917:940,Testability,test,testing,940,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917
https://github.com/google/deepvariant/issues/394#issuecomment-742326989:32,Deployability,release,releases,32,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742326989
https://github.com/google/deepvariant/issues/394#issuecomment-742326989:167,Deployability,install,install,167,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742326989
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:333,Availability,echo,echo,333,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:15,Deployability,update,update,15,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:339,Deployability,Install,Installing,339,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:415,Deployability,install,install,415,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:502,Deployability,install,install,502,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:790,Deployability,release,release,790,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:688,Testability,test,testing,688,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-742700347:753,Usability,simpl,simplifying,753,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347
https://github.com/google/deepvariant/issues/394#issuecomment-743131852:124,Deployability,install,installing,124,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394#issuecomment-743131852
https://github.com/google/deepvariant/issues/396#issuecomment-743387975:5,Availability,error,error,5,"This error might be caused by missing base quality scores in the BAM file, were you expecting this?; DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396#issuecomment-743387975
https://github.com/google/deepvariant/issues/396#issuecomment-743387975:286,Availability,reliab,reliable,286,"This error might be caused by missing base quality scores in the BAM file, were you expecting this?; DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396#issuecomment-743387975
https://github.com/google/deepvariant/issues/396#issuecomment-743390856:7,Availability,error,error,7,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this?; > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396#issuecomment-743390856
https://github.com/google/deepvariant/issues/396#issuecomment-743390856:290,Availability,reliab,reliable,290,"> This error might be caused by missing base quality scores in the BAM file, were you expecting this?; > DeepVariant does require valid base quality scores. You could technically use filler values, but DeepVariant was only trained with real base qualities, so the results will be much more reliable and accurate if you can get a BAM file with real base quality scores. I am expecting missing quality scores. I would like to ignore it. Would that be possible?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396#issuecomment-743390856
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:185,Availability,down,download,185,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:562,Availability,error,errors,562,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:1127,Availability,reliab,reliably,1127,"but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] St",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:1443,Availability,checkpoint,checkpoint,1443,"don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for ; best performance.; I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters; W1217 09:08:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:7021,Availability,checkpoint,checkpoint,7021,"ored_session.py:222] Graph was finalized.; W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho; n.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt; I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op.; I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op.; I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA...; I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt; I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]; real 2m5.783s; user 0m49.664s; sys 0m7.008s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm; p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:104,Deployability,install,install,104,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:150,Deployability,update,update,150,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:225,Deployability,Release,Release,225,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:250,Deployability,Release,Release,250,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:284,Deployability,install,install,284,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:333,Deployability,install,installation,333,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:394,Deployability,install,installation,394,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:434,Deployability,install,install,434,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:2622,Modifiability,config,config,2622,"not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for ; best performance.; I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters; W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T; I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra; in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non; e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':; None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus; ter': 0, '_master': ''}; I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.; python.framework.ops) is deprecated and will be removed in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:2281,Performance,Tune,Tune,2281,"/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for ; best performance.; I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters; W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T; I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra; in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non; e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':; None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus; ter': 0, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:2332,Performance,perform,performance,2332,"/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for ; best performance.; I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters; W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T; I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra; in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non; e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':; None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus; ter': 0, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749313156:1100,Testability,test,test,1100,"but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] St",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:391,Availability,error,error,391,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:10,Deployability,install,installation,10,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:137,Deployability,install,install,137,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:153,Deployability,install,install-using-the-repository,153,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:806,Deployability,configurat,configuration,806,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:936,Deployability,release,released,936,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:806,Modifiability,config,configuration,806,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-749327121:376,Usability,clear,clear,376,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121
https://github.com/google/deepvariant/issues/399#issuecomment-751141693:319,Deployability,install,install,319,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-751141693
https://github.com/google/deepvariant/issues/399#issuecomment-751141693:375,Deployability,release,release,375,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-751141693
https://github.com/google/deepvariant/issues/399#issuecomment-751141693:592,Deployability,deploy,deploy,592,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-751141693
https://github.com/google/deepvariant/issues/399#issuecomment-751141693:246,Energy Efficiency,allocate,allocated,246,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399#issuecomment-751141693
https://github.com/google/deepvariant/issues/400#issuecomment-749251835:32,Availability,error,error,32,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-749251835
https://github.com/google/deepvariant/issues/400#issuecomment-749251835:38,Integrability,message,message,38,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-749251835
https://github.com/google/deepvariant/issues/400#issuecomment-749251835:416,Modifiability,config,config,416,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-749251835
https://github.com/google/deepvariant/issues/400#issuecomment-749549925:39,Availability,error,error,39,"> Hi @husamia; > ; > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine.; > ; > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia; > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-749549925
https://github.com/google/deepvariant/issues/400#issuecomment-749549925:45,Integrability,message,message,45,"> Hi @husamia; > ; > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine.; > ; > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia; > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-749549925
https://github.com/google/deepvariant/issues/400#issuecomment-749549925:430,Modifiability,config,config,430,"> Hi @husamia; > ; > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine.; > ; > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia; > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-749549925
https://github.com/google/deepvariant/issues/400#issuecomment-750309235:124,Availability,error,error,124,"after cleaning up Docker space with prune and making sure there is space on disk and rerunning. I am getting the same exact error. I feel that the docker container settings itself is the issue but not the software. i.e. maybe there is a hard limit on the temporary space in the container, as the data is generated and written in the tmp when it fails, nothing is saved to the disk where my input and output is located when it fails. It completes the process and fails before the folder is being written to disk from inside the container. I pulled it from docker pull google/deepvariant:latest-deeptrio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-750309235
https://github.com/google/deepvariant/issues/400#issuecomment-750715758:638,Testability,test,testdata,638,"Hi @husamia ,; I see that you're running DeepTrio.; For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; TOTAL: 3 objects, 138872581896 bytes (129.34 GiB); ```; 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-750715758
https://github.com/google/deepvariant/issues/400#issuecomment-750715758:758,Testability,test,testdata,758,"Hi @husamia ,; I see that you're running DeepTrio.; For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; TOTAL: 3 objects, 138872581896 bytes (129.34 GiB); ```; 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-750715758
https://github.com/google/deepvariant/issues/400#issuecomment-750715758:878,Testability,test,testdata,878,"Hi @husamia ,; I see that you're running DeepTrio.; For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; TOTAL: 3 objects, 138872581896 bytes (129.34 GiB); ```; 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-750715758
https://github.com/google/deepvariant/issues/400#issuecomment-750715758:998,Testability,test,testdata,998,"Hi @husamia ,; I see that you're running DeepTrio.; For both DeepVariant or DeepTrio, it will generate intermediate files for each step (make_examples, call_variants). . For DeepTrio, the size of these intermediate files might even be bigger, which might fill up your disk in the process. I guess it is possible that Docker setting has some limit. I'm not familiar enough Docker to help with that. A few suggestions here:. 1. Can you tell us how big your 3 BAM files are? For comparison, what we used in WGS in https://github.com/google/deepvariant/blob/r1.1/docs/metrics-deeptrio.md is:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG00?.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.87 GiB 2020-07-16T22:57:42Z gs://deepvariant/case-study-testdata/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 43.59 GiB 2020-08-19T20:40:44Z gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; 42.88 GiB 2020-08-19T20:40:34Z gs://deepvariant/case-study-testdata/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam; TOTAL: 3 objects, 138872581896 bytes (129.34 GiB); ```; 2. You can try one run with just one chromosome first, e.g., just chr20 (using the `--regions` flag) to see if I can get that run to finish. . I'm going to close this issue first. But if there are anything else we can help with, feel free to reopen or file a new issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-750715758
https://github.com/google/deepvariant/issues/400#issuecomment-751781255:391,Energy Efficiency,reduce,reduce,391,"1. my files are larger than the examples . 60G Dec 6 2019 19CT021737-19CT021737-20190619_64683_S7_346963.bam; 60G Dec 6 2019 19CT021740-19CT021740-20190619_64686_S8_346962.bam; 61G Dec 6 2019 DS187706-DS187706_39743_S1_261735.bam. 2. I am going to try it with --regions 20. **3. I am only interested in the PASS variants, so is it possible to filter output to only those variants? this will reduce the size of the output considerably.**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-751781255
https://github.com/google/deepvariant/issues/400#issuecomment-751790870:564,Energy Efficiency,reduce,reduce,564,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:; ```; TOTAL: 387 objects, 277723436467 bytes (258.65 GiB); ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. ; https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124; If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400#issuecomment-751790870
https://github.com/google/deepvariant/issues/402#issuecomment-756252754:123,Testability,test,testdata,123,"This seems to run fine when using the following script:; ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-756252754
https://github.com/google/deepvariant/issues/402#issuecomment-757012167:75,Availability,echo,echo,75,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks!. ```; BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-757012167
https://github.com/google/deepvariant/issues/402#issuecomment-757012167:385,Availability,echo,echo,385,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks!. ```; BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-757012167
https://github.com/google/deepvariant/issues/402#issuecomment-757012167:904,Availability,echo,echo,904,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks!. ```; BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-757012167
https://github.com/google/deepvariant/issues/402#issuecomment-757012167:335,Testability,test,testdata,335,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks!. ```; BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-757012167
https://github.com/google/deepvariant/issues/402#issuecomment-757014380:11,Safety,sanity check,sanity check,11,"As another sanity check, what's the output of `ls -l ${INPUT_DIR}`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-757014380
https://github.com/google/deepvariant/issues/402#issuecomment-761662095:239,Testability,test,testdata,239,"Here's the output of those two commands:; ```; singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz; ```. The output of `ls -l ${INPUT_DIR}` is:; ```; -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam; -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai; -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed; -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai; -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai; -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-761662095
https://github.com/google/deepvariant/issues/402#issuecomment-761662095:321,Testability,test,testdata,321,"Here's the output of those two commands:; ```; singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz; ```. The output of `ls -l ${INPUT_DIR}` is:; ```; -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam; -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai; -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed; -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai; -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai; -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-761662095
https://github.com/google/deepvariant/issues/402#issuecomment-761662095:848,Testability,test,testdata,848,"Here's the output of those two commands:; ```; singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz; ```. The output of `ls -l ${INPUT_DIR}` is:; ```; -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam; -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai; -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed; -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai; -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai; -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-761662095
https://github.com/google/deepvariant/issues/402#issuecomment-761662095:930,Testability,test,testdata,930,"Here's the output of those two commands:; ```; singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=/home/sk2847/scratch60/quickstart-output/output.vcf.gz --output_gvcf=/home/sk2847/scratch60/quickstart-output/output.g.vcf.gz --intermediate_results_dir /home/sk2847/scratch60/quickstart-output/intermediate_results_dir --num_shards=1. singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/home/sk2847/scratch60/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions chr20:10,000,000-10,010,000 --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz; ```. The output of `ls -l ${INPUT_DIR}` is:; ```; -rw-r--r-- 1 sk2847 kahle 3925783 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam; -rw-r--r-- 1 sk2847 kahle 5472 Nov 27 2017 NA12878_S1.chr20.10_10p1mb.bam.bai; -rw-r--r-- 1 sk2847 kahle 264 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.bed; -rw-r--r-- 1 sk2847 kahle 5728 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; -rw-r--r-- 1 sk2847 kahle 197 Nov 27 2017 test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; -rw-r--r-- 1 sk2847 kahle 64286038 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.fai; -rw-r--r-- 1 sk2847 kahle 606944 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz; -rw-r--r-- 1 sk2847 kahle 23 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.fai; -rw-r--r-- 1 sk2847 kahle 15704 Nov 27 2017 ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Which is what I would expect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-761662095
https://github.com/google/deepvariant/issues/402#issuecomment-901465519:44,Availability,error,error,44,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-901465519
https://github.com/google/deepvariant/issues/402#issuecomment-901465519:50,Integrability,message,message,50,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402#issuecomment-901465519
https://github.com/google/deepvariant/issues/403#issuecomment-759187363:1073,Testability,log,logic,1073,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-759187363
https://github.com/google/deepvariant/issues/403#issuecomment-759187363:1089,Usability,simpl,simple,1089,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-759187363
https://github.com/google/deepvariant/issues/403#issuecomment-760221333:77,Usability,clear,clear,77,"Hi @AndrewCarroll,. Thank you very much for this detailed explanation, it is clear to me what is happening now. Unfortunately, this is rather problematic for my use case because I am processing a cohort (DeepVariant + GLnexus) and I impute the variants afterwards. The imputation is based on the PL values: If there are sites where some samples have an actual variant but most of the other samples have a no call (hence with a discrepancy between GQ and PLs), those sites will end up with a low imputation score. Since my reads are ONT corrected, I rely very much on the imputation scores as a post-filtering step to guarantee the quality of my set. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760221333
https://github.com/google/deepvariant/issues/403#issuecomment-760500649:948,Availability,down,downstream,948,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself?; 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)?; 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation?. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760500649
https://github.com/google/deepvariant/issues/403#issuecomment-760500649:1292,Availability,down,downstream,1292,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself?; 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)?; 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation?. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760500649
https://github.com/google/deepvariant/issues/403#issuecomment-760500649:1209,Usability,simpl,simple,1209,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself?; 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)?; 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation?. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760500649
https://github.com/google/deepvariant/issues/403#issuecomment-760775624:382,Availability,down,downstream,382,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation.; 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation).; 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760775624
https://github.com/google/deepvariant/issues/403#issuecomment-760775624:160,Usability,simpl,simply,160,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation.; 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation).; 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760775624
https://github.com/google/deepvariant/issues/403#issuecomment-760775624:226,Usability,simpl,simpler,226,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation.; 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation).; 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-760775624
https://github.com/google/deepvariant/issues/403#issuecomment-761187324:309,Availability,down,downstream,309,"Thank you for clarification Guillaume - I think deleting `GQ = 0` sites is a reasonable solution if that works for your use case. I agree with you that the current `GT`, `GQ`, `PL` values we emit in this particular case can be confusing. Please let us know if you have any suggestion on improving it for your downstream application (i.e. emitting different `PL` values) - I'll be sure to discuss it with my team. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-761187324
https://github.com/google/deepvariant/issues/403#issuecomment-768232068:39,Usability,feedback,feedback,39,"Sorry for the delay, I appreciate your feedback in the matter. Closing the issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403#issuecomment-768232068
https://github.com/google/deepvariant/issues/404#issuecomment-761285739:993,Deployability,update,update,993,"Hi @ASLeonard , thanks for the report.; Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together.; I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:; 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves.; 2. At this line:; https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80; We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. ; Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761285739
https://github.com/google/deepvariant/issues/404#issuecomment-761285739:79,Testability,test,tested,79,"Hi @ASLeonard , thanks for the report.; Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together.; I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:; 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves.; 2. At this line:; https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80; We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. ; Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761285739
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1789,Availability,down,download,1789,"ant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2997,Availability,ERROR,ERROR,2997,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:4302,Availability,down,download,4302,"Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; ...; ```; After this, the call_variants steps started running with no issues:; ```; ...; I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants.; I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO); I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO); I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO); I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO); ...; ```; My run complete",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1098,Deployability,install,install,1098,"n't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1256,Deployability,install,installing,1256,"ss.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1307,Deployability,install,install,1307,"command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3712,Deployability,Update,Update,3712,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3723,Deployability,configurat,configuration,3723,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3784,Deployability,configurat,configuration,3784,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:4323,Deployability,upgrade,upgrade,4323,"Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; ...; ```; After this, the call_variants steps started running with no issues:; ```; ...; I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants.; I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO); I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO); I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO); I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO); ...; ```; My run complete",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3028,Modifiability,inherit,inherited,3028,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3062,Modifiability,layers,layers,3062,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3085,Modifiability,inherit,inherited,3085,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3120,Modifiability,layers,layers,3120,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3143,Modifiability,inherit,inherited,3143,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3200,Modifiability,inherit,inherited,3200,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3681,Modifiability,layers,layers,3681,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3723,Modifiability,config,configuration,3723,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3784,Modifiability,config,configuration,3784,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3873,Modifiability,config,config,3873,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2824,Performance,Optimiz,Optimizer,2824,"/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection AP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3898,Performance,Optimiz,Optimizer,3898,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3842,Safety,Detect,Detection,3842,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:268,Security,access,access,268,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:34,Testability,log,log,34,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:56,Testability,test,test,56,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1276,Testability,log,logged,1276,"ss.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1294,Testability,log,logged,1294,"ss.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2697,Testability,log,logs,2697,"-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2986,Testability,Log,Log,2986,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014
https://github.com/google/deepvariant/issues/404#issuecomment-761608040:222,Security,access,access,222,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761608040
https://github.com/google/deepvariant/issues/404#issuecomment-761608040:347,Security,access,access,347,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761608040
https://github.com/google/deepvariant/issues/404#issuecomment-761609874:461,Deployability,update,update,461,"Thanks @ASLeonard . Can you tell me:; 1. Which Singularity version do you have. (and OS version too); 2. What command did you run?. I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look.; I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-761609874
https://github.com/google/deepvariant/issues/404#issuecomment-762181198:343,Availability,error,error,343,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198
https://github.com/google/deepvariant/issues/404#issuecomment-762181198:1061,Availability,error,error,1061,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198
https://github.com/google/deepvariant/issues/404#issuecomment-762181198:1301,Availability,error,error,1301,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198
https://github.com/google/deepvariant/issues/404#issuecomment-762181198:1431,Availability,error,error,1431,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198
https://github.com/google/deepvariant/issues/404#issuecomment-762181198:60,Deployability,release,release,60,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198
https://github.com/google/deepvariant/issues/404#issuecomment-762181198:1397,Testability,log,login,1397,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198
https://github.com/google/deepvariant/issues/404#issuecomment-763239411:936,Availability,error,error,936,"Thanks @ASLeonard . I tried one more with (still with my own versions this time) and added the `-B`:. ```; singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}; mkdir -p intermediate_results_dir; ```; ```; singularity run \; -B ${HOME}/input:/input,${HOME}/reference:/reference,${HOME}/deepvariant1:/deepvariant1,${HOME}/intermediate_results_dir:/intermediate_results_dir \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf /deepvariant1/output.vcf.gz \; --output_gvcf /deepvariant1/output.g.vcf.gz \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /intermediate_results_dir; ```. I tried this with a newer Singularity version, and still can't reproduce your error yet. Here's the machine I'm on:; ```; (deepvariant_whatshap) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.6.4; (deepvariant_whatshap) pichuan@pichuan-cpu:~$ uname -a; Linux pichuan-cpu 4.15.0-1091-gcp #104~16.04.1-Ubuntu SMP Tue Dec 15 19:05:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux; ```. I'll try again later with CentOS as well to see if I can reproduce your issue. If you find out anything else, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763239411
https://github.com/google/deepvariant/issues/404#issuecomment-763490283:639,Availability,checkpoint,checkpoint,639,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell; `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants; ```; cd ouput; ../opt/deepvariant/bin/call_variants \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \; --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \; --checkpoint ""/opt/models/pacbio/model.ckpt"" \; --use_openvino; ```; At this point, we made progress with the following; ```; Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /output/./model.xml; [ SUCCESS ] BIN file: /output/./model.bin; [ SUCCESS ] Total execution time: 30.04 seconds. ; [ SUCCESS ] Memory consumed: 699 MB; ```; With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763490283
https://github.com/google/deepvariant/issues/404#issuecomment-763490283:769,Performance,Optimiz,Optimizer,769,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell; `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants; ```; cd ouput; ../opt/deepvariant/bin/call_variants \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \; --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \; --checkpoint ""/opt/models/pacbio/model.ckpt"" \; --use_openvino; ```; At this point, we made progress with the following; ```; Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /output/./model.xml; [ SUCCESS ] BIN file: /output/./model.bin; [ SUCCESS ] Total execution time: 30.04 seconds. ; [ SUCCESS ] Memory consumed: 699 MB; ```; With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763490283
https://github.com/google/deepvariant/issues/404#issuecomment-763490283:39,Testability,test,tested,39,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell; `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants; ```; cd ouput; ../opt/deepvariant/bin/call_variants \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \; --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \; --checkpoint ""/opt/models/pacbio/model.ckpt"" \; --use_openvino; ```; At this point, we made progress with the following; ```; Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /output/./model.xml; [ SUCCESS ] BIN file: /output/./model.bin; [ SUCCESS ] Total execution time: 30.04 seconds. ; [ SUCCESS ] Memory consumed: 699 MB; ```; With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763490283
https://github.com/google/deepvariant/issues/404#issuecomment-763572811:589,Availability,checkpoint,checkpoint,589,"I found an ungainly solution so I can still batch submit jobs. Using `/bin/bash -c ""<stuff>""`, I can pass two commands to singularity exec. So I can do changing directory and running deepvariant in ""one"" command. The full command that worked was; ```; singularity exec -no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif \; /bin/bash -c ""cd output; ../opt/deepvariant/bin/call_variants \; --outfile /output/intermediate_results_dir/call_variants_output.tfrecord.gz \; --examples /output/intermediate_results_dir/make_examples.tfrecord@24.gz \; --checkpoint /opt/models/pacbio/model.ckpt \; --use_openvino""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763572811
https://github.com/google/deepvariant/issues/404#issuecomment-763821009:173,Deployability,update,updated,173,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763821009
https://github.com/google/deepvariant/issues/404#issuecomment-763821009:301,Deployability,update,update,301,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763821009
https://github.com/google/deepvariant/issues/404#issuecomment-763821009:161,Testability,test,test,161,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404#issuecomment-763821009
https://github.com/google/deepvariant/issues/405#issuecomment-761936653:78,Energy Efficiency,efficient,efficient,78,"Hi @aardes . Yes, this is exactly what GLnexus was designed for, to allow for efficient merging of the N+1 sample in a manner analogous to genomicsDB (the [GLnexus paper](https://www.biorxiv.org/content/10.1101/343970v1) is informative for understanding this. The generation of a gVCF from DeepVariant is independent of its subsequent analysis by GLnexus, so you can incrementally add more sample, and later run GLnexus to merge these gVCFs. Hopefully this answered your question. Please let me know if there is something that remains unclear. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405#issuecomment-761936653
https://github.com/google/deepvariant/issues/408#issuecomment-766309733:192,Availability,error,error,192,"Hi @husamia . The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408#issuecomment-766309733
https://github.com/google/deepvariant/issues/408#issuecomment-766349948:199,Availability,error,error,199,"> Hi @husamia; > ; > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408#issuecomment-766349948
https://github.com/google/deepvariant/issues/408#issuecomment-766349948:354,Integrability,depend,dependent,354,"> Hi @husamia; > ; > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408#issuecomment-766349948
https://github.com/google/deepvariant/issues/408#issuecomment-766349948:312,Performance,perform,performance,312,"> Hi @husamia; > ; > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408#issuecomment-766349948
https://github.com/google/deepvariant/issues/410#issuecomment-766309585:109,Testability,benchmark,benchmarks,109,"Hi @ASLeonard . For the hybrid model, we do not use haplotagged BAMs in the training process. In our initial benchmarks of the hybrid model, we observed a similar accuracy when training with haplotag as compared to training without it. Since haplotagging is an additional step, we decided not to include haplotag in the hybrid model. If you provide a haplotagged BAM to the hybrid model, it will still be able to read the BAM and should result in the same accuracy as when providing an untagged BAM, since the parsing of the haplotags is an additional step which is enabled by a flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410#issuecomment-766309585
https://github.com/google/deepvariant/issues/411#issuecomment-765805463:86,Deployability,patch,patch,86,"The solution in this case was to use the original GRCh38 reference genome, not GRCh38 patch 12.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411#issuecomment-765805463
https://github.com/google/deepvariant/issues/412#issuecomment-767302975:124,Availability,error,error,124,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767302975
https://github.com/google/deepvariant/issues/412#issuecomment-767302975:130,Integrability,message,messages,130,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767302975
https://github.com/google/deepvariant/issues/412#issuecomment-767302975:36,Testability,log,logs,36,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767302975
https://github.com/google/deepvariant/issues/412#issuecomment-767600297:126,Availability,error,error,126,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?. I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767600297
https://github.com/google/deepvariant/issues/412#issuecomment-767600297:132,Integrability,message,messages,132,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?. I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767600297
https://github.com/google/deepvariant/issues/412#issuecomment-767600297:38,Testability,log,logs,38,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?. I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767600297
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:40,Availability,reliab,reliable,40,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:112,Modifiability,layers,layers,112,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:242,Modifiability,layers,layers,242,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:738,Modifiability,layers,layers,738,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:98,Performance,perform,performs,98,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:639,Performance,queue,queue,639,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-767885510:648,Usability,clear,clear,648,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510
https://github.com/google/deepvariant/issues/412#issuecomment-1281200289:62,Deployability,release,released,62,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289
https://github.com/google/deepvariant/issues/412#issuecomment-1281200289:326,Deployability,update,updated,326,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289
https://github.com/google/deepvariant/issues/412#issuecomment-1281200289:378,Energy Efficiency,efficient,efficient,378,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289
https://github.com/google/deepvariant/issues/412#issuecomment-1281200289:524,Energy Efficiency,efficient,efficiently,524,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289
https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:224,Deployability,release,released,224,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484
https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:502,Deployability,update,updated,502,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484
https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:554,Energy Efficiency,efficient,efficient,554,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484
https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:704,Energy Efficiency,efficient,efficiently,704,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484
https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:1031,Integrability,Message,Message,1031,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484
https://github.com/google/deepvariant/issues/413#issuecomment-767954630:33,Availability,error,error,33,"Hi @leorippel . In your log, the error says:; ```; 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; ```; Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-767954630
https://github.com/google/deepvariant/issues/413#issuecomment-767954630:24,Testability,log,log,24,"Hi @leorippel . In your log, the error says:; ```; 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; ```; Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-767954630
https://github.com/google/deepvariant/issues/413#issuecomment-771149411:40,Availability,error,error,40,"> Hi @leorippel; > ; > In your log, the error says:; > ; > ```; > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; > ```; > ; > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument?. Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-771149411
https://github.com/google/deepvariant/issues/413#issuecomment-771149411:31,Testability,log,log,31,"> Hi @leorippel; > ; > In your log, the error says:; > ; > ```; > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; > ```; > ; > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument?. Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-771149411
https://github.com/google/deepvariant/issues/413#issuecomment-771818067:1107,Performance,perform,performing,1107,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```; /input/gvcf.tfrecord-00000-of-00030.gz; /input/gvcf.tfrecord-00001-of-00030.gz; /input/gvcf.tfrecord-00002-of-00030.gz; ...; ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move?. Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-771818067
https://github.com/google/deepvariant/issues/413#issuecomment-1477945211:507,Availability,error,errors,507,"I was able to run `make_examples` multiple times and have `call_variants` use multiple sharded tfrecord files at once. But I guess from this thread that the same is not possible for `postprocess_variants`?. If I run:. ```; postprocess_variants --ref Homo_sapiens.GRCh37.dna.primary_assembly.fa \; --infile call_variants_mysample.tfrecord.gz --outfile mysample.vcf.gz \; --nonvariant_site_tfrecord_path gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord@32.gz \; --gvcf_outfile mysample.gvcf.gz; ```. I get errors like:. ```; gvcf1_parent1.tfrecord@32.gz,gvcf3_parent1.tfrecord-00031-of-00032.gz; No such file or directory; ```. So is the solution to loop through (or run in parallel) my gvcf tfrecord files, output multiple gvcfs, and then concat them with bcftools?. FYI, the `postprocess_variants` help page from docker://google/deepvariant:deeptrio-1.5.0-gpu implies that multiple tfrecord files will be accepted:. ```; --nonvariant_site_tfrecord_path: Optional. Path(s) to the non-variant sites protos in TFRecord format to convert to gVCF file. This should be the complete set of outputs from the --gvcf flag of make_examples.py.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-1477945211
https://github.com/google/deepvariant/issues/413#issuecomment-1480069130:130,Deployability,pipeline,pipeline,130,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130
https://github.com/google/deepvariant/issues/413#issuecomment-1480069130:480,Usability,feedback,feedback,480,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130
https://github.com/google/deepvariant/issues/414#issuecomment-768582500:260,Deployability,release,release,260,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-768582500
https://github.com/google/deepvariant/issues/414#issuecomment-768599474:395,Deployability,integrat,integrated,395,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it.; Adding it to our Docker image should be quite straightforward. I have one more question:; Can you provide a specific example usage you have in mind? ; You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script.; In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-768599474
https://github.com/google/deepvariant/issues/414#issuecomment-768599474:395,Integrability,integrat,integrated,395,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it.; Adding it to our Docker image should be quite straightforward. I have one more question:; Can you provide a specific example usage you have in mind? ; You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script.; In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-768599474
https://github.com/google/deepvariant/issues/414#issuecomment-771075195:43,Deployability,update,update,43,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195
https://github.com/google/deepvariant/issues/414#issuecomment-771075195:78,Deployability,release,release,78,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195
https://github.com/google/deepvariant/issues/414#issuecomment-771075195:137,Deployability,update,update,137,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195
https://github.com/google/deepvariant/issues/414#issuecomment-771075195:175,Usability,feedback,feedback,175,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195
https://github.com/google/deepvariant/issues/415#issuecomment-771074189:1445,Availability,error,error,1445,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well.; * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415#issuecomment-771074189
https://github.com/google/deepvariant/issues/415#issuecomment-771074189:1088,Energy Efficiency,reduce,reduce,1088,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well.; * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415#issuecomment-771074189
https://github.com/google/deepvariant/issues/415#issuecomment-771074189:1186,Performance,perform,performance,1186,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well.; * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415#issuecomment-771074189
https://github.com/google/deepvariant/issues/415#issuecomment-771074189:1374,Performance,perform,perform,1374,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well.; * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/415#issuecomment-771074189
https://github.com/google/deepvariant/issues/416#issuecomment-772180895:275,Deployability,release,release,275,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-772180895
https://github.com/google/deepvariant/issues/416#issuecomment-889450969:58,Testability,test,tests,58,"Did openvino make it into deeptrio-1.2.0? From some quick tests, it doesn't look like it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889450969
https://github.com/google/deepvariant/issues/416#issuecomment-889821634:485,Performance,bottleneck,bottlenecking,485,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634
https://github.com/google/deepvariant/issues/416#issuecomment-889821634:223,Testability,test,tested,223,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634
https://github.com/google/deepvariant/issues/416#issuecomment-889821634:527,Usability,clear,clearer,527,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634
https://github.com/google/deepvariant/issues/418#issuecomment-773782606:224,Performance,perform,perform,224,"@B10inform I'm not sure phasing is necessary for a haploid genome - because there is only one set of chromosomes the expectation is that identified variants are all already in phase. Additionally, DeepVariant is designed to perform germline variant calling in diploid organisms, and requires a reference genome. If the species you are working with does not have a reference genome than as @husamia suggested, this sounds like more of an assembly issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/418#issuecomment-773782606
https://github.com/google/deepvariant/issues/419#issuecomment-774270943:21,Availability,error,error,21,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774270943
https://github.com/google/deepvariant/issues/419#issuecomment-774270943:27,Integrability,message,messages,27,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774270943
https://github.com/google/deepvariant/issues/419#issuecomment-774270943:94,Integrability,wrap,wrapper,94,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774270943
https://github.com/google/deepvariant/issues/419#issuecomment-774270943:53,Testability,log,logs,53,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774270943
https://github.com/google/deepvariant/issues/419#issuecomment-774276776:50,Availability,error,error,50,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0; ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774276776
https://github.com/google/deepvariant/issues/419#issuecomment-774276776:841,Availability,error,error,841,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0; ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774276776
https://github.com/google/deepvariant/issues/419#issuecomment-774276776:847,Integrability,message,message,847,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0; ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774276776
https://github.com/google/deepvariant/issues/419#issuecomment-774282552:33,Availability,error,error,33,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774282552
https://github.com/google/deepvariant/issues/419#issuecomment-774282552:39,Integrability,message,message,39,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774282552
https://github.com/google/deepvariant/issues/419#issuecomment-774282552:57,Testability,test,test,57,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774282552
https://github.com/google/deepvariant/issues/419#issuecomment-774634611:651,Availability,down,downloaded,651,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
https://github.com/google/deepvariant/issues/419#issuecomment-774634611:373,Deployability,Install,Install,373,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
https://github.com/google/deepvariant/issues/419#issuecomment-774634611:460,Deployability,install,installation,460,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
https://github.com/google/deepvariant/issues/419#issuecomment-774634611:478,Deployability,install,installation-on-linux,478,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
https://github.com/google/deepvariant/issues/419#issuecomment-774634611:437,Usability,guid,guides,437,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
https://github.com/google/deepvariant/issues/419#issuecomment-774634611:454,Usability,guid,guide,454,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611
https://github.com/google/deepvariant/issues/419#issuecomment-774693583:12,Availability,error,error,12,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash; $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py; $; ```; Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774693583
https://github.com/google/deepvariant/issues/419#issuecomment-774693583:21,Usability,simpl,simpler,21,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash; $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py; $; ```; Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774693583
https://github.com/google/deepvariant/issues/419#issuecomment-774782391:173,Availability,error,error,173,> @williamrowell Can you check whether your CPU supports AVX instruction?. This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash; wrowell@mp0608-sge:~$ lscpu | grep Flags; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774782391
https://github.com/google/deepvariant/issues/419#issuecomment-774782391:498,Energy Efficiency,monitor,monitor,498,> @williamrowell Can you check whether your CPU supports AVX instruction?. This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash; wrowell@mp0608-sge:~$ lscpu | grep Flags; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774782391
https://github.com/google/deepvariant/issues/419#issuecomment-774782391:179,Integrability,message,message,179,> @williamrowell Can you check whether your CPU supports AVX instruction?. This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash; wrowell@mp0608-sge:~$ lscpu | grep Flags; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419#issuecomment-774782391
https://github.com/google/deepvariant/issues/422#issuecomment-775615394:245,Deployability,update,update,245,"Hello,; The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory?; We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422#issuecomment-775615394
https://github.com/google/deepvariant/issues/423#issuecomment-777731334:140,Deployability,release,release,140,"Thanks for bringing this up Brendan! I have added the white background to the diagram internally, so it will be fixed on GitHub in the next release. If this is annoying to more people, I could cherry-pick it to fix it sooner. Leave the pair-of-eyes emoji here to vote for us to prioritize this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/423#issuecomment-777731334
https://github.com/google/deepvariant/pull/424#issuecomment-779362572:898,Security,authoriz,authorized,898,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F424) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424#issuecomment-779362572
https://github.com/google/deepvariant/pull/424#issuecomment-779362572:966,Security,authoriz,authorized,966,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F424) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424#issuecomment-779362572
https://github.com/google/deepvariant/pull/424#issuecomment-779362572:1239,Security,authoriz,authorized,1239,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F424) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424#issuecomment-779362572
https://github.com/google/deepvariant/pull/424#issuecomment-779362572:1534,Security,authoriz,authorized,1534,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F424) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424#issuecomment-779362572
https://github.com/google/deepvariant/pull/424#issuecomment-780944002:172,Deployability,release,release,172,"Hi @dridk, thank you for the pull request! Unfortunately, we cannot merge pull requests through GitHub. I can fix this internally, and the change should go out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424#issuecomment-780944002
https://github.com/google/deepvariant/pull/424#issuecomment-780948386:201,Availability,error,error,201,"Hi @dridk, my apologies, I didn't notice the branch for this pull request. We are no longer maintaining the r0.5 branch. However, thank you for the suggestion. We will keep an eye out for this kind of error in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/424#issuecomment-780948386
https://github.com/google/deepvariant/issues/425#issuecomment-782444906:105,Availability,down,downsamples,105,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425#issuecomment-782444906
https://github.com/google/deepvariant/issues/425#issuecomment-782444906:468,Integrability,depend,depending,468,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/425#issuecomment-782444906
https://github.com/google/deepvariant/issues/426#issuecomment-782298000:0,Testability,test,test,0,test 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426#issuecomment-782298000
https://github.com/google/deepvariant/issues/426#issuecomment-782440933:0,Testability,Test,Test,0,Test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426#issuecomment-782440933
https://github.com/google/deepvariant/issues/426#issuecomment-782471153:8,Testability,test,test,8,another test by not-Pi-Chuan,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426#issuecomment-782471153
https://github.com/google/deepvariant/issues/427#issuecomment-788314979:142,Availability,error,error,142,"Hi @kostasgalexiou , can you provide more information like:; - What type of machines are you working on (how many cores, how much RAM); - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-788314979
https://github.com/google/deepvariant/issues/427#issuecomment-788314979:148,Integrability,message,messages,148,"Hi @kostasgalexiou , can you provide more information like:; - What type of machines are you working on (how many cores, how much RAM); - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-788314979
https://github.com/google/deepvariant/issues/427#issuecomment-788690917:84,Availability,error,error,84,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:. ├── [drwxrwxr-x 4.0K] input; │ └── [drwxrwxr-x 4.0K] data; │ ├── [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa; │ ├── [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai; │ ├── [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam; │ └── [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai; └── [drwxrwxr-x 4.0K] output; └── [drwxr-xr-x 4.0K] intermediate_results_dir; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-00016",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-788690917
https://github.com/google/deepvariant/issues/427#issuecomment-788690917:144,Availability,error,errors,144,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:. ├── [drwxrwxr-x 4.0K] input; │ └── [drwxrwxr-x 4.0K] data; │ ├── [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa; │ ├── [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai; │ ├── [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam; │ └── [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai; └── [drwxrwxr-x 4.0K] output; └── [drwxr-xr-x 4.0K] intermediate_results_dir; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-00016",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-788690917
https://github.com/google/deepvariant/issues/427#issuecomment-788690917:90,Integrability,message,messages,90,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:. ├── [drwxrwxr-x 4.0K] input; │ └── [drwxrwxr-x 4.0K] data; │ ├── [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa; │ ├── [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai; │ ├── [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam; │ └── [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai; └── [drwxrwxr-x 4.0K] output; └── [drwxr-xr-x 4.0K] intermediate_results_dir; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-00016",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-788690917
https://github.com/google/deepvariant/issues/427#issuecomment-788690917:186,Testability,log,logs,186,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:. ├── [drwxrwxr-x 4.0K] input; │ └── [drwxrwxr-x 4.0K] data; │ ├── [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa; │ ├── [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai; │ ├── [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam; │ └── [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai; └── [drwxrwxr-x 4.0K] output; └── [drwxr-xr-x 4.0K] intermediate_results_dir; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-00016",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-788690917
https://github.com/google/deepvariant/issues/427#issuecomment-789047987:30,Testability,log,log,30,"Hi @kostasgalexiou , from the log, does it look like all 16 make_examples have finished? (At the end it should have something like:; ```; Task 1/16: Created ... examples; ```; If that's hard to see, can you check how many of the make_examples are still running?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789047987
https://github.com/google/deepvariant/issues/427#issuecomment-789089400:104,Deployability,update,update,104,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants?. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz; -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz; -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz; -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz; -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz; -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz; -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz; -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz; -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz; -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz; -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz; -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz; -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz; -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz; -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789089400
https://github.com/google/deepvariant/issues/427#issuecomment-789089400:47,Testability,log,log,47,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants?. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz; -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz; -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz; -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz; -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz; -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz; -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz; -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz; -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz; -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz; -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz; -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz; -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz; -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz; -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789089400
https://github.com/google/deepvariant/issues/427#issuecomment-789115542:292,Availability,error,error,292,"Hi @kostasgalexiou ,; Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not.; You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789115542
https://github.com/google/deepvariant/issues/427#issuecomment-789115542:395,Testability,log,logs,395,"Hi @kostasgalexiou ,; Looking at the file size, they look reasonable, but I can't tell exactly whether they're complete or not.; You can certainly try directly to run call_variants. If some of the records are not complete or corrupted, call_variants or postprocess_variants might give you an error later on. I'm still curious on why it didn't finish though. If you have more new observations or logs that might be informative, please let me know. I suppose your input BAM file is not public? If it is public, I would love to try to reproduce your issue on a n1-standard-16 machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789115542
https://github.com/google/deepvariant/issues/427#issuecomment-789122414:66,Availability,down,download,66,"Hi @kalexiou ,; if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789122414
https://github.com/google/deepvariant/issues/427#issuecomment-789122414:81,Testability,test,test,81,"Hi @kalexiou ,; if the file is not public, unfortunately I cannot download it or test it. Thanks for offering though. If you can share a bit more about what type of file this is (sequencing instrument, coverage, PCR-free or PCR-plus, etc) that might make this file unique, maybe I can try to see if I can reproduce the issue on another public file. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-789122414
https://github.com/google/deepvariant/issues/427#issuecomment-790376285:460,Availability,checkpoint,checkpoint,460,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:; ```; docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help; ```; to look at all relevant flags. Your command should be probably something like:; ```; ...; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt""; ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-790376285
https://github.com/google/deepvariant/issues/427#issuecomment-790376285:510,Availability,checkpoint,checkpoint,510,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:; ```; docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help; ```; to look at all relevant flags. Your command should be probably something like:; ```; ...; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt""; ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-790376285
https://github.com/google/deepvariant/issues/427#issuecomment-790376285:558,Integrability,depend,depending,558,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:; ```; docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help; ```; to look at all relevant flags. Your command should be probably something like:; ```; ...; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt""; ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-790376285
https://github.com/google/deepvariant/issues/427#issuecomment-793438811:490,Availability,error,error,490,"Hi @kalexiou ,; I think what might have happened is that the make_examples stage ran out of memory.; I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:; (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM.; (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-793438811
https://github.com/google/deepvariant/issues/427#issuecomment-793438811:496,Integrability,message,messages,496,"Hi @kalexiou ,; I think what might have happened is that the make_examples stage ran out of memory.; I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:; (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM.; (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-793438811
https://github.com/google/deepvariant/issues/427#issuecomment-793492563:29,Deployability,update,update,29,"Hi @pichuan,. Thanks for the update! I will try your suggestions next time I come up with a ""heavy"" bam file. Kostas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/427#issuecomment-793492563
https://github.com/google/deepvariant/issues/429#issuecomment-796937010:178,Deployability,release,release,178,"Hi @jkalleberg thanks raising this issue. You're right that this actually should be optional, this seems to be bug - I've noted it in internally and we'll have a fix in the next release. For now, I would suggest just specifying some path for the output_gvcf flags, even if you don't actually need them. Also, would you be able to share why you don't want to use the gVCF file? Normally people will want to merge these callsets, which would require using the gVCFs, so I'd be curious to know if there's a specific use case where you are not needing gVCF output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429#issuecomment-796937010
https://github.com/google/deepvariant/issues/429#issuecomment-797059169:15,Deployability,update,update,15,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429#issuecomment-797059169
https://github.com/google/deepvariant/issues/429#issuecomment-797059169:341,Deployability,pipeline,pipeline,341,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429#issuecomment-797059169
https://github.com/google/deepvariant/issues/430#issuecomment-796847786:123,Deployability,upgrade,upgrade,123,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430#issuecomment-796847786
https://github.com/google/deepvariant/issues/430#issuecomment-797712308:202,Testability,test,test,202,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430#issuecomment-797712308
https://github.com/google/deepvariant/issues/431#issuecomment-804422288:428,Testability,test,test,428,"It looks like examples were created for all the samples but call_variants did not run for the child. ; Could you please check the sizes of all files with extension ```tfrecord``` for all samples? To make sure that parents' files are not empty.; Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally.; Also, could you try to run the test on Ubuntu OS?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431#issuecomment-804422288
https://github.com/google/deepvariant/issues/431#issuecomment-805135046:19701,Testability,test,test,19701,"17:55 make_examples_parent2.tfrecord-00026-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00027-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00028-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00029-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00030-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00031-of-00052.gz; 1.2G Mar 9 17:56 make_examples_parent2.tfrecord-00032-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00033-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00034-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00035-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00036-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00037-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00038-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00039-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00040-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00041-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00042-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00043-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00044-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00045-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00046-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00047-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00048-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00049-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00050-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00051-of-00052.gz. > Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. I don't have cloud storage for thi. > Also, could you try to run the test on Ubuntu OS?. I can't run it on Ubuntu OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431#issuecomment-805135046
https://github.com/google/deepvariant/issues/431#issuecomment-807106323:154,Deployability,pipeline,pipeline,154,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:; ```; /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa; ```. To generate a g.VCF:; ```; /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz; ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431#issuecomment-807106323
https://github.com/google/deepvariant/issues/431#issuecomment-808287669:156,Deployability,pipeline,pipeline,156,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually.; > ; > To generate a VCF:; > ; > ```; > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa; > ```; > ; > To generate a g.VCF:; > ; > ```; > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz; > ```; > ; > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431#issuecomment-808287669
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:140,Availability,down,down,140,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine ; ```; gcloud compute instances create ""${USER}-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:1334,Availability,error,errors,1334,"; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another pointer for you:; In our Dockerfile, we set th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:1717,Availability,checkpoint,checkpoint,1717,"ithub.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another pointer for you:; In our Dockerfile, we set these environment variables:; https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:2048,Availability,checkpoint,checkpoint,2048,"ithub.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another pointer for you:; In our Dockerfile, we set these environment variables:; https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:483,Deployability,update,update,483,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine ; ```; gcloud compute instances create ""${USER}-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:505,Deployability,install,install,505,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine ; ```; gcloud compute instances create ""${USER}-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:2316,Modifiability,variab,variables,2316,"ithub.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another pointer for you:; In our Dockerfile, we set these environment variables:; https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:2542,Modifiability,variab,variables,2542,"ithub.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another pointer for you:; In our Dockerfile, we set these environment variables:; https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:239,Testability,test,test,239,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine ; ```; gcloud compute instances create ""${USER}-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-806341687:1249,Testability,log,log,1249,"pes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687
https://github.com/google/deepvariant/issues/432#issuecomment-810913512:69,Deployability,install,installation,69,"Thanks a lot @pichuan, I am working on this - apparently my OpenVino installation wasn't good. Your answer was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432#issuecomment-810913512
https://github.com/google/deepvariant/issues/433#issuecomment-807046936:167,Deployability,release,released,167,"Hi @maryawood , thanks for the question.; As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-807046936
https://github.com/google/deepvariant/issues/433#issuecomment-807115854:211,Availability,avail,available,211,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-807115854
https://github.com/google/deepvariant/issues/433#issuecomment-807115854:232,Performance,perform,performance,232,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-807115854
https://github.com/google/deepvariant/issues/433#issuecomment-807187618:170,Availability,error,error,170,"`unspecified_caller` is an invalid option. I've never actually specified it, but I think the behavior should be that it should crash earlier (hopefully with a meaningful error message). . To train your own sequencing-type specific model, generally if the sequencer's base error rate is not too high, the default `very_sensitive_caller` should just work. But, if it's tricker cases like ONT reads, having a separate candidate generation process which is smarter about proposing candidates, and then using `vcf_candidate_importer` is the right way to go. @maryawood Can you say a bit more about why the default `vcf_candidate_importer` doesn't work for you? Do you get very imbalanced training data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-807187618
https://github.com/google/deepvariant/issues/433#issuecomment-807187618:272,Availability,error,error,272,"`unspecified_caller` is an invalid option. I've never actually specified it, but I think the behavior should be that it should crash earlier (hopefully with a meaningful error message). . To train your own sequencing-type specific model, generally if the sequencer's base error rate is not too high, the default `very_sensitive_caller` should just work. But, if it's tricker cases like ONT reads, having a separate candidate generation process which is smarter about proposing candidates, and then using `vcf_candidate_importer` is the right way to go. @maryawood Can you say a bit more about why the default `vcf_candidate_importer` doesn't work for you? Do you get very imbalanced training data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-807187618
https://github.com/google/deepvariant/issues/433#issuecomment-807187618:176,Integrability,message,message,176,"`unspecified_caller` is an invalid option. I've never actually specified it, but I think the behavior should be that it should crash earlier (hopefully with a meaningful error message). . To train your own sequencing-type specific model, generally if the sequencer's base error rate is not too high, the default `very_sensitive_caller` should just work. But, if it's tricker cases like ONT reads, having a separate candidate generation process which is smarter about proposing candidates, and then using `vcf_candidate_importer` is the right way to go. @maryawood Can you say a bit more about why the default `vcf_candidate_importer` doesn't work for you? Do you get very imbalanced training data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-807187618
https://github.com/google/deepvariant/issues/433#issuecomment-810405206:378,Availability,avail,available,378,"Hi @pichuan, sorry for the slow response! I've only tried `very_sensitive_caller`, and while it seems to be working alright, I'm getting lower-than-desired precision and quite low recall, even when adding additional training data or adjusting the evaluation metrics. This may be fixable by tuning parameters more, so I'm just trying to get a handle on all the different options available to customize the training process!. One other question - I see that including a VCF with population-level allele frequencies is an option. Is there a specific format that the frequencies need to be in for compatibility with DeepVariant? (e.g. do they need to be in a specific INFO field?) Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/433#issuecomment-810405206
https://github.com/google/deepvariant/issues/434#issuecomment-808822540:362,Availability,error,error,362,"Hi @elcortegano . There are a few potential causes for this. First, you may want to extract this particular read and compare the number of bases in it against the number of base quality values (these will need to match). samtools view ${BAM} | grep m64036_210113_122249\/147655225\/ccs. should retrieve this particular read. One other thing which can cause this error is if the BAM file is truncated (for example if a download ran out of space). Could you check both of these (that the file isn't truncated, and that the read doesn't have an obvious problem) and provide a bit more information?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-808822540
https://github.com/google/deepvariant/issues/434#issuecomment-808822540:418,Availability,down,download,418,"Hi @elcortegano . There are a few potential causes for this. First, you may want to extract this particular read and compare the number of bases in it against the number of base quality values (these will need to match). samtools view ${BAM} | grep m64036_210113_122249\/147655225\/ccs. should retrieve this particular read. One other thing which can cause this error is if the BAM file is truncated (for example if a download ran out of space). Could you check both of these (that the file isn't truncated, and that the read doesn't have an obvious problem) and provide a bit more information?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-808822540
https://github.com/google/deepvariant/issues/434#issuecomment-813523906:135,Availability,error,error,135,"Hi Andrew,. I am not sure how to provide the BAM file you request, if I run:. samtools view -bh <file.bam> <read> > file_read.bam . An error appears indicating that the read specifies an invalid region or unknown reference. Could you please provide a more detailed command?. Thank you; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-813523906
https://github.com/google/deepvariant/issues/434#issuecomment-814530426:81,Testability,test,test,81,"Hi @elcortegano . Interesting, that is a lot larger than I expected. But I ran a test and got a similar result with a PacBio BAM. . Can you attach just that single read here as a BAM file (using the grep command). I can likely run a test from that. If that fails, the best way might be to upload to a Google Drive object and share permissions with me for the 300MB file. Thank you for your patience,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-814530426
https://github.com/google/deepvariant/issues/434#issuecomment-814530426:233,Testability,test,test,233,"Hi @elcortegano . Interesting, that is a lot larger than I expected. But I ran a test and got a similar result with a PacBio BAM. . Can you attach just that single read here as a BAM file (using the grep command). I can likely run a test from that. If that fails, the best way might be to upload to a Google Drive object and share permissions with me for the 300MB file. Thank you for your patience,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-814530426
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:699,Availability,error,error,699,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1890,Availability,error,error,1890,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1584,Deployability,release,releases,1584,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1393,Integrability,wrap,wraps,1393,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:932,Performance,perform,perform,932,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1272,Performance,optimiz,optimized,1272,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1424,Performance,optimiz,optimized,1424,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572
https://github.com/google/deepvariant/issues/434#issuecomment-815783472:139,Availability,error,error,139,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
https://github.com/google/deepvariant/issues/434#issuecomment-815783472:145,Integrability,message,message,145,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
https://github.com/google/deepvariant/issues/434#issuecomment-815783472:556,Safety,safe,safely,556,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
https://github.com/google/deepvariant/issues/434#issuecomment-815783472:39,Usability,feedback,feedback,39,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
https://github.com/google/deepvariant/issues/434#issuecomment-815783472:78,Usability,learn,learnt,78,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472
https://github.com/google/deepvariant/issues/434#issuecomment-815972148:1006,Availability,down,downstream,1006,"Hello Eugenio,. - I agree that it looks like the secondary alignments are causing problems, since neither the actual read sequence nor the base qualities are stored in the BAM records for secondary alignments. When the parser hits these rows, it seems to be having trouble parsing the `*` base quality string as individual values.; - The `minimap2` parameters `-x map-pb` and `-x asm` refer to alignment parameters, and don't make any assumptions on what types of reads are being aligned (e.g. reads-to-reference vs assembly-to-reference).; - The PacBio model for DeepVariant has been trained on reads-to-reference alignments with pbmm2.; - I would highly recommend aligning your HiFi reads for DeepVariant with [pbmm2](https://github.com/PacificBiosciences/pbmm2). In addition to the alignment presets (which I discuss in the next point), we have some post-alignment filters that are applied. It's also just easier to use with PacBio data. To align a human HiFi uBAM (`hifi_reads.bam`) to a reference for downstream variant calling, I use: `pbmm2 align --num-threads 24 --preset HIFI --sort -c 0 -y 70 --sample <sample_name> reference.fasta hifi_reads.bam aligned.bam`. This produces an aligned, sorted BAM, with all of the fields and tags necessary to be processed by DeepVariant. If you don't have a local SMRTLink installation, you can install pbmm2 with `conda install -c bioconda pbmm2`; - The `pbmm2 --preset HIFI` parameters are _roughly_ equivalent to these minimap parameters: `-x map-pb -a --eqx -L -O 5,56 -E 4,1 -B 5 --secondary=no -z 400,50 -r 2k -Y`. Notice that we don't use homopolymer compressed minimizers (`-H`). Billy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815972148
https://github.com/google/deepvariant/issues/434#issuecomment-815972148:1318,Deployability,install,installation,1318,"Hello Eugenio,. - I agree that it looks like the secondary alignments are causing problems, since neither the actual read sequence nor the base qualities are stored in the BAM records for secondary alignments. When the parser hits these rows, it seems to be having trouble parsing the `*` base quality string as individual values.; - The `minimap2` parameters `-x map-pb` and `-x asm` refer to alignment parameters, and don't make any assumptions on what types of reads are being aligned (e.g. reads-to-reference vs assembly-to-reference).; - The PacBio model for DeepVariant has been trained on reads-to-reference alignments with pbmm2.; - I would highly recommend aligning your HiFi reads for DeepVariant with [pbmm2](https://github.com/PacificBiosciences/pbmm2). In addition to the alignment presets (which I discuss in the next point), we have some post-alignment filters that are applied. It's also just easier to use with PacBio data. To align a human HiFi uBAM (`hifi_reads.bam`) to a reference for downstream variant calling, I use: `pbmm2 align --num-threads 24 --preset HIFI --sort -c 0 -y 70 --sample <sample_name> reference.fasta hifi_reads.bam aligned.bam`. This produces an aligned, sorted BAM, with all of the fields and tags necessary to be processed by DeepVariant. If you don't have a local SMRTLink installation, you can install pbmm2 with `conda install -c bioconda pbmm2`; - The `pbmm2 --preset HIFI` parameters are _roughly_ equivalent to these minimap parameters: `-x map-pb -a --eqx -L -O 5,56 -E 4,1 -B 5 --secondary=no -z 400,50 -r 2k -Y`. Notice that we don't use homopolymer compressed minimizers (`-H`). Billy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815972148
https://github.com/google/deepvariant/issues/434#issuecomment-815972148:1340,Deployability,install,install,1340,"Hello Eugenio,. - I agree that it looks like the secondary alignments are causing problems, since neither the actual read sequence nor the base qualities are stored in the BAM records for secondary alignments. When the parser hits these rows, it seems to be having trouble parsing the `*` base quality string as individual values.; - The `minimap2` parameters `-x map-pb` and `-x asm` refer to alignment parameters, and don't make any assumptions on what types of reads are being aligned (e.g. reads-to-reference vs assembly-to-reference).; - The PacBio model for DeepVariant has been trained on reads-to-reference alignments with pbmm2.; - I would highly recommend aligning your HiFi reads for DeepVariant with [pbmm2](https://github.com/PacificBiosciences/pbmm2). In addition to the alignment presets (which I discuss in the next point), we have some post-alignment filters that are applied. It's also just easier to use with PacBio data. To align a human HiFi uBAM (`hifi_reads.bam`) to a reference for downstream variant calling, I use: `pbmm2 align --num-threads 24 --preset HIFI --sort -c 0 -y 70 --sample <sample_name> reference.fasta hifi_reads.bam aligned.bam`. This produces an aligned, sorted BAM, with all of the fields and tags necessary to be processed by DeepVariant. If you don't have a local SMRTLink installation, you can install pbmm2 with `conda install -c bioconda pbmm2`; - The `pbmm2 --preset HIFI` parameters are _roughly_ equivalent to these minimap parameters: `-x map-pb -a --eqx -L -O 5,56 -E 4,1 -B 5 --secondary=no -z 400,50 -r 2k -Y`. Notice that we don't use homopolymer compressed minimizers (`-H`). Billy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815972148
https://github.com/google/deepvariant/issues/434#issuecomment-815972148:1366,Deployability,install,install,1366,"Hello Eugenio,. - I agree that it looks like the secondary alignments are causing problems, since neither the actual read sequence nor the base qualities are stored in the BAM records for secondary alignments. When the parser hits these rows, it seems to be having trouble parsing the `*` base quality string as individual values.; - The `minimap2` parameters `-x map-pb` and `-x asm` refer to alignment parameters, and don't make any assumptions on what types of reads are being aligned (e.g. reads-to-reference vs assembly-to-reference).; - The PacBio model for DeepVariant has been trained on reads-to-reference alignments with pbmm2.; - I would highly recommend aligning your HiFi reads for DeepVariant with [pbmm2](https://github.com/PacificBiosciences/pbmm2). In addition to the alignment presets (which I discuss in the next point), we have some post-alignment filters that are applied. It's also just easier to use with PacBio data. To align a human HiFi uBAM (`hifi_reads.bam`) to a reference for downstream variant calling, I use: `pbmm2 align --num-threads 24 --preset HIFI --sort -c 0 -y 70 --sample <sample_name> reference.fasta hifi_reads.bam aligned.bam`. This produces an aligned, sorted BAM, with all of the fields and tags necessary to be processed by DeepVariant. If you don't have a local SMRTLink installation, you can install pbmm2 with `conda install -c bioconda pbmm2`; - The `pbmm2 --preset HIFI` parameters are _roughly_ equivalent to these minimap parameters: `-x map-pb -a --eqx -L -O 5,56 -E 4,1 -B 5 --secondary=no -z 400,50 -r 2k -Y`. Notice that we don't use homopolymer compressed minimizers (`-H`). Billy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815972148
https://github.com/google/deepvariant/issues/434#issuecomment-815982608:646,Performance,tune,tuned,646,"Hi @elcortegano . Billy has provided some excellent, detailed answers. I'll just add a little bit of context to your question . ""Our understanding ```deepvariant``` is designed for read alignments (and not assembly-to-reference alignments as achieved with ```minimap2 -ax asm```"". This is correct, DeepVariant is for read alignments not assembly-to-reference alignment. Because minimap2 has original written when most PacBio data was CLR, the choice of the word ```map-pb``` was chosen for this present. When CCS became more common, a new recommendation was made by minimap2 authors to use the parameters for assembly with an sequence divergence tuned to HiFi data (this is the 20 part of ```--asm20```). If you search for the string ``ccs``` in the minimap2 github, this should pull up the line. So in short, DeepVariant is for read alignments, and the parameter for read mapping HiFi reads happens to manifest in minimap2 as ```minimap2 -ax asm20```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-815982608
https://github.com/google/deepvariant/issues/434#issuecomment-816108659:102,Usability,clear,clear,102,"I see! I am very grateful for the support. Not only the problem is solved, it is everything much more clear now, and I have learned a lot from your feedback. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-816108659
https://github.com/google/deepvariant/issues/434#issuecomment-816108659:124,Usability,learn,learned,124,"I see! I am very grateful for the support. Not only the problem is solved, it is everything much more clear now, and I have learned a lot from your feedback. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-816108659
https://github.com/google/deepvariant/issues/434#issuecomment-816108659:148,Usability,feedback,feedback,148,"I see! I am very grateful for the support. Not only the problem is solved, it is everything much more clear now, and I have learned a lot from your feedback. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434#issuecomment-816108659
https://github.com/google/deepvariant/issues/435#issuecomment-809953490:117,Testability,log,log,117,@nilesh-iiita I can't immediately spot anything wrong with your command. I'd recommend looking at the `make_examples.log` file in the output directory to make sure that step completed successfully and candidates were generated. You can also set `--intermediate_results_dir=/output` to see if the `make_examples.tfrecord@8.gz` files are being created.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-809953490
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:293,Availability,error,errors,293,"Thanks for the quick reply:; For me, make_examples.tfrecord@8.gz file is not generated and the following is the `make_examples.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:503,Availability,error,errors,503,"Thanks for the quick reply:; For me, make_examples.tfrecord@8.gz file is not generated and the following is the `make_examples.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:713,Availability,error,errors,713,"Thanks for the quick reply:; For me, make_examples.tfrecord@8.gz file is not generated and the following is the `make_examples.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:923,Availability,error,errors,923,"Thanks for the quick reply:; For me, make_examples.tfrecord@8.gz file is not generated and the following is the `make_examples.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:1133,Availability,error,errors,1133,s.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /out,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:1343,Availability,error,errors,1343,must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --task 2. ```; I think the bam file is not corrected - would you please let me know which is the best suitable tool for aligning sequencing reads?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:1553,Availability,error,errors,1553,must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --task 2. ```; I think the bam file is not corrected - would you please let me know which is the best suitable tool for aligning sequencing reads?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:1763,Availability,error,errors,1763,must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --task 2. ```; I think the bam file is not corrected - would you please let me know which is the best suitable tool for aligning sequencing reads?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:127,Testability,log,log,127,"Thanks for the quick reply:; For me, make_examples.tfrecord@8.gz file is not generated and the following is the `make_examples.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810383024:2049,Testability,log,logs,2049,must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --task 2. ```; I think the bam file is not corrected - would you please let me know which is the best suitable tool for aligning sequencing reads?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024
https://github.com/google/deepvariant/issues/435#issuecomment-810423494:361,Availability,avail,available,361,"@nilesh-iiita to address this issue, you should not need to realign the reads. Instead, you can pass the `--sample_name=<NAME>` to the `run_deepvariant` command. `<NAME>` can be set to any desired value. It seems like your BAM file is missing a sample name, and when this happens, `make_examples` fails. Adding this flag will ensure that there is a sample name available for `make_examples` to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810423494
https://github.com/google/deepvariant/issues/435#issuecomment-810516515:363,Availability,avail,available,363,"> @nilesh-iiita to address this issue, you should not need to realign the reads. Instead, you can pass the `--sample_name=<NAME>` to the `run_deepvariant` command. `<NAME>` can be set to any desired value. It seems like your BAM file is missing a sample name, and when this happens, `make_examples` fails. Adding this flag will ensure that there is a sample name available for `make_examples` to use. Thanks! It worked very well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435#issuecomment-810516515
https://github.com/google/deepvariant/issues/436#issuecomment-814505837:129,Availability,down,download,129,Thanks @nikostr for reporting this.; The zip file for 1.1.0 should be avaiable at https://github.com/google/deepvariant/releases/download/v1.1.0/deepvariant.zip now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436#issuecomment-814505837
https://github.com/google/deepvariant/issues/436#issuecomment-814505837:120,Deployability,release,releases,120,Thanks @nikostr for reporting this.; The zip file for 1.1.0 should be avaiable at https://github.com/google/deepvariant/releases/download/v1.1.0/deepvariant.zip now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436#issuecomment-814505837
https://github.com/google/deepvariant/issues/437#issuecomment-816788129:476,Testability,test,test,476,"The parameter (see the flags at the top of make_examples.py) is ""realign_reads"", and the way to set it to false is to pass `--norealign_reads`. As stated in the usage you quoted above, reads longer than 500 bp are never realigned (this was added in v1.0). We still add `--norealign_reads` to PacBio runs, but if you forget to add it, there's likely not a big difference since very few pacbio reads will be below 500 bp. If you want to try a shorter run before the full one to test your setup, you can always run DeepVariant all the way through with a small region, like `--regions ""chr20:10,000,000-10,010,000""`, which should only take a few minutes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/437#issuecomment-816788129
https://github.com/google/deepvariant/issues/438#issuecomment-819098517:141,Testability,log,logic,141,"Hi @gyuhee0928 ,. By default, `make_examples` only creates pileup images for candidates that are created by our default candidate generation logic. After the examples are created, you can then visualize these examples by using `show_examples`: https://github.com/google/deepvariant/blob/r1.1/docs/show-examples.md. We do have a more advanced optional flag you can specify to use `vcf_candidate_importer` instead. It is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for --proposed_variants, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. Our team might be a bit slower to answer specific questions on vcf_candidate_importer, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/438#issuecomment-819098517
https://github.com/google/deepvariant/issues/440#issuecomment-820157988:758,Usability,learn,learned,758,"Hi @shadrinams . From your description, it looks like you are running the DeepTrio merge component in the way that I would recommend. . Can I ask a few questions:. 1. Are you taking the values from the QUAL field of the multisample glnexus VCF; 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. Your QUAL values for all calls match expectations for DeepTrio. However, I would not expect that ""true"" variants would have a distribution which departs from all calls. I can think of one reason that this might be the case:. Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. . The other things it might be good to look at is the genotype quality (GQ) field. This is the most direct measure of DeepTrio's confidence in a call, and maps directly from the probability for the called class. The QUAL value of a multisample VCF comes from GLnexus, and is a bit less direct measure of call confidence (still, I don't expect the distributions to be off in the manner that you see). I don't think I can immediately answer this puzzle, but hopefully with a little more information we can figure it out. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820157988
https://github.com/google/deepvariant/issues/440#issuecomment-820564569:1024,Deployability,pipeline,pipeline,1024,"drew,. Thank you for your quick reply!. > 1. Are you taking the values from the QUAL field of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
https://github.com/google/deepvariant/issues/440#issuecomment-820564569:1135,Deployability,pipeline,pipeline,1135,"d of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is there a way to filter my variants from DeepTrio further?. Thank you!. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
https://github.com/google/deepvariant/issues/440#issuecomment-820564569:1894,Testability,test,test,1894,"d of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is there a way to filter my variants from DeepTrio further?. Thank you!. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
https://github.com/google/deepvariant/issues/440#issuecomment-820564569:661,Usability,learn,learned,661,"Hello Andrew,. Thank you for your quick reply!. > 1. Are you taking the values from the QUAL field of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569
https://github.com/google/deepvariant/issues/440#issuecomment-821020591:476,Performance,optimiz,optimizing,476,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
https://github.com/google/deepvariant/issues/440#issuecomment-821020591:715,Safety,predict,predictive,715,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
https://github.com/google/deepvariant/issues/440#issuecomment-821020591:132,Usability,learn,learned,132,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
https://github.com/google/deepvariant/issues/440#issuecomment-821020591:686,Usability,learn,learned,686,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591
https://github.com/google/deepvariant/issues/440#issuecomment-822947862:1936,Modifiability,inherit,inherited,1936,"squished and expanded views) with corresponding output in DeepTrio VCF to show you discrepancies between DeepTrio VCFs and BAMs. I think it could be a part of the GQ issue in DeNovos.; The order of FORMAT fields in multisample VCF is proband, mother, father.; The order of samples in IGV is father, mother, proband (from top to bottom). ### **1) True Denovo, QUAL=3, proband GQ=5. It looks good except very low proband GQ.**. chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:**5**:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:**27,1,0**:18:0,18,45,990,990,990:II . chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | AATATAT | 46 | . | AF=0.166667;AQ=15 | GT:DP:AD:GQ:PL:RNC | 0/1:30:5,13:13:44,15,53:.. | 0/0:31:16,0:46:46,990,990:.. | ./.:30:**27,0**:18:0,990,990:II. ![DT_1_04190_chr5_24093912](https://user-images.githubusercontent.com/22089494/115330437-09e80680-a162-11eb-96cd-2f2d27d45896.png); ![DT_1_04190_chr5_24093912_zoom](https://user-images.githubusercontent.com/22089494/115330450-11a7ab00-a162-11eb-8f5c-927bd1445056.png). ### **3) Filtered Denovo-like, QUAL=27, proband GQ=28. Inherited; it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr7 | 54624683 | chr7_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862
https://github.com/google/deepvariant/issues/440#issuecomment-822947862:2870,Modifiability,Inherit,Inherited,2870,".. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:**27,1,0**:18:0,18,45,990,990,990:II . chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | AATATAT | 46 | . | AF=0.166667;AQ=15 | GT:DP:AD:GQ:PL:RNC | 0/1:30:5,13:13:44,15,53:.. | 0/0:31:16,0:46:46,990,990:.. | ./.:30:**27,0**:18:0,990,990:II. ![DT_1_04190_chr5_24093912](https://user-images.githubusercontent.com/22089494/115330437-09e80680-a162-11eb-96cd-2f2d27d45896.png); ![DT_1_04190_chr5_24093912_zoom](https://user-images.githubusercontent.com/22089494/115330450-11a7ab00-a162-11eb-8f5c-927bd1445056.png). ### **3) Filtered Denovo-like, QUAL=27, proband GQ=28. Inherited; it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:**28,0**:50:0,90,899:.. ![DT_1_04190_chr7_54624683](https://user-images.githubusercontent.com/22089494/115331518-0ce3f680-a164-11eb-91e1-2250266a421c.png). ![DT_1_04190_chr7_54624683_zoom](https://user-images.githubusercontent.com/22089494/115331520-0eadba00-a164-11eb-9c8d-2eeb8e63e1dc.png). What do you think could be a reason for DeepTrio calling to miss some information from read allignments?; Thank you. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862
https://github.com/google/deepvariant/issues/440#issuecomment-822947862:131,Usability,learn,learned,131,"Hello Andrew,. >Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. I have tried to verify the idea and plotted normalized histograms for all DeepTrio calls, filtered DeNovo-like (0/1-0/0-0/0) calls and true DeNovo calls (verified in IGV). Please see an attached picture. True DeNovo calls are still in the middle-low part of proband GQ range for DeNovo-like calls. I have tried to look at several DeNovo-like variants with a higher proband GQ, which were not in my list of true DeNovos, but all of them were false-positives. ![DeepTrio_GQproband2](https://user-images.githubusercontent.com/22089494/115329837-ea041300-a160-11eb-8788-31136171e157.png). I have prepared a few examples of DeepTrio variants in IGV (squished and expanded views) with corresponding output in DeepTrio VCF to show you discrepancies between DeepTrio VCFs and BAMs. I think it could be a part of the GQ issue in DeNovos.; The order of FORMAT fields in multisample VCF is proband, mother, father.; The order of samples in IGV is father, mother, proband (from top to bottom). ### **1) True Denovo, QUAL=3, proband GQ=5. It looks good except very low proband GQ.**. chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:**5**:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as fat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862
https://github.com/google/deepvariant/issues/440#issuecomment-823860823:1240,Availability,down,downstream,1240,"hether some harmonization or filtering after calls could make the resulting calls more uniform. With respect to your examples. It is often difficult to be definitive about why DeepVariant does not make certain calls. I cannot give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio doesn't match the view in the BAM. DeepTrio is saying that both parents don't have evidence for variant reads at this position. It is possible the after r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823
https://github.com/google/deepvariant/issues/440#issuecomment-823860823:1397,Availability,down,downstream,1397,"t give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio doesn't match the view in the BAM. DeepTrio is saying that both parents don't have evidence for variant reads at this position. It is possible the after re-alignment (which DeepTrio does) there isn't evidence for a variant. However, the child is still called and has similar evidence. Is it possible for you to share snippets of this BAM file (e.g. a ~1000 bp window around thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823
https://github.com/google/deepvariant/issues/440#issuecomment-823860823:758,Usability,clear,clear,758,"Hi @shadrinams . Your observations about some apparent de novos originating from undercalling of a parent is interesting. I think that we'll look into whether some harmonization or filtering after calls could make the resulting calls more uniform. With respect to your examples. It is often difficult to be definitive about why DeepVariant does not make certain calls. I cannot give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823
https://github.com/google/deepvariant/issues/440#issuecomment-823860823:783,Usability,learn,learned,783,"Hi @shadrinams . Your observations about some apparent de novos originating from undercalling of a parent is interesting. I think that we'll look into whether some harmonization or filtering after calls could make the resulting calls more uniform. With respect to your examples. It is often difficult to be definitive about why DeepVariant does not make certain calls. I cannot give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823
https://github.com/google/deepvariant/issues/440#issuecomment-824200851:326,Availability,down,downstream,326,"Hello Andrew,. Thank you for your reply. . I have prepared snippets of the BAM files for the trio with variant chr7:54624683. It would be interesting to see what you find!. > One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. It is an interesting thought! But even if there is an explanation for missing HET T, the AATATAT insertion allele is also missed in the father genotype. And for some reason it is shown as ./. Best regards,; Maria. [chr7_54624683.tar.gz](https://github.com/google/deepvariant/files/6352555/chr7_54624683.tar.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-824200851
https://github.com/google/deepvariant/issues/440#issuecomment-824200851:483,Availability,down,downstream,483,"Hello Andrew,. Thank you for your reply. . I have prepared snippets of the BAM files for the trio with variant chr7:54624683. It would be interesting to see what you find!. > One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. It is an interesting thought! But even if there is an explanation for missing HET T, the AATATAT insertion allele is also missed in the father genotype. And for some reason it is shown as ./. Best regards,; Maria. [chr7_54624683.tar.gz](https://github.com/google/deepvariant/files/6352555/chr7_54624683.tar.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-824200851
https://github.com/google/deepvariant/issues/440#issuecomment-836280023:1351,Deployability,install,install,1351,"Hi @shadrinams . My apologies for the delay in reply. It took me awhile to get to this issue. I have some thoughts about how to improve consistency in the trio calling that may help some of these cases. But it may take a bit of time to explore those. For the chr7_54624683 case, I suspect there is some interaction with the variant call and the normalization done by GLnexus. When I look at the VCF for the Proband and Father, I see:. Proband; ```; chr7 54624686 . A ATC 61.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:61:39:17,21:0.538462:61,0,7; ```; Father; ```; chr7 54624686 . A ATC 54.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:55:33:18,15:0.454545:54,0,68; ```. In my GLnexus VCF, I also see the following line, which has a call in both the child and the father (the variant position is a bit different, but this does seem to reflect the reads I see in the sample). ; ```; chr7 54624686 chr7_54624686_A_ATC A ATC 61 . AF=0.333333;AQ=61 GT:DP:AD:GQ:PL:RNC 0/1:33:18,15:55:54,0,68:.. 0/0:32:32,0:50:0,180,1799:.. 0/1:39:17,21:61:61,0,76:..; ```. Would you mind pasting the command that you used to merge the gVCFs from this sample? When you jointly genotyped, were there also other samples at the time of joint genotyping, or was it only this sample?. For reference, this is the command that I used on the gVCFs of the files to generate this gVCF:. ```; sudo apt-get -y install bcftools; sudo apt-get -y install tabix; sudo docker pull quay.io/mlin/glnexus:v1.2.7. sudo docker run \; -v ""${PWD}/outputs"":""/outputs"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWGS \; /outputs/Child_chr7_54624683.g.vcf.gz \; /outputs/Father_chr7_54624683.g.vcf.gz \; /outputs/Mother_chr7_54624683.g.vcf.gz \; | bcftools view - | bgzip -c > outputs/GitHub440.cohort.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-836280023
https://github.com/google/deepvariant/issues/440#issuecomment-836280023:1385,Deployability,install,install,1385,"Hi @shadrinams . My apologies for the delay in reply. It took me awhile to get to this issue. I have some thoughts about how to improve consistency in the trio calling that may help some of these cases. But it may take a bit of time to explore those. For the chr7_54624683 case, I suspect there is some interaction with the variant call and the normalization done by GLnexus. When I look at the VCF for the Proband and Father, I see:. Proband; ```; chr7 54624686 . A ATC 61.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:61:39:17,21:0.538462:61,0,7; ```; Father; ```; chr7 54624686 . A ATC 54.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:55:33:18,15:0.454545:54,0,68; ```. In my GLnexus VCF, I also see the following line, which has a call in both the child and the father (the variant position is a bit different, but this does seem to reflect the reads I see in the sample). ; ```; chr7 54624686 chr7_54624686_A_ATC A ATC 61 . AF=0.333333;AQ=61 GT:DP:AD:GQ:PL:RNC 0/1:33:18,15:55:54,0,68:.. 0/0:32:32,0:50:0,180,1799:.. 0/1:39:17,21:61:61,0,76:..; ```. Would you mind pasting the command that you used to merge the gVCFs from this sample? When you jointly genotyped, were there also other samples at the time of joint genotyping, or was it only this sample?. For reference, this is the command that I used on the gVCFs of the files to generate this gVCF:. ```; sudo apt-get -y install bcftools; sudo apt-get -y install tabix; sudo docker pull quay.io/mlin/glnexus:v1.2.7. sudo docker run \; -v ""${PWD}/outputs"":""/outputs"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWGS \; /outputs/Child_chr7_54624683.g.vcf.gz \; /outputs/Father_chr7_54624683.g.vcf.gz \; /outputs/Mother_chr7_54624683.g.vcf.gz \; | bcftools view - | bgzip -c > outputs/GitHub440.cohort.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-836280023
https://github.com/google/deepvariant/issues/440#issuecomment-836280023:1562,Modifiability,config,config,1562,"Hi @shadrinams . My apologies for the delay in reply. It took me awhile to get to this issue. I have some thoughts about how to improve consistency in the trio calling that may help some of these cases. But it may take a bit of time to explore those. For the chr7_54624683 case, I suspect there is some interaction with the variant call and the normalization done by GLnexus. When I look at the VCF for the Proband and Father, I see:. Proband; ```; chr7 54624686 . A ATC 61.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:61:39:17,21:0.538462:61,0,7; ```; Father; ```; chr7 54624686 . A ATC 54.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:55:33:18,15:0.454545:54,0,68; ```. In my GLnexus VCF, I also see the following line, which has a call in both the child and the father (the variant position is a bit different, but this does seem to reflect the reads I see in the sample). ; ```; chr7 54624686 chr7_54624686_A_ATC A ATC 61 . AF=0.333333;AQ=61 GT:DP:AD:GQ:PL:RNC 0/1:33:18,15:55:54,0,68:.. 0/0:32:32,0:50:0,180,1799:.. 0/1:39:17,21:61:61,0,76:..; ```. Would you mind pasting the command that you used to merge the gVCFs from this sample? When you jointly genotyped, were there also other samples at the time of joint genotyping, or was it only this sample?. For reference, this is the command that I used on the gVCFs of the files to generate this gVCF:. ```; sudo apt-get -y install bcftools; sudo apt-get -y install tabix; sudo docker pull quay.io/mlin/glnexus:v1.2.7. sudo docker run \; -v ""${PWD}/outputs"":""/outputs"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWGS \; /outputs/Child_chr7_54624683.g.vcf.gz \; /outputs/Father_chr7_54624683.g.vcf.gz \; /outputs/Mother_chr7_54624683.g.vcf.gz \; | bcftools view - | bgzip -c > outputs/GitHub440.cohort.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-836280023
https://github.com/google/deepvariant/issues/440#issuecomment-839088398:1535,Modifiability,config,config,1535,"Hello Andrew,. Thank you for the information! It is interesting!. chr7:54624686 A-ATC and chr7:54624683 A-AATC are different variants. I see both in my output. But still my output for chr7:54624686 A-ATC is not the same. It was not called in my proband. The one in my father looks similar but QUAL and GQ are quite different. I think it comes from the DeepTrio calling step. How can our outputs be so different? ; I use deepvariant_deeptrio-1.1.0.sif. For any case, I show all three 54624683/54624686 variants here:; My proband:; `chr7 54624683 . A AATC 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:39:22,16:0.410256:27,0,48`; `chr7 54624686 . A C 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:38:16,22:0.578947:27,0,53`; `chr7 54624686 . A ATC - no call`. My father:; `chr7 54624683 . A AATC - no call`; `chr7 54624686 . A C - no call`; `chr7 54624686 . A ATC 26.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:26:33:18,15:0.454545:26,0,44`. My GLnexus VCF:; `chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:28:28,0:50:0,90,899:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A C 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:38:16,22:28:27,0,53:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:33:18,0:26:26,990,990:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A ATC 27 . AF=0.166667;AQ=26 GT:DP:AD:GQ:PL:RNC 0/0:38:16,0:28:27,990,990:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:33:18,15:26:26,0,44:..`. I use the following command for glnexus. I had to switch to --config DeepVariant_unfiltered as --config DeepVariantWGS filtered out most of my DeNovo calls as they tend to have QUAL of <20. `module load glnexus/1.2.7`; `glnexus_cli ; --config DeepVariant_unfiltered ; --threads $(nproc) ; $FATHER.g.vcf.gz ; $MOTHER.g.vcf.gz ; $CHILD.g.vcf.gz ; 	| bcftools view - | bgzip -c > ${FAMILY}.deeptrio.vcf.gz`. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-839088398
https://github.com/google/deepvariant/issues/440#issuecomment-839088398:1570,Modifiability,config,config,1570,"Hello Andrew,. Thank you for the information! It is interesting!. chr7:54624686 A-ATC and chr7:54624683 A-AATC are different variants. I see both in my output. But still my output for chr7:54624686 A-ATC is not the same. It was not called in my proband. The one in my father looks similar but QUAL and GQ are quite different. I think it comes from the DeepTrio calling step. How can our outputs be so different? ; I use deepvariant_deeptrio-1.1.0.sif. For any case, I show all three 54624683/54624686 variants here:; My proband:; `chr7 54624683 . A AATC 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:39:22,16:0.410256:27,0,48`; `chr7 54624686 . A C 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:38:16,22:0.578947:27,0,53`; `chr7 54624686 . A ATC - no call`. My father:; `chr7 54624683 . A AATC - no call`; `chr7 54624686 . A C - no call`; `chr7 54624686 . A ATC 26.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:26:33:18,15:0.454545:26,0,44`. My GLnexus VCF:; `chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:28:28,0:50:0,90,899:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A C 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:38:16,22:28:27,0,53:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:33:18,0:26:26,990,990:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A ATC 27 . AF=0.166667;AQ=26 GT:DP:AD:GQ:PL:RNC 0/0:38:16,0:28:27,990,990:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:33:18,15:26:26,0,44:..`. I use the following command for glnexus. I had to switch to --config DeepVariant_unfiltered as --config DeepVariantWGS filtered out most of my DeNovo calls as they tend to have QUAL of <20. `module load glnexus/1.2.7`; `glnexus_cli ; --config DeepVariant_unfiltered ; --threads $(nproc) ; $FATHER.g.vcf.gz ; $MOTHER.g.vcf.gz ; $CHILD.g.vcf.gz ; 	| bcftools view - | bgzip -c > ${FAMILY}.deeptrio.vcf.gz`. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-839088398
https://github.com/google/deepvariant/issues/440#issuecomment-839088398:1709,Modifiability,config,config,1709,"Hello Andrew,. Thank you for the information! It is interesting!. chr7:54624686 A-ATC and chr7:54624683 A-AATC are different variants. I see both in my output. But still my output for chr7:54624686 A-ATC is not the same. It was not called in my proband. The one in my father looks similar but QUAL and GQ are quite different. I think it comes from the DeepTrio calling step. How can our outputs be so different? ; I use deepvariant_deeptrio-1.1.0.sif. For any case, I show all three 54624683/54624686 variants here:; My proband:; `chr7 54624683 . A AATC 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:39:22,16:0.410256:27,0,48`; `chr7 54624686 . A C 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:38:16,22:0.578947:27,0,53`; `chr7 54624686 . A ATC - no call`. My father:; `chr7 54624683 . A AATC - no call`; `chr7 54624686 . A C - no call`; `chr7 54624686 . A ATC 26.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:26:33:18,15:0.454545:26,0,44`. My GLnexus VCF:; `chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:28:28,0:50:0,90,899:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A C 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:38:16,22:28:27,0,53:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:33:18,0:26:26,990,990:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A ATC 27 . AF=0.166667;AQ=26 GT:DP:AD:GQ:PL:RNC 0/0:38:16,0:28:27,990,990:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:33:18,15:26:26,0,44:..`. I use the following command for glnexus. I had to switch to --config DeepVariant_unfiltered as --config DeepVariantWGS filtered out most of my DeNovo calls as they tend to have QUAL of <20. `module load glnexus/1.2.7`; `glnexus_cli ; --config DeepVariant_unfiltered ; --threads $(nproc) ; $FATHER.g.vcf.gz ; $MOTHER.g.vcf.gz ; $CHILD.g.vcf.gz ; 	| bcftools view - | bgzip -c > ${FAMILY}.deeptrio.vcf.gz`. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-839088398
https://github.com/google/deepvariant/issues/440#issuecomment-839088398:1671,Performance,load,load,1671,"Hello Andrew,. Thank you for the information! It is interesting!. chr7:54624686 A-ATC and chr7:54624683 A-AATC are different variants. I see both in my output. But still my output for chr7:54624686 A-ATC is not the same. It was not called in my proband. The one in my father looks similar but QUAL and GQ are quite different. I think it comes from the DeepTrio calling step. How can our outputs be so different? ; I use deepvariant_deeptrio-1.1.0.sif. For any case, I show all three 54624683/54624686 variants here:; My proband:; `chr7 54624683 . A AATC 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:39:22,16:0.410256:27,0,48`; `chr7 54624686 . A C 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:38:16,22:0.578947:27,0,53`; `chr7 54624686 . A ATC - no call`. My father:; `chr7 54624683 . A AATC - no call`; `chr7 54624686 . A C - no call`; `chr7 54624686 . A ATC 26.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:26:33:18,15:0.454545:26,0,44`. My GLnexus VCF:; `chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:28:28,0:50:0,90,899:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A C 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:38:16,22:28:27,0,53:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:33:18,0:26:26,990,990:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A ATC 27 . AF=0.166667;AQ=26 GT:DP:AD:GQ:PL:RNC 0/0:38:16,0:28:27,990,990:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:33:18,15:26:26,0,44:..`. I use the following command for glnexus. I had to switch to --config DeepVariant_unfiltered as --config DeepVariantWGS filtered out most of my DeNovo calls as they tend to have QUAL of <20. `module load glnexus/1.2.7`; `glnexus_cli ; --config DeepVariant_unfiltered ; --threads $(nproc) ; $FATHER.g.vcf.gz ; $MOTHER.g.vcf.gz ; $CHILD.g.vcf.gz ; 	| bcftools view - | bgzip -c > ${FAMILY}.deeptrio.vcf.gz`. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-839088398
https://github.com/google/deepvariant/issues/440#issuecomment-899266001:32,Deployability,update,updates,32,"Hi, I wondered if there are any updates on this issue. Were you able to separate the true positives from false positives?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899266001
https://github.com/google/deepvariant/issues/440#issuecomment-899967692:158,Deployability,update,updated,158,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692
https://github.com/google/deepvariant/issues/440#issuecomment-899967692:509,Energy Efficiency,reduce,reduces,509,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692
https://github.com/google/deepvariant/issues/440#issuecomment-899967692:563,Usability,clear,clearly,563,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692
https://github.com/google/deepvariant/issues/441#issuecomment-820768080:36,Deployability,update,updated,36,"Hi @yanyang1989 ,; internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. We're still using Python 3.6 though. Can you tell us why you're considering using 3.8?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-820768080
https://github.com/google/deepvariant/issues/441#issuecomment-820768080:79,Deployability,update,updates,79,"Hi @yanyang1989 ,; internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. We're still using Python 3.6 though. Can you tell us why you're considering using 3.8?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-820768080
https://github.com/google/deepvariant/issues/441#issuecomment-820768080:133,Deployability,release,release,133,"Hi @yanyang1989 ,; internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. We're still using Python 3.6 though. Can you tell us why you're considering using 3.8?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-820768080
https://github.com/google/deepvariant/issues/441#issuecomment-821002845:167,Security,secur,security,167,"Python 3.6 is meant to be ""unsupported"" by the end of this year ([pep 494](https://www.python.org/dev/peps/pep-0494/)), so would be decent future-proofing or ensuring security.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821002845
https://github.com/google/deepvariant/issues/441#issuecomment-821249453:31,Deployability,integrat,integrate,31,"Hi @pichuan , I am planning to integrate deepvariant into a whole germline variant calling pipeline which is using python3.8 and built everything into a Docker. So I need to build every prereq relating deepvariant inside it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821249453
https://github.com/google/deepvariant/issues/441#issuecomment-821249453:91,Deployability,pipeline,pipeline,91,"Hi @pichuan , I am planning to integrate deepvariant into a whole germline variant calling pipeline which is using python3.8 and built everything into a Docker. So I need to build every prereq relating deepvariant inside it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821249453
https://github.com/google/deepvariant/issues/441#issuecomment-821249453:31,Integrability,integrat,integrate,31,"Hi @pichuan , I am planning to integrate deepvariant into a whole germline variant calling pipeline which is using python3.8 and built everything into a Docker. So I need to build every prereq relating deepvariant inside it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821249453
https://github.com/google/deepvariant/issues/441#issuecomment-821311465:34,Availability,error,error,34,"I have been experiencing the same error when trying to build DeepVariant from the Dockerfile they provided but I managed to pull working images from Docker by following the steps below:; ```BASH; BIN_VERSION=""1.1.0-gpu""; docker image pull google/deepvariant:$BIN_VERSION; ```. Pulling the image from remote is not ideal for us as I would prefer to use the Dockerfile to increase transparency in how the image is created. However, having a working DeepVariant is better than nothing. We are using Ubuntu 16.04 but we would greatly prefer to run Ubuntu 18.04 as our OS for running DeepVariant. Does DeepVariant 1.1.0 now support Ubuntu 18.04 hosts?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821311465
https://github.com/google/deepvariant/issues/441#issuecomment-821524931:94,Deployability,update,update,94,"Thanks @ASLeonard for the pointer on the timeline for Python 3.6 and 3.8. I'll take a look to update the Python version.; @yanyang1989 thanks for sharing your use case! We'll look into updating to Python 3.8 this but it will take a while. If you can try pulling our image directly like @aliceseaborn , that might work for you now?. @aliceseaborn My experience is that `google/deepvariant:1.1.0` and `google/deepvariant:1.1.0-gpu` work with no issues directly on Ubuntu 18.04 machines. Are you having any issues when you pull our images?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821524931
https://github.com/google/deepvariant/issues/441#issuecomment-821553096:28,Availability,avail,available,28,"is it possible to make this available https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-20.latest.tgz. Since I have another problem about ""Install CLIF binary"" using ubuntu 20.04. Quote from build-prereq.sh script:. # Figure out which linux installation we are on to fetch an appropriate # version of the pre-built CLIF binary. Note that we only support now Ubuntu # 14, 16, and 18.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821553096
https://github.com/google/deepvariant/issues/441#issuecomment-821553096:170,Deployability,Install,Install,170,"is it possible to make this available https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-20.latest.tgz. Since I have another problem about ""Install CLIF binary"" using ubuntu 20.04. Quote from build-prereq.sh script:. # Figure out which linux installation we are on to fetch an appropriate # version of the pre-built CLIF binary. Note that we only support now Ubuntu # 14, 16, and 18.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821553096
https://github.com/google/deepvariant/issues/441#issuecomment-821553096:272,Deployability,install,installation,272,"is it possible to make this available https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-20.latest.tgz. Since I have another problem about ""Install CLIF binary"" using ubuntu 20.04. Quote from build-prereq.sh script:. # Figure out which linux installation we are on to fetch an appropriate # version of the pre-built CLIF binary. Note that we only support now Ubuntu # 14, 16, and 18.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821553096
https://github.com/google/deepvariant/issues/441#issuecomment-821556321:25,Deployability,release,release,25,"@yanyang1989 in our next release (which we have internally updated already), we'll be building clif from scratch. However, in our next release we will only support Ubuntu18. For any other versions, please directly refer to https://github.com/google/clif to build clif binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821556321
https://github.com/google/deepvariant/issues/441#issuecomment-821556321:59,Deployability,update,updated,59,"@yanyang1989 in our next release (which we have internally updated already), we'll be building clif from scratch. However, in our next release we will only support Ubuntu18. For any other versions, please directly refer to https://github.com/google/clif to build clif binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821556321
https://github.com/google/deepvariant/issues/441#issuecomment-821556321:135,Deployability,release,release,135,"@yanyang1989 in our next release (which we have internally updated already), we'll be building clif from scratch. However, in our next release we will only support Ubuntu18. For any other versions, please directly refer to https://github.com/google/clif to build clif binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821556321
https://github.com/google/deepvariant/issues/441#issuecomment-821755610:94,Deployability,update,update,94,"Please see https://github.com/google/deepvariant/issues/443#issuecomment-821755400 for how to update to Ubuntu 18.04.; I haven't tried updating to Python 3.8 yet. But I'll create an internal issue to track it.; Closing this issue now, but feel free to add more questions/comments if related to this one, or feel free to open another issue for any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-821755610
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:20,Availability,down,downloaded,20,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:414,Availability,error,error,414,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:460,Availability,error,errors,460,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:495,Availability,failure,failure,495,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:739,Availability,error,errors,739,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:774,Availability,failure,failure,774,"@pichuan Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1017,Availability,error,errors,1017,"Hi! When I downloaded all the pre-built binaries and run the run-prereq.sh scripts. How I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1052,Availability,failure,failure,1052," I suppose to run the make_example parallel?; I use the following command: ; seq 0 47 | parallel -q --halt 2 --line-buffer python3 make_examples.zip --mode calling --ref /data/input/human_g1k_v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1296,Availability,error,errors,1296,"v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1331,Availability,failure,failure,1331," make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1574,Availability,error,errors,1574,"e present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1609,Availability,failure,failure,1609,"variant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1853,Availability,error,errors,1853,"e present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1888,Availability,failure,failure,1888,"variant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2132,Availability,error,errors,2132," present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2167,Availability,failure,failure,2167,"ariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2411,Availability,error,errors,2411," present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2446,Availability,failure,failure,2446,"ariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2690,Availability,error,errors,2690,"present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2725,Availability,failure,failure,2725,"riant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2969,Availability,error,errors,2969,"present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3004,Availability,failure,failure,3004,"riant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3248,Availability,error,errors,3248,"present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3283,Availability,failure,failure,3283,"riant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3527,Availability,error,errors,3527,"present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3562,Availability,failure,failure,3562,"riant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3806,Availability,error,errors,3806,"present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3841,Availability,failure,failure,3841,"riant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4085,Availability,error,errors,4085,"present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4120,Availability,failure,failure,4120,"riant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4364,Availability,error,errors,4364,"present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4399,Availability,failure,failure,4399,"riant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4643,Availability,error,errors,4643,"present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4678,Availability,failure,failure,4678,"riant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4922,Availability,error,errors,4922,"present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4957,Availability,failure,failure,4957,"riant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5201,Availability,error,errors,5201,"present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5236,Availability,failure,failure,5236,"riant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5479,Availability,error,errors,5479," present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5514,Availability,failure,failure,5514,"ariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5758,Availability,error,errors,5758," present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5793,Availability,failure,failure,5793,"ariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6037,Availability,error,errors,6037," present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6072,Availability,failure,failure,6072,"ariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6316,Availability,error,errors,6316," present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6351,Availability,failure,failure,6351,"ariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6595,Availability,error,errors,6595,"present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6630,Availability,failure,failure,6630,"riant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6873,Availability,error,errors,6873," present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6908,Availability,failure,failure,6908,"ariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7152,Availability,error,errors,7152," present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7187,Availability,failure,failure,7187,"ariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7430,Availability,error,errors,7430,"e present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7465,Availability,failure,failure,7465,"variant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7709,Availability,error,errors,7709,"e present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7744,Availability,failure,failure,7744,"variant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7988,Availability,error,errors,7988," present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8023,Availability,failure,failure,8023,"ariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8267,Availability,error,errors,8267," present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8302,Availability,failure,failure,8302,"ariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8545,Availability,error,errors,8545," present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8580,Availability,failure,failure,8580,"ariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8824,Availability,error,errors,8824," present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:8859,Availability,failure,failure,8859,"ariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9103,Availability,error,errors,9103," present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9138,Availability,failure,failure,9138,"ariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9382,Availability,error,errors,9382," present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9417,Availability,failure,failure,9417,"ariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9660,Availability,error,errors,9660," present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9695,Availability,failure,failure,9695,"ariant/deepvariant/make_examples.py', '13']"".; E0430 18:57:45.303056 140048653686592 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9939,Availability,error,errors,9939," present on the command line: ""['/tmp/Bazel.runfiles_yk1hqf6a/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:9974,Availability,failure,failure,9974,"ariant/deepvariant/make_examples.py', '39']"".; E0430 18:57:45.368143 139769855092544 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10217,Availability,error,errors,10217,"e present on the command line: ""['/tmp/Bazel.runfiles_3ci75isq/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10252,Availability,failure,failure,10252,"variant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10496,Availability,error,errors,10496,"e present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10531,Availability,failure,failure,10531,"variant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10775,Availability,error,errors,10775," present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10810,Availability,failure,failure,10810,"ariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11054,Availability,error,errors,11054," present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11089,Availability,failure,failure,11089,"ariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11333,Availability,error,errors,11333,"present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11368,Availability,failure,failure,11368,"riant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11612,Availability,error,errors,11612,"present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11647,Availability,failure,failure,11647,"riant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11891,Availability,error,errors,11891,"present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11926,Availability,failure,failure,11926,"riant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12170,Availability,error,errors,12170,"present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12205,Availability,failure,failure,12205,"riant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12449,Availability,error,errors,12449,"present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12484,Availability,failure,failure,12484,"riant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12727,Availability,error,errors,12727," present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12762,Availability,failure,failure,12762,"ariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13006,Availability,error,errors,13006,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13041,Availability,failure,failure,13041,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13285,Availability,error,errors,13285,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13320,Availability,failure,failure,13320,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13564,Availability,error,errors,13564,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13599,Availability,failure,failure,13599,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13854,Usability,guid,guidance,13854,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850
https://github.com/google/deepvariant/pull/442#issuecomment-821760836:242,Testability,log,log,242,"Thanks @dkurt ! ; I'll take a look next week and make sure I can incorporate the changes internally.; Can you confirm again that you're ok with us doing that? (I'll add a pointer to this PR when I make the internal change, so that the commit log will have the information. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-821760836
https://github.com/google/deepvariant/pull/442#issuecomment-821781022:184,Deployability,upgrade,upgrade,184,"@pichuan, Sure, I don't mind if you just copy the changes. Yes, this way is valid because OpenVINO from pip uses manylinux toolchain so it's compatible with Ubuntu 16.04 and allows to upgrade to newer versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-821781022
https://github.com/google/deepvariant/pull/442#issuecomment-823513740:76,Deployability,release,release,76,@dkurt FYI: This is now the merged internally and will come out in the next release.; Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-823513740
https://github.com/google/deepvariant/pull/442#issuecomment-850907219:40,Deployability,update,updates,40,"@dkurt Question for you: I'm working an updates to update to Ubuntu20.04, TF2.5, Python3.8, etc.; Want to check in with you on whether we need to update any OpenVINO version before the next release.; Let me know! If it's easier through email, please email me at pichuan@google.com. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-850907219
https://github.com/google/deepvariant/pull/442#issuecomment-850907219:51,Deployability,update,update,51,"@dkurt Question for you: I'm working an updates to update to Ubuntu20.04, TF2.5, Python3.8, etc.; Want to check in with you on whether we need to update any OpenVINO version before the next release.; Let me know! If it's easier through email, please email me at pichuan@google.com. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-850907219
https://github.com/google/deepvariant/pull/442#issuecomment-850907219:146,Deployability,update,update,146,"@dkurt Question for you: I'm working an updates to update to Ubuntu20.04, TF2.5, Python3.8, etc.; Want to check in with you on whether we need to update any OpenVINO version before the next release.; Let me know! If it's easier through email, please email me at pichuan@google.com. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-850907219
https://github.com/google/deepvariant/pull/442#issuecomment-850907219:190,Deployability,release,release,190,"@dkurt Question for you: I'm working an updates to update to Ubuntu20.04, TF2.5, Python3.8, etc.; Want to check in with you on whether we need to update any OpenVINO version before the next release.; Let me know! If it's easier through email, please email me at pichuan@google.com. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-850907219
https://github.com/google/deepvariant/pull/442#issuecomment-851332367:68,Deployability,release,released,68,"Hi, @pichuan! OpenVINO 2021.3 is the latest version. 2021.4 will be released only at least in next month so it's OK to use 2021.3. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442#issuecomment-851332367
https://github.com/google/deepvariant/issues/443#issuecomment-821522061:32,Deployability,update,updated,32,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061
https://github.com/google/deepvariant/issues/443#issuecomment-821522061:75,Deployability,update,updates,75,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061
https://github.com/google/deepvariant/issues/443#issuecomment-821522061:129,Deployability,release,release,129,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061
https://github.com/google/deepvariant/issues/443#issuecomment-821522061:294,Deployability,update,updated,294,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061
https://github.com/google/deepvariant/issues/443#issuecomment-821522061:427,Deployability,release,release,427,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061
https://github.com/google/deepvariant/issues/443#issuecomment-821522061:322,Testability,test,test,322,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061
https://github.com/google/deepvariant/issues/443#issuecomment-821755400:16,Deployability,Update,Update,16,"Hi @mkazanov ,. Update:; First, I want to remind you that:; * Our next release (r1.2) will be on Ubuntu 18.04. ; * And, even right now (with r1.1), you should be able to run our `google/deepvariant:1.1.0` on Ubuntu18.04. Just in case you really need to build your own on a Ubuntu18.04 now, I put together some changes that we already made internally to update to Ubuntu 18.04 here:; https://github.com/pichuan/deepvariant/tree/r1.1-ubuntu18; (**Note**: This is my own fork. In the future I might delete it. But at least before r1.2 comes out, I'll keep it there.). If you want to understand what changes have been made, you can see this:; https://github.com/google/deepvariant/compare/r1.1...pichuan:r1.1-ubuntu18",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821755400
https://github.com/google/deepvariant/issues/443#issuecomment-821755400:71,Deployability,release,release,71,"Hi @mkazanov ,. Update:; First, I want to remind you that:; * Our next release (r1.2) will be on Ubuntu 18.04. ; * And, even right now (with r1.1), you should be able to run our `google/deepvariant:1.1.0` on Ubuntu18.04. Just in case you really need to build your own on a Ubuntu18.04 now, I put together some changes that we already made internally to update to Ubuntu 18.04 here:; https://github.com/pichuan/deepvariant/tree/r1.1-ubuntu18; (**Note**: This is my own fork. In the future I might delete it. But at least before r1.2 comes out, I'll keep it there.). If you want to understand what changes have been made, you can see this:; https://github.com/google/deepvariant/compare/r1.1...pichuan:r1.1-ubuntu18",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821755400
https://github.com/google/deepvariant/issues/443#issuecomment-821755400:353,Deployability,update,update,353,"Hi @mkazanov ,. Update:; First, I want to remind you that:; * Our next release (r1.2) will be on Ubuntu 18.04. ; * And, even right now (with r1.1), you should be able to run our `google/deepvariant:1.1.0` on Ubuntu18.04. Just in case you really need to build your own on a Ubuntu18.04 now, I put together some changes that we already made internally to update to Ubuntu 18.04 here:; https://github.com/pichuan/deepvariant/tree/r1.1-ubuntu18; (**Note**: This is my own fork. In the future I might delete it. But at least before r1.2 comes out, I'll keep it there.). If you want to understand what changes have been made, you can see this:; https://github.com/google/deepvariant/compare/r1.1...pichuan:r1.1-ubuntu18",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443#issuecomment-821755400
https://github.com/google/deepvariant/issues/444#issuecomment-821754409:275,Availability,error,error,275,"Hi @woodoo46 ; Can you give me more information like:; Which OS you're using, what singularity version, etc. And, are there more logs before the first line?; ```; I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; ```. It'll help if I can try reproducing the error first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/444#issuecomment-821754409
https://github.com/google/deepvariant/issues/444#issuecomment-821754409:129,Testability,log,logs,129,"Hi @woodoo46 ; Can you give me more information like:; Which OS you're using, what singularity version, etc. And, are there more logs before the first line?; ```; I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; ```. It'll help if I can try reproducing the error first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/444#issuecomment-821754409
https://github.com/google/deepvariant/issues/445#issuecomment-822571186:117,Deployability,install,installing,117,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186
https://github.com/google/deepvariant/issues/445#issuecomment-822571186:155,Deployability,install,install,155,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186
https://github.com/google/deepvariant/issues/445#issuecomment-822571186:212,Deployability,install,install,212,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186
https://github.com/google/deepvariant/issues/445#issuecomment-822571186:65,Usability,clear,clear,65,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186
https://github.com/google/deepvariant/issues/445#issuecomment-822613451:572,Availability,ERROR,ERROR,572,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
https://github.com/google/deepvariant/issues/445#issuecomment-822613451:352,Modifiability,sandbox,sandbox,352,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
https://github.com/google/deepvariant/issues/445#issuecomment-822613451:450,Modifiability,sandbox,sandbox,450,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
https://github.com/google/deepvariant/issues/445#issuecomment-822613451:352,Testability,sandbox,sandbox,352,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
https://github.com/google/deepvariant/issues/445#issuecomment-822613451:450,Testability,sandbox,sandbox,450,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
https://github.com/google/deepvariant/issues/445#issuecomment-822613451:807,Usability,simpl,simply,807,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451
https://github.com/google/deepvariant/issues/445#issuecomment-822620636:243,Deployability,install,install,243,@brentp what version of singularity are you using? I have had issues with older versions of Singularity in the past and was able to get things to work by using 3+. I can experiment with this for a bit and try to put together a Dockerfile that install Samtools and Bcftools and runs on singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822620636
https://github.com/google/deepvariant/issues/445#issuecomment-822637152:14,Usability,simpl,simply,14,"Yes, it works simply updating singularity. Thanks for figuring this out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445#issuecomment-822637152
https://github.com/google/deepvariant/issues/446#issuecomment-826519447:203,Availability,error,error,203,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447
https://github.com/google/deepvariant/issues/446#issuecomment-826519447:209,Integrability,message,messages,209,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447
https://github.com/google/deepvariant/issues/446#issuecomment-826519447:247,Usability,clear,clear,247,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447
https://github.com/google/deepvariant/issues/446#issuecomment-826959857:309,Testability,test,test-,309,"Thanks @pgrosu !. Follow up with you @ruolin , ; I started a machine with:; `gcloud compute instances create --project OUR_PROJECT --zone us-west1-b --image-project ubuntu-os-cloud --image-family ubuntu-1804-lts --machine-type custom-64-131072 --min-cpu-platform ""Intel Skylake"" --boot-disk-size 300G pichuan-test-20210425`. which is a 64 core, 128 GB RAM machine, and the command finished running without any issue.; The command I ran was:. ```; sudo docker run \; -v /home/pichuan/pacbio-case-study/input/data:/input \; -v /home/pichuan/pacbio-case-study/output:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/Homo_sapiens_assembly19.fasta \; --reads=/input/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam \; --output_vcf=/output/deepvariant.output.vcf.gz \; --output_gvcf=/output/deepvariant.output.g.vcf.gz \; --num_shards=64 \; --logging_dir=/output/logs \; --runtime_report --use_hp_information=true; ```. I can try another run with 20 cores, and I can also try with smaller disks to see if that would produce a similar issue. @ruolin let us know if you have any other ideas for us to reproduce this issue. I haven't tried Terra, but there's someone I can contact there for DeepVariant setup, feel free to let me know too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826959857
https://github.com/google/deepvariant/issues/446#issuecomment-826959857:933,Testability,log,logs,933,"Thanks @pgrosu !. Follow up with you @ruolin , ; I started a machine with:; `gcloud compute instances create --project OUR_PROJECT --zone us-west1-b --image-project ubuntu-os-cloud --image-family ubuntu-1804-lts --machine-type custom-64-131072 --min-cpu-platform ""Intel Skylake"" --boot-disk-size 300G pichuan-test-20210425`. which is a 64 core, 128 GB RAM machine, and the command finished running without any issue.; The command I ran was:. ```; sudo docker run \; -v /home/pichuan/pacbio-case-study/input/data:/input \; -v /home/pichuan/pacbio-case-study/output:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/Homo_sapiens_assembly19.fasta \; --reads=/input/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam \; --output_vcf=/output/deepvariant.output.vcf.gz \; --output_gvcf=/output/deepvariant.output.g.vcf.gz \; --num_shards=64 \; --logging_dir=/output/logs \; --runtime_report --use_hp_information=true; ```. I can try another run with 20 cores, and I can also try with smaller disks to see if that would produce a similar issue. @ruolin let us know if you have any other ideas for us to reproduce this issue. I haven't tried Terra, but there's someone I can contact there for DeepVariant setup, feel free to let me know too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-826959857
https://github.com/google/deepvariant/issues/446#issuecomment-827022298:152,Deployability,configurat,configurations,152,@pichuan @danielecook Thank you guys for such a quick response. It is something I feared. The Cromwell dispatcher is a black box to me. I will try your configurations and some others too to see if it solves the problems.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-827022298
https://github.com/google/deepvariant/issues/446#issuecomment-827022298:152,Modifiability,config,configurations,152,@pichuan @danielecook Thank you guys for such a quick response. It is something I feared. The Cromwell dispatcher is a black box to me. I will try your configurations and some others too to see if it solves the problems.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446#issuecomment-827022298
https://github.com/google/deepvariant/issues/447#issuecomment-830259038:283,Usability,feedback,feedback,283,"Hi Andrew, sorry for the delay,. I was aiming to get per nucleotide values of read depth, base and mapping qualities. I just come to check that this information can in fact be obtained from `samtools depth` and `samtools mpileup`, so I guess that will work for us. Thank you for the feedback!; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/447#issuecomment-830259038
https://github.com/google/deepvariant/issues/448#issuecomment-827132888:54,Performance,cache,cache,54,then I found the call_variants.py is listed in /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants.py,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/448#issuecomment-827132888
https://github.com/google/deepvariant/issues/448#issuecomment-827135295:40,Availability,error,error,40,"when I build_and_test.sh, there showing error:; bazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist. where is this directory?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/448#issuecomment-827135295
https://github.com/google/deepvariant/issues/448#issuecomment-828660901:170,Modifiability,config,config,170,I'm not sure I understand what you are trying to do. If you'd like to build your own docker image that includes DeepVariant you may follow commands in DeepVariant docker config - deepvariant/Dockerfile,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/448#issuecomment-828660901
https://github.com/google/deepvariant/issues/449#issuecomment-828712929:73,Testability,log,logic,73,@mvilsker The pileup_image python and C++ code here contains most of the logic for constructing the channels. https://github.com/google/deepvariant/blob/r1.1/deepvariant/pileup_image.py; https://github.com/google/deepvariant/blob/r1.1/deepvariant/pileup_image_native.h; https://github.com/google/deepvariant/blob/r1.1/deepvariant/pileup_image_native.cc. This code does the work of generating tensors for each example:. https://github.com/google/deepvariant/blob/r1.1/deepvariant/python/clif_converters.cc,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/449#issuecomment-828712929
https://github.com/google/deepvariant/issues/450#issuecomment-829516306:1200,Availability,error,error,1200,"Hi @husamia . Thank you for the question. The answer is a big complex. To summarize your question it is roughly. ```; The biggest issue that we have in calling de novos is managing ""false positive"" de novo events - not necessarily where the proband call is incorrect, but where a 0/1-0/0-0/0 call should be (for example 0/1-0/1-0/0). Will DeepTrio reduce the number of these calls.; ```. Calls in DeepVariant which are 0/1-0/0-0/0 where a human inspection in IGV shows evidence in the parent are generally caused by:. 1. Reads which have a MAPQ below the threshold that DeepVariant sees (MAPQ < 5). ; 2. Regions which DeepVariant seems to think may represent a segmental duplication where reads are mapped from a different region. In these cases, DeepVariant can all a position as 0/0. In terms of overall precision, DeepTrio has higher precision on de novo calls (0/1-0/0-0/0) than DeepVariant [tables for this are in the manuscript](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). In the cases that you mention, DeepTrio will more often designate the parent as a nocall (0/1-./.-0/0) for example. The genotype quality in the parents in DeepTrio can be useful for reducing the Type II error that you mention. If you want to further reduce these positions, I expect it would be possible to post-filter using the coverage values for the parent calls in the VCF (further filtering those that have a reasonable number of AD for ALT reads). We'll also consider whether including that during the VCF generation could make sense. Please let me know if this didn't answer your question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-829516306
https://github.com/google/deepvariant/issues/450#issuecomment-829516306:348,Energy Efficiency,reduce,reduce,348,"Hi @husamia . Thank you for the question. The answer is a big complex. To summarize your question it is roughly. ```; The biggest issue that we have in calling de novos is managing ""false positive"" de novo events - not necessarily where the proband call is incorrect, but where a 0/1-0/0-0/0 call should be (for example 0/1-0/1-0/0). Will DeepTrio reduce the number of these calls.; ```. Calls in DeepVariant which are 0/1-0/0-0/0 where a human inspection in IGV shows evidence in the parent are generally caused by:. 1. Reads which have a MAPQ below the threshold that DeepVariant sees (MAPQ < 5). ; 2. Regions which DeepVariant seems to think may represent a segmental duplication where reads are mapped from a different region. In these cases, DeepVariant can all a position as 0/0. In terms of overall precision, DeepTrio has higher precision on de novo calls (0/1-0/0-0/0) than DeepVariant [tables for this are in the manuscript](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). In the cases that you mention, DeepTrio will more often designate the parent as a nocall (0/1-./.-0/0) for example. The genotype quality in the parents in DeepTrio can be useful for reducing the Type II error that you mention. If you want to further reduce these positions, I expect it would be possible to post-filter using the coverage values for the parent calls in the VCF (further filtering those that have a reasonable number of AD for ALT reads). We'll also consider whether including that during the VCF generation could make sense. Please let me know if this didn't answer your question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-829516306
https://github.com/google/deepvariant/issues/450#issuecomment-829516306:1247,Energy Efficiency,reduce,reduce,1247,"Hi @husamia . Thank you for the question. The answer is a big complex. To summarize your question it is roughly. ```; The biggest issue that we have in calling de novos is managing ""false positive"" de novo events - not necessarily where the proband call is incorrect, but where a 0/1-0/0-0/0 call should be (for example 0/1-0/1-0/0). Will DeepTrio reduce the number of these calls.; ```. Calls in DeepVariant which are 0/1-0/0-0/0 where a human inspection in IGV shows evidence in the parent are generally caused by:. 1. Reads which have a MAPQ below the threshold that DeepVariant sees (MAPQ < 5). ; 2. Regions which DeepVariant seems to think may represent a segmental duplication where reads are mapped from a different region. In these cases, DeepVariant can all a position as 0/0. In terms of overall precision, DeepTrio has higher precision on de novo calls (0/1-0/0-0/0) than DeepVariant [tables for this are in the manuscript](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). In the cases that you mention, DeepTrio will more often designate the parent as a nocall (0/1-./.-0/0) for example. The genotype quality in the parents in DeepTrio can be useful for reducing the Type II error that you mention. If you want to further reduce these positions, I expect it would be possible to post-filter using the coverage values for the parent calls in the VCF (further filtering those that have a reasonable number of AD for ALT reads). We'll also consider whether including that during the VCF generation could make sense. Please let me know if this didn't answer your question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-829516306
https://github.com/google/deepvariant/issues/450#issuecomment-829604549:321,Availability,error,error,321,"Yes, it's a fine balance indeed. Looking at it from different perspective. Higher than normal (in matched trios) of presumed ""de novo germline variants in the child"" , generally indicates sample QC problem. Another perspective, 99.9% of the de novo germline candidates in the child are false positives due to the type II error that I described. It's pretty significant!. In the context of the calling de novo germline variants in child. When there is QC issue, the will be way too many false positivies. Samples should be flagged. When it's just mapping quality issues, the variants should be execluded in the search i.e. don't call them as de novo. This would make the search much smaller. . This would be a model for the ""de novo germline in the child"". Would it be possible to then make calls for this model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-829604549
https://github.com/google/deepvariant/issues/450#issuecomment-830343312:197,Usability,simpl,simple,197,"Hi @husamia . I am curious, are you working with human sequence data, or do these come from non-human species?. I will take a look at the ratio of these events in our sample. If we could provide a simple filtering script to postprocess a callset, would this be something that you might use?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-830343312
https://github.com/google/deepvariant/issues/450#issuecomment-830492777:419,Usability,feedback,feedback,419,@AndrewCarroll I am working with human data for clinical diagnosis of rare disease. There is great interest in the clinical community to call de novo variants in the child from trio data. There is data about the rates of those events. . In 1% of the human genome you should find zero de novo in most cases. In less than 50% of the cases there are one or two that are real. I can run it on some curated data and provide feedback.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-830492777
https://github.com/google/deepvariant/issues/450#issuecomment-831068064:67,Usability,feedback,feedback,67,"Hi @husamia . Thank you for the extra information. If there is any feedback that you can provide, I would be very grateful for examples. If you would like to follow up by email, you can reach me at awcarroll@google.com. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450#issuecomment-831068064
https://github.com/google/deepvariant/issues/451#issuecomment-830342295:983,Availability,error,errors,983,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295
https://github.com/google/deepvariant/issues/451#issuecomment-830342295:276,Performance,perform,performed,276,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295
https://github.com/google/deepvariant/issues/451#issuecomment-830342295:563,Usability,learn,learned,563,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295
https://github.com/google/deepvariant/issues/451#issuecomment-831232855:172,Performance,perform,performing,172,"Thanks Andrew,. This work was specifically motivated by recent approaches like those in the VGP of using SNP/indels to polish large assemblies, and deepvariant being a top performing tool for this. Unfortunately, they don't use triobinning, and triobinning papers generally only polish with the binnable long reads, so perhaps this problem is currently unsolvable. I'll keep trying out some different ideas in case anything works, but thanks for the discussion.; Alex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/451#issuecomment-831232855
https://github.com/google/deepvariant/issues/452#issuecomment-833796921:145,Availability,error,errors,145,"The TF/Cuda/cuDNN versions being used may be incompatible with your GPU. A few questions for you:. * Does the quickstart without GPU run without errors? Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; * What OS are you running on?; * Command to reproduce your issue?; * Are you able to run TF + GPU separately from DeepVariant?; * Do older versions of DeepVariant run without errors? I recommend trying 1.0.0 or 0.10.0. We recommend using the latest version for best results, but this is just for debugging.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-833796921
https://github.com/google/deepvariant/issues/452#issuecomment-833796921:428,Availability,error,errors,428,"The TF/Cuda/cuDNN versions being used may be incompatible with your GPU. A few questions for you:. * Does the quickstart without GPU run without errors? Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; * What OS are you running on?; * Command to reproduce your issue?; * Are you able to run TF + GPU separately from DeepVariant?; * Do older versions of DeepVariant run without errors? I recommend trying 1.0.0 or 0.10.0. We recommend using the latest version for best results, but this is just for debugging.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-833796921
https://github.com/google/deepvariant/issues/452#issuecomment-833796921:160,Testability,test,test,160,"The TF/Cuda/cuDNN versions being used may be incompatible with your GPU. A few questions for you:. * Does the quickstart without GPU run without errors? Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; * What OS are you running on?; * Command to reproduce your issue?; * Are you able to run TF + GPU separately from DeepVariant?; * Do older versions of DeepVariant run without errors? I recommend trying 1.0.0 or 0.10.0. We recommend using the latest version for best results, but this is just for debugging.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-833796921
https://github.com/google/deepvariant/issues/452#issuecomment-834942601:665,Deployability,install,installation-guide,665,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601
https://github.com/google/deepvariant/issues/452#issuecomment-834942601:690,Deployability,update,update,690,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601
https://github.com/google/deepvariant/issues/452#issuecomment-834942601:798,Deployability,update,update,798,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601
https://github.com/google/deepvariant/issues/452#issuecomment-834942601:678,Usability,guid,guide,678,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601
https://github.com/google/deepvariant/issues/452#issuecomment-837436553:4,Deployability,update,update,4,any update?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-837436553
https://github.com/google/deepvariant/issues/452#issuecomment-838559679:146,Deployability,upgrade,upgraded,146,"Hi @aardes, my understanding is that cuDNN v8, CUDA 11, TF 2.5, and Python 3.8 will be needed for RTX 3090. Our code is currently not ready to be upgraded to Python 3.8, but this is something we are looking into for future releases.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-838559679
https://github.com/google/deepvariant/issues/452#issuecomment-838559679:223,Deployability,release,releases,223,"Hi @aardes, my understanding is that cuDNN v8, CUDA 11, TF 2.5, and Python 3.8 will be needed for RTX 3090. Our code is currently not ready to be upgraded to Python 3.8, but this is something we are looking into for future releases.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-838559679
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2797,Deployability,configurat,configuration,2797,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:1801,Energy Efficiency,monitor,monitor,1801,"19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 mem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2513,Energy Efficiency,power,power,2513,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:156,Integrability,message,messages,156,"may 2023, ; running on a dell 730 with 88 cores using google/deepvariant:1.5.0. I do not have a GPU on this server and my cpu are avx2. I get the following messages:. ```; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-04 12:48:39.049123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; ```. how can I get the docker to match my architecture. The quickstart code runs with many such warnings and generates data; is this data OK?. ```; sudo docker run \; -u $(id -u) \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2797,Modifiability,config,configuration,2797,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:372,Performance,optimiz,optimized,372,"may 2023, ; running on a dell 730 with 88 cores using google/deepvariant:1.5.0. I do not have a GPU on this server and my cpu are avx2. I get the following messages:. ```; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-04 12:48:39.049123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; ```. how can I get the docker to match my architecture. The quickstart code runs with many such warnings and generates data; is this data OK?. ```; sudo docker run \; -u $(id -u) \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:472,Performance,perform,performance-critical,472,"may 2023, ; running on a dell 730 with 88 cores using google/deepvariant:1.5.0. I do not have a GPU on this server and my cpu are avx2. I get the following messages:. ```; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-04 12:48:39.049123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; ```. how can I get the docker to match my architecture. The quickstart code runs with many such warnings and generates data; is this data OK?. ```; sudo docker run \; -u $(id -u) \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:1354,Performance,cache,cache,1354,"19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 mem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2827,Performance,latency,latency,2827,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205
https://github.com/google/deepvariant/issues/452#issuecomment-1496617163:219,Testability,test,test,219,@splaisan if the quickstart runs and the timing is acceptable to you then you can use the Docker image as is. Otherwise you can [build from source](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1496617163
https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:574,Deployability,install,install,574,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051
https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:591,Deployability,install,install,591,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051
https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:847,Deployability,configurat,configurations,847,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051
https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:25,Integrability,depend,depends,25,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051
https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:847,Modifiability,config,configurations,847,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051
https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:404,Safety,risk,risk,404,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051
https://github.com/google/deepvariant/issues/452#issuecomment-1503989450:22,Deployability,install,installed,22,"@splaisan Which OS is installed on your server? Linux, Ubuntu, ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452#issuecomment-1503989450
https://github.com/google/deepvariant/issues/453#issuecomment-834362623:30,Availability,error,error,30,"I stand corrected, this is an error with GLNexus, The per-sample vcf produced by DeepVariant shows the locus as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/453#issuecomment-834362623
https://github.com/google/deepvariant/issues/454#issuecomment-834808144:275,Safety,predict,predict,275,"Hi @maryawood ,; `customized_classes_labeler` is an internal, experimental feature that we didn't provide documentation because we don't plan to provide support on it at this moment. Can you provide a bit more detail on your problem definition - E..g, what are you trying to predict? ; You mentioned `callsets` in INFO -- does that mean you're trying to build a variant classifier that could predict these callsets values? If so, if you don't mind sharing, what's the problem that you're trying to solve? . In general, DeepVariant utilizes a general classification algorithm (InceptionV3), however, our codebase serves a much more specialized purpose than InceptionV3, so the codebase does have a lot more hard-coded constraint. For example, the codebase assumes that there are 3 class: 0, 1, 2. [This blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) has more details on what the inputs and outputs are for the InceptionV3 model. It is possible to make enough changes to DeepVariant to get it work on other problems. But, if you're solving a very different problem from DeepVariant, I strongly suggest that you look into more general libraries - such as other image classification libraries in TensorFlow or other frameworks you like. If you need to process files such as VCFs, you can consider using [Nucleus](https://github.com/google/nucleus) which is what we used to process common genomics file formats in DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-834808144
https://github.com/google/deepvariant/issues/454#issuecomment-834808144:392,Safety,predict,predict,392,"Hi @maryawood ,; `customized_classes_labeler` is an internal, experimental feature that we didn't provide documentation because we don't plan to provide support on it at this moment. Can you provide a bit more detail on your problem definition - E..g, what are you trying to predict? ; You mentioned `callsets` in INFO -- does that mean you're trying to build a variant classifier that could predict these callsets values? If so, if you don't mind sharing, what's the problem that you're trying to solve? . In general, DeepVariant utilizes a general classification algorithm (InceptionV3), however, our codebase serves a much more specialized purpose than InceptionV3, so the codebase does have a lot more hard-coded constraint. For example, the codebase assumes that there are 3 class: 0, 1, 2. [This blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) has more details on what the inputs and outputs are for the InceptionV3 model. It is possible to make enough changes to DeepVariant to get it work on other problems. But, if you're solving a very different problem from DeepVariant, I strongly suggest that you look into more general libraries - such as other image classification libraries in TensorFlow or other frameworks you like. If you need to process files such as VCFs, you can consider using [Nucleus](https://github.com/google/nucleus) which is what we used to process common genomics file formats in DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-834808144
https://github.com/google/deepvariant/issues/454#issuecomment-835842617:102,Safety,predict,predict,102,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617
https://github.com/google/deepvariant/issues/454#issuecomment-835842617:305,Safety,predict,predicted,305,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617
https://github.com/google/deepvariant/issues/454#issuecomment-835842617:691,Usability,learn,learn,691,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617
https://github.com/google/deepvariant/issues/454#issuecomment-836125971:363,Availability,error,error,363,"Hi @maryawood . It might be possible to train a classifier in this manner, but it is certainly outside the use cases which we would typically train for. If you are looking for an assessment of how ""confident"" DeepVariant is in a call, I would recommend you take a look at the ""GQ"" field. This PHRED value is quite well calibrated with the probability of a caller error in our investigations (see: Figure2 of [the original DeepVariant paper](https://www.nature.com/articles/nbt.4235). If possible using GQ may be more informative than training a model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454#issuecomment-836125971
https://github.com/google/deepvariant/issues/455#issuecomment-836919821:20,Availability,error,error,20,Hi @zjminglead; The error messages say the BAM file looks truncated. Does it give any errors when you run `samtools view <BAM> | tail` and look at the end of the BAM file?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-836919821
https://github.com/google/deepvariant/issues/455#issuecomment-836919821:86,Availability,error,errors,86,Hi @zjminglead; The error messages say the BAM file looks truncated. Does it give any errors when you run `samtools view <BAM> | tail` and look at the end of the BAM file?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-836919821
https://github.com/google/deepvariant/issues/455#issuecomment-836919821:26,Integrability,message,messages,26,Hi @zjminglead; The error messages say the BAM file looks truncated. Does it give any errors when you run `samtools view <BAM> | tail` and look at the end of the BAM file?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-836919821
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:189,Availability,error,error,189,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:397,Availability,error,error,397,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:485,Availability,error,error,485,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:518,Availability,error,error,518,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:584,Availability,error,error,584,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:880,Availability,error,error,880,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:1204,Availability,down,download,1204,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/455#issuecomment-839412109:1270,Availability,error,error,1270,"Hello @MariaNattestad ; Thanks for reply, it turns out BAM file is truncated, here is my train of thought and step to fix the problem.; 1. Because 3 bam(HG002 HG003 HG004) file report same error, and I also tried to redownload bam file, it didn't work  either,  it reports [W::bam_hdr_read] EOF marker is absent. The input is probably truncated and [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; 2. I google [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes, this error mostly occur at samtools usage, I think it maybe bam format error.; 3. I convert bam to sam  using samtools , and it reports bam is truncated, Then convert sam back to bam. The size between two bam is different, the  original one about 8GB, but the one I generated is just about 800M; 4. I run make_example.py using new bam file,  it also report my bam is error, it may be bai and bam don't match.; 5. I try another bam file, https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG005-EEogPU_v02-KIT-Av5_CGCATACA_L008.posiSrt.markDup.bam,; because i familiar with wget, so I use wget to download bam file. And it runs successful.; 6. So it maybe aria2c error, I redownload all file using wget and demo is successfully executed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455#issuecomment-839412109
https://github.com/google/deepvariant/issues/457#issuecomment-838736936:331,Availability,error,errors,331,Haha thanks for catching that warGning message! I'll take care of that :). Sorry it looks like the warnings about these HP-related flags are confusing!. First let me try to understand what you are trying to do:; 1) Is your BAM phased? e.g. using whatshap.; 2) Which documentation page/example are you following that gave you these errors originally?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-838736936
https://github.com/google/deepvariant/issues/457#issuecomment-838736936:39,Integrability,message,message,39,Haha thanks for catching that warGning message! I'll take care of that :). Sorry it looks like the warnings about these HP-related flags are confusing!. First let me try to understand what you are trying to do:; 1) Is your BAM phased? e.g. using whatshap.; 2) Which documentation page/example are you following that gave you these errors originally?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-838736936
https://github.com/google/deepvariant/issues/457#issuecomment-839642012:437,Integrability,depend,depending,437,"Hi Maria! Thanks for your fast reply,; so the Bam I'm using is acutually generated a bit differently than usual (at least it's not actual HiFi ccs reads).. I performed LAA from pb software (v8) on targeted (one gene) HiFi amplicon data. LAA performs ccs and demultiplexing, and from that it immediately creates consensus 'clustered/phased' sequences (as the target is only one gene, the fastq consists of only one or two consensus seqs, depending on the found alleles). I have mapped those against our reference gene using minimap2 and this results in the Bam I'm using as input here. . This particular Bam contains only one read. My worry is that DeepVariant will not like the fact that there is only one read, but I believe this issue would be similar when performing lowcov sequencing. Maybe the model I'm using 'PACBIO' is considering actual PacBio HiFi ccs reads? Anyways, I didn't get to that conclusion because it didn't find the record at all. My guess was that it had to do with sam parsing. ; I'm curious for your opinion.. :). Annabel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-839642012
https://github.com/google/deepvariant/issues/457#issuecomment-839642012:158,Performance,perform,performed,158,"Hi Maria! Thanks for your fast reply,; so the Bam I'm using is acutually generated a bit differently than usual (at least it's not actual HiFi ccs reads).. I performed LAA from pb software (v8) on targeted (one gene) HiFi amplicon data. LAA performs ccs and demultiplexing, and from that it immediately creates consensus 'clustered/phased' sequences (as the target is only one gene, the fastq consists of only one or two consensus seqs, depending on the found alleles). I have mapped those against our reference gene using minimap2 and this results in the Bam I'm using as input here. . This particular Bam contains only one read. My worry is that DeepVariant will not like the fact that there is only one read, but I believe this issue would be similar when performing lowcov sequencing. Maybe the model I'm using 'PACBIO' is considering actual PacBio HiFi ccs reads? Anyways, I didn't get to that conclusion because it didn't find the record at all. My guess was that it had to do with sam parsing. ; I'm curious for your opinion.. :). Annabel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-839642012
https://github.com/google/deepvariant/issues/457#issuecomment-839642012:241,Performance,perform,performs,241,"Hi Maria! Thanks for your fast reply,; so the Bam I'm using is acutually generated a bit differently than usual (at least it's not actual HiFi ccs reads).. I performed LAA from pb software (v8) on targeted (one gene) HiFi amplicon data. LAA performs ccs and demultiplexing, and from that it immediately creates consensus 'clustered/phased' sequences (as the target is only one gene, the fastq consists of only one or two consensus seqs, depending on the found alleles). I have mapped those against our reference gene using minimap2 and this results in the Bam I'm using as input here. . This particular Bam contains only one read. My worry is that DeepVariant will not like the fact that there is only one read, but I believe this issue would be similar when performing lowcov sequencing. Maybe the model I'm using 'PACBIO' is considering actual PacBio HiFi ccs reads? Anyways, I didn't get to that conclusion because it didn't find the record at all. My guess was that it had to do with sam parsing. ; I'm curious for your opinion.. :). Annabel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-839642012
https://github.com/google/deepvariant/issues/457#issuecomment-839642012:759,Performance,perform,performing,759,"Hi Maria! Thanks for your fast reply,; so the Bam I'm using is acutually generated a bit differently than usual (at least it's not actual HiFi ccs reads).. I performed LAA from pb software (v8) on targeted (one gene) HiFi amplicon data. LAA performs ccs and demultiplexing, and from that it immediately creates consensus 'clustered/phased' sequences (as the target is only one gene, the fastq consists of only one or two consensus seqs, depending on the found alleles). I have mapped those against our reference gene using minimap2 and this results in the Bam I'm using as input here. . This particular Bam contains only one read. My worry is that DeepVariant will not like the fact that there is only one read, but I believe this issue would be similar when performing lowcov sequencing. Maybe the model I'm using 'PACBIO' is considering actual PacBio HiFi ccs reads? Anyways, I didn't get to that conclusion because it didn't find the record at all. My guess was that it had to do with sam parsing. ; I'm curious for your opinion.. :). Annabel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-839642012
https://github.com/google/deepvariant/issues/457#issuecomment-839953175:103,Performance,perform,perform,103,"Okay, I see.; Yes, the PacBio model is meant for HiFi reads only, but also DeepVariant is not going to perform well with a single read. For one thing, the very sensitive caller that finds candidates looks for 2 reads showing the same potential variant allele, but also the models are trained with coverages only as low as about 15X, so even if you changed the thresholds for candidate generation to 1 read, the model would probably call it not-a-real-variant (class 0).; When you saw that DeepVariant finished but created 0 candidates, that is the expected behavior since the coverage is too low to create any candidates with 2+ supporting reads. You might want to check with PacBio what they recommend for calling variants from LAA, since I don't think DeepVariant would be your best choice for this type of data. Good luck!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-839953175
https://github.com/google/deepvariant/issues/457#issuecomment-840635414:38,Integrability,depend,depends,38,"Happy to help!; For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-840635414
https://github.com/google/deepvariant/issues/457#issuecomment-840889768:127,Integrability,message,message,127,"By the way, can you let me know what documentation page you were working from for the original command?. I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-840889768
https://github.com/google/deepvariant/issues/457#issuecomment-844185505:295,Availability,error,error,295,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
https://github.com/google/deepvariant/issues/457#issuecomment-844185505:614,Deployability,install,installed,614,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
https://github.com/google/deepvariant/issues/457#issuecomment-844185505:584,Integrability,depend,dependencies,584,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
https://github.com/google/deepvariant/issues/457#issuecomment-844185505:177,Testability,test,test,177,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
https://github.com/google/deepvariant/issues/457#issuecomment-844185505:738,Usability,feedback,feedback,738,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505
https://github.com/google/deepvariant/issues/457#issuecomment-844209552:52,Integrability,depend,depending,52,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
https://github.com/google/deepvariant/issues/457#issuecomment-844209552:774,Safety,avoid,avoid,774,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
https://github.com/google/deepvariant/issues/457#issuecomment-844209552:439,Usability,simpl,simplified,439,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
https://github.com/google/deepvariant/issues/457#issuecomment-844209552:1108,Usability,simpl,simpler,1108,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552
https://github.com/google/deepvariant/issues/457#issuecomment-845522247:1812,Deployability,update,update,1812,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247
https://github.com/google/deepvariant/issues/457#issuecomment-845522247:1698,Testability,log,logic,1698,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247
https://github.com/google/deepvariant/issues/457#issuecomment-845522247:301,Usability,simpl,simplify,301,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247
https://github.com/google/deepvariant/issues/458#issuecomment-844317545:1189,Deployability,update,update,1189,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545
https://github.com/google/deepvariant/issues/458#issuecomment-844317545:1222,Safety,avoid,avoid,1222,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545
https://github.com/google/deepvariant/issues/458#issuecomment-844317545:132,Usability,simpl,simplify,132,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545
https://github.com/google/deepvariant/issues/459#issuecomment-849788653:23,Testability,benchmark,benchmarking,23,"Hi, I am interested in benchmarking DV on exome data from a group of snakes. I have a single reference genome from a closely related species, but I would not have the trio data as was done with the Drosophila data. Do you know if anyone has been able to retrain DV using one or two genomes without pedigree info? . Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-849788653
https://github.com/google/deepvariant/issues/459#issuecomment-850009279:227,Performance,perform,performed,227,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-850009279
https://github.com/google/deepvariant/issues/459#issuecomment-858073154:602,Usability,learn,learning,602,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues?. Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)?. We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-858073154
https://github.com/google/deepvariant/issues/459#issuecomment-858159795:958,Modifiability,extend,extend,958,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. ; I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-858159795
https://github.com/google/deepvariant/issues/459#issuecomment-858159795:617,Testability,test,test,617,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. ; I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-858159795
https://github.com/google/deepvariant/issues/459#issuecomment-858159795:701,Testability,test,test,701,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. ; I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459#issuecomment-858159795
https://github.com/google/deepvariant/issues/460#issuecomment-854263305:583,Testability,log,logic,583,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460#issuecomment-854263305
https://github.com/google/deepvariant/issues/460#issuecomment-854263305:751,Testability,log,logic,751,"Hi @lpryszcz . With respect to your aligner question, we have evaluated DeepVariant with BWA-MEM/BWA-MEM2, minimap2, DRAGEN, and the graph mapper Giraffe. For mapping to a linear reference, we would recommend BWA-MEM/BWA-MEM2 or DRAGEN. DeepVariant is trained on BWA MEM data. Minimap2 will work, but has slightly lower accuracy. . We have not evaluated HISAT2. If I had to guess, I would suspect that there might be split read mapping which is creating more candidates. The difference in speed that you encounter suggests that this has something to do with our candidate generation logic, as opposed to something about the neural network. This would probably require us to look at what in HISAT2 is causing an edge case with our candidate generation logic. It's possible there is a flag in HISAT2 that could be altered, but failing that it would probably take us some time to prioritize supporting HISAT2. . We typically find that marking duplicates is not necessary, but it does not have a negative effect on accuracy to do so for typical coverages (25x-50x). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/460#issuecomment-854263305
https://github.com/google/deepvariant/issues/462#issuecomment-866335254:71,Usability,guid,guidance,71,"I was not able to get this working @pichuan , if you could provide any guidance that'd still be helpful for me. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-866335254
https://github.com/google/deepvariant/issues/462#issuecomment-866404518:243,Availability,error,error,243,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. ; DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:; ```; time sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; In my test run on a **t2.medium** instance, this took: ; ```; real0m23.790s; user0m0.032s; sys0m0.028s; ```; to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-866404518
https://github.com/google/deepvariant/issues/462#issuecomment-866404518:249,Integrability,message,message,249,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. ; DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:; ```; time sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; In my test run on a **t2.medium** instance, this took: ; ```; real0m23.790s; user0m0.032s; sys0m0.028s; ```; to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-866404518
https://github.com/google/deepvariant/issues/462#issuecomment-866404518:1206,Testability,test,test,1206,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. ; DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:; ```; time sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; In my test run on a **t2.medium** instance, this took: ; ```; real0m23.790s; user0m0.032s; sys0m0.028s; ```; to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-866404518
https://github.com/google/deepvariant/issues/462#issuecomment-867185369:55,Usability,guid,guide,55,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```; docker: invalid reference format.; See 'docker run --help'. real	0m0.046s; user	0m0.023s; sys	0m0.028s; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-867185369
https://github.com/google/deepvariant/issues/462#issuecomment-867206060:138,Availability,echo,echo,138,"@mvelinder I think sometimes this could happen when the environment variables were not manually set right. For example, can you do:; ```; echo $BIN_VERSION; ```; (and all the others that you might have in your command) to make sure they're all set as expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-867206060
https://github.com/google/deepvariant/issues/462#issuecomment-867206060:68,Modifiability,variab,variables,68,"@mvelinder I think sometimes this could happen when the environment variables were not manually set right. For example, can you do:; ```; echo $BIN_VERSION; ```; (and all the others that you might have in your command) to make sure they're all set as expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-867206060
https://github.com/google/deepvariant/issues/462#issuecomment-867736803:110,Testability,test,test,110,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \; 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=/output/output.vcf.gz \; 	--output_gvcf=/output/output.g.vcf.gz \; 	--intermediate_results_dir /output/intermediate_results_dir \; 	--num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-867736803
https://github.com/google/deepvariant/issues/462#issuecomment-867736803:201,Testability,test,testdata,201,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \; 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=/output/output.vcf.gz \; 	--output_gvcf=/output/output.g.vcf.gz \; 	--intermediate_results_dir /output/intermediate_results_dir \; 	--num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462#issuecomment-867736803
https://github.com/google/deepvariant/issues/463#issuecomment-864304812:703,Availability,avail,available,703,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812
https://github.com/google/deepvariant/issues/463#issuecomment-864304812:1102,Testability,test,testdata,1102,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812
https://github.com/google/deepvariant/issues/463#issuecomment-864304812:390,Usability,learn,learn,390,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:69,Availability,avail,available,69,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:742,Availability,avail,available,742,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:353,Deployability,configurat,configuration,353,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:425,Deployability,configurat,configuration,425,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:353,Modifiability,config,configuration,353,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:425,Modifiability,config,configuration,425,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:101,Testability,benchmark,benchmark,101,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-864505178:315,Testability,log,log,315,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178
https://github.com/google/deepvariant/issues/463#issuecomment-866205653:699,Energy Efficiency,power,power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,699,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653
https://github.com/google/deepvariant/issues/463#issuecomment-866205653:609,Performance,optimiz,optimization,609,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653
https://github.com/google/deepvariant/issues/463#issuecomment-866205653:781,Performance,optimiz,optimizations,781,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653
https://github.com/google/deepvariant/issues/463#issuecomment-866205653:433,Testability,test,test,433,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:985,Energy Efficiency,power,power,985,"Thanks @pichuan,; I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:377,Performance,cache,cache,377,"Thanks @pichuan,; I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1117,Testability,log,log,1117,"elow cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1673,Testability,log,logs,1673,"l nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1689,Testability,log,logs,1689,"l nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1820,Testability,log,logs,1820,"e4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4',",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:2289,Testability,log,logs,2289," and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1087,Usability,clear,clear,1087,"elow cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:937,Availability,down,download,937,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:469,Deployability,install,installed,469,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:4149,Performance,Load,Load,4149,"make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]; I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants; I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples; ```. Here is a snapshot of `htop` while `make_examples` is running:; ```; 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running; Swp[ 0K/0K] Load average: 8.09 7.59 4.84 ; Uptime: 01:04:14; ```. make_examples took:; ```; real 32m36.929s; user 253m55.294s; sys 1m1.715s; ```. ## call_variants; At the beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:4458,Performance,optimiz,optimized,4458,"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running; Swp[ 0K/0K] Load average: 8.09 7.59 4.84 ; Uptime: 01:04:14; ```. make_examples took:; ```; real 32m36.929s; user 253m55.294s; sys 1m1.715s; ```. ## call_variants; At the beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:4557,Performance,perform,performance-critical,4557,"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running; Swp[ 0K/0K] Load average: 8.09 7.59 4.84 ; Uptime: 01:04:14; ```. make_examples took:; ```; real 32m36.929s; user 253m55.294s; sys 1m1.715s; ```. ## call_variants; At the beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5276,Performance,Tune,Tune,5276,"e beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5325,Performance,perform,performance,5325,"e beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5378,Performance,optimiz,optimization,5378,":25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:135,Testability,test,test,135,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:188,Testability,test,test-speed,188,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:766,Testability,test,test-speed,766,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:1666,Testability,log,log,1666,"thub.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]; I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]; I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]; I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]; ```. Here are the last few lines from the log:; ```; I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples; I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]; I0622 21",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:1699,Testability,log,log,1699,"\; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]; I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]; I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]; I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]; ```. Here are the last few lines from the log:; ```; I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples; I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]; I0622 21:24:43.683619 140528910345984 make_ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:2439,Testability,log,log,2439,nput/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]; I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]; I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]; I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]; ```. Here are the last few lines from the log:; ```; I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples; I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]; I0622 21:24:43.683619 140528910345984 make_examples.py:648] Task 0/8: 5700 candidates (6127 examples) [24.04s elapsed]; I0622 21:24:44.784906 139897026688768 make_examples.py:648] Task 4/8: 6002 candidates (6422 examples) [6.72s elapsed]; I0622 21:24:46.344424 139897026688768 make_examples.py:648] Task 4/8: Found 6004 candidate variants; I0622 21:24:46.344626 139897026688768 make_examples.py:648] Task 4/8: Created 6424 examples; I0622 21:24:58.252706 140528910345984 make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]; I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants; I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples; ```. Here is a snapshot,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5396,Testability,log,log,5396,"his TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32m1.122s; sys 0m32.211s; ```. Note that the CPU usage for `call_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5496,Testability,log,log,5496,"y (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32m1.122s; sys 0m32.211s; ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/463#issuecomment-866352316:6694,Testability,log,log,6694," 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32m1.122s; sys 0m32.211s; ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look).; I didn't take a snapshot of `htop` here. ## postprocess_variants; The log here is pretty short, so I'll paste below:; ```; ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003; 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz; 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760; I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes; I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants.; I0622 21:30:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316
https://github.com/google/deepvariant/issues/464#issuecomment-867256204:373,Integrability,depend,depend,373,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204
https://github.com/google/deepvariant/issues/464#issuecomment-867256204:607,Integrability,depend,depends,607,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204
https://github.com/google/deepvariant/issues/464#issuecomment-867256204:644,Usability,simpl,simple,644,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:23,Availability,error,error,23,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:1066,Availability,error,error,1066,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:286,Deployability,release,release,286,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:1072,Integrability,message,messages,1072,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:374,Testability,log,logs,374,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:835,Testability,log,logs,835,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:404,Usability,clear,clear,404,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870041849:1094,Usability,clear,clear,1094,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849
https://github.com/google/deepvariant/issues/465#issuecomment-870744502:179,Availability,error,errors,179,"Hi @pichuan ; I just used the command you suggested but I am not getting any log.; could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors?; Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870744502
https://github.com/google/deepvariant/issues/465#issuecomment-870744502:77,Testability,log,log,77,"Hi @pichuan ; I just used the command you suggested but I am not getting any log.; could you please let me know how to set /opt/deepvariant/bin/make_examples parameter to get the errors?; Thanks,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870744502
https://github.com/google/deepvariant/issues/465#issuecomment-870758299:43,Availability,error,error,43,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before); In this case it seems like we're trying to find out why it didn't. ; I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870758299
https://github.com/google/deepvariant/issues/465#issuecomment-870758299:128,Availability,error,errors,128,"@Asppagh Usually when make_examples has an error and crashes, it should have the stack trace at the end. (I've seen them in the errors I've encountered before); In this case it seems like we're trying to find out why it didn't. ; I think it's possible to change the verbose level. I'll need to look into that later. I'm traveling so it'll take a while for me to get back to this. Adding @danielecook in case you have a chance to take a look.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870758299
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:193,Availability,error,error,193,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:268,Availability,error,error,268,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:4681,Availability,error,error,4681,"s_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz; I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz; I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]; I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_jqt5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:6716,Availability,error,error,6716,"e ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s; user 1m28.020s; sys 0m5.611s; I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:6893,Availability,error,error,6893,"examples_runner(options); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.truncated.bam --examples /tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz --gvcf /tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz --task 0. real 1m25.608s; user 1m28.020s; sys 0m5.611s; I0629 23:10:12.080424 139667868600064 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:11368,Availability,error,error,11368," absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:22:14.603199 140118199654144 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz; I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz; I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]; I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_im0i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:13403,Availability,error,error,13403,"e ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:13580,Availability,error,error,13580,"examples_runner(options); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issue",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:13702,Availability,failure,failure,13702,"e_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:17949,Availability,error,error,17949,"ad3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7g_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:19984,Availability,error,error,19984,"hird_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:20161,Availability,error,error,20161,"hird_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14616,Deployability,install,install,14616,"ng, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA128",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:199,Integrability,message,messages,199,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14029,Integrability,Message,Messages,14029,"examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA1287",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:4877,Modifiability,extend,extend,4877,"1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz; I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz; I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]; I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:11564,Modifiability,extend,extend,11564,"genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz; I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz; I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]; I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:18145,Modifiability,extend,extend,18145,"testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15281,Performance,cache,cached,15281,"er); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:136,Testability,log,logs,136,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:158,Testability,test,test,158,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:391,Testability,test,test,391,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:639,Testability,test,testdata,639,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:740,Testability,test,testdata,740,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:818,Testability,test,testdata,818,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:871,Testability,test,testdata,871,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:936,Testability,test,testdata,936,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:991,Testability,test,testdata,991,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:1495,Testability,log,log,1495,"blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:49.663154 139654431344384 make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14004,Testability,Log,Logging,14004,"s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14038,Testability,log,logged,14038,"examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA1287",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14106,Testability,log,logging,14106,"rror_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14363,Testability,log,logging,14363,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably tru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14410,Testability,log,logs,14410,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably tru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14978,Testability,test,testdata,14978,"evel. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15042,Testability,test,testdata,15042," logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15228,Testability,log,log,15228,"er); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15446,Testability,test,testdata,15446," useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15582,Testability,test,testdata,15582,"/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15879,Testability,test,testdata,15879,":/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:16015,Testability,test,testdata,16015,".fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quick",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:16663,Testability,test,testdata,16663,"runcated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 13979615457049",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:16799,Testability,test,testdata,16799,"runcated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:17018,Testability,test,testdata,17018,"nomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 byte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:17154,Testability,test,testdata,17154,"mon contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/465#issuecomment-870990381:13776,Usability,clear,clear,13776,"e_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381
https://github.com/google/deepvariant/issues/467#issuecomment-870947313:135,Deployability,release,release,135,"@kokyriakidis currently we do not have a way of doing this, but we are working on an option to output candidate alleles in an upcoming release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467#issuecomment-870947313
https://github.com/google/deepvariant/issues/467#issuecomment-870965925:579,Deployability,release,release,579,"If you want to directly take from the output of make_examples and create vcf from there, you can see this implementation here:; https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py. Note that this currently requires the examples to have labels (i.e., generated with `training` mode). But you can relax the constraint here:; https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py#L123-L127. But, also note that I'm not sure if this code properly deals with multi-allelics. So, waiting for the next release to use the new option that @danielecook mentioned might be a better solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467#issuecomment-870965925
https://github.com/google/deepvariant/issues/467#issuecomment-871094311:63,Deployability,release,release,63,"Thanks a lot, both of you, for your reply. I will for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/467#issuecomment-871094311
https://github.com/google/deepvariant/issues/469#issuecomment-870966227:77,Deployability,release,release,77,Thanks @mattwood-codifiedgenomics . I'll take a look at this before the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-870966227
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6548,Availability,echo,echo,6548,"ation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:2603,Deployability,update,update,2603,"ools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64; ```. ```; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz""; ```. ```; sudo apt -y update; sudo apt -y install parallel; sudo apt -y install docker.io; ```. ```; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:2623,Deployability,install,install,2623,"ools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64; ```. ```; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz""; ```. ```; sudo apt -y update; sudo apt -y install parallel; sudo apt -y install docker.io; ```. ```; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:2653,Deployability,install,install,2653,"ools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64; ```. ```; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz""; ```. ```; sudo apt -y update; sudo apt -y install parallel; sudo apt -y install docker.io; ```. ```; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4270,Deployability,update,update,4270," ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4290,Deployability,install,install,4290," ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4328,Deployability,install,install,4328," ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4349,Deployability,upgrade,upgrade,4349," ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4464,Deployability,install,install,4464,"/home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4503,Deployability,install,install,4503,"VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6081,Deployability,install,install,6081,"OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6257,Deployability,install,install,6257,"ut_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6282,Deployability,install,install,6282,"ut_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7590,Deployability,install,installed,7590,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7577,Integrability,depend,dependencies,7577,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:913,Modifiability,variab,variables,913,"First thing I'm trying to do is to see if I can follow similar steps in:; https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of; https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md; and; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:; ```; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west2-b"" \; --min-cpu-platform ""Intel Skylake""; ```. Set variables; ```; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:1578,Testability,log,logs,1578,"cs/deepvariant-training-case-study.md. ---. I got a CPU machine:; ```; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west2-b"" \; --min-cpu-platform ""Intel Skylake""; ```. Set variables; ```; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64; ```. ```; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:3244,Testability,log,log,3244,"R}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz""; ```. ```; sudo apt -y update; sudo apt -y install parallel; sudo apt -y install docker.io; ```. ```; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:3885,Testability,log,log,3885,""" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6181,Testability,test,test,6181,"on us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be havi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7075,Testability,log,log,7075,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7634,Testability,test,tests,7634,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544
https://github.com/google/deepvariant/issues/469#issuecomment-876548428:145,Integrability,depend,dependency,145,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now.; If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-876548428
https://github.com/google/deepvariant/issues/469#issuecomment-880959171:42,Deployability,update,update,42,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171
https://github.com/google/deepvariant/issues/469#issuecomment-880959171:171,Deployability,release,release,171,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171
https://github.com/google/deepvariant/issues/469#issuecomment-880959171:1135,Deployability,release,release,1135,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171
https://github.com/google/deepvariant/issues/469#issuecomment-880959171:57,Testability,test,tested,57,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171
https://github.com/google/deepvariant/issues/469#issuecomment-880959171:227,Testability,test,test,227,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171
https://github.com/google/deepvariant/issues/470#issuecomment-878807029:40,Deployability,pipeline,pipeline,40,"I should also note that we ran the same pipeline on data we produced in-house, and that worked perfectly fine (including the locus I picked out here). So I am wondering if there is something about the reads our collaborator provided that somehow conflicts with assumptions made by DeepVariant (?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-878807029
https://github.com/google/deepvariant/issues/470#issuecomment-880951168:827,Deployability,release,releases,827,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment.; Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. ; It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate.; This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:; ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:; ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-880951168
https://github.com/google/deepvariant/issues/470#issuecomment-881195025:97,Deployability,release,releases,97,"Thanks for digging into this - yea I guess that makes sense. Will be waiting for a fix in future releases then , thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-881195025
https://github.com/google/deepvariant/issues/470#issuecomment-891869362:9,Deployability,update,update,9,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56; Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-891869362
https://github.com/google/deepvariant/issues/470#issuecomment-891869362:176,Testability,benchmark,benchmarking,176,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56; Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-891869362
https://github.com/google/deepvariant/issues/470#issuecomment-891873668:141,Deployability,release,release,141,"Hi, is this using WGS model?; You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps?; In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-891873668
https://github.com/google/deepvariant/issues/470#issuecomment-891873668:235,Deployability,update,update,235,"Hi, is this using WGS model?; You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps?; In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-891873668
https://github.com/google/deepvariant/issues/470#issuecomment-891873668:351,Deployability,release,release,351,"Hi, is this using WGS model?; You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps?; In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-891873668
https://github.com/google/deepvariant/issues/470#issuecomment-892811998:436,Deployability,release,release,436,"Thanks @marchoeppner . Good to hear that 1.2 worked.; I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-892811998
https://github.com/google/deepvariant/issues/470#issuecomment-892811998:489,Deployability,release,releases,489,"Thanks @marchoeppner . Good to hear that 1.2 worked.; I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470#issuecomment-892811998
https://github.com/google/deepvariant/pull/472#issuecomment-878550413:346,Energy Efficiency,reduce,reduce,346,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:; * WGS:; * Without OpenVINO: ~166m (this was an average from 3 runs); * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472#issuecomment-878550413
https://github.com/google/deepvariant/pull/472#issuecomment-878550413:18,Testability,test,tested,18,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:; * WGS:; * Without OpenVINO: ~166m (this was an average from 3 runs); * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472#issuecomment-878550413
https://github.com/google/deepvariant/pull/472#issuecomment-879985843:34,Deployability,update,updated,34,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472#issuecomment-879985843
https://github.com/google/deepvariant/pull/472#issuecomment-879985843:103,Testability,log,log,103,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/472#issuecomment-879985843
https://github.com/google/deepvariant/pull/473#issuecomment-882717016:64,Deployability,release,release,64,Looks good to me. Will the flags will stay the same in the next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/473#issuecomment-882717016
https://github.com/google/deepvariant/pull/473#issuecomment-882719687:61,Deployability,release,release,61,"@williamrowell yes, the flags will stay the same in the next release for DeepVariant, and we're adding this to DeepTrio too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/473#issuecomment-882719687
https://github.com/google/deepvariant/issues/474#issuecomment-883613731:482,Availability,avail,available,482,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731
https://github.com/google/deepvariant/issues/474#issuecomment-883613731:663,Performance,optimiz,optimize,663,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731
https://github.com/google/deepvariant/issues/474#issuecomment-883613731:437,Usability,simpl,simple,437,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731
https://github.com/google/deepvariant/issues/474#issuecomment-885520428:728,Performance,load,load,728,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :; - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add; - Our cluster runs pbs so I need to create a submission script for this.; - We also have singularity; - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a; my reference sequence in path_b; my pacbio bam in path_c. Could you advise on the singularity command to execute this job?. Below is what i've tried but I must be missing and misunderstanding a few key elements.; =====; module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO; --ref=""References/Panu3.0_X_Y_Mito.fa"" \; --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \; --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \; --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \; ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-885520428
https://github.com/google/deepvariant/issues/474#issuecomment-885791994:346,Availability,error,error,346,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:; https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-885791994
https://github.com/google/deepvariant/issues/474#issuecomment-885791994:352,Integrability,message,messages,352,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:; https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474#issuecomment-885791994
https://github.com/google/deepvariant/issues/475#issuecomment-892390206:759,Availability,avail,available,759,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
https://github.com/google/deepvariant/issues/475#issuecomment-892390206:118,Deployability,update,update,118,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
https://github.com/google/deepvariant/issues/475#issuecomment-892390206:182,Deployability,release,release,182,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
https://github.com/google/deepvariant/issues/475#issuecomment-892390206:664,Usability,simpl,simply,664,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206
https://github.com/google/deepvariant/issues/475#issuecomment-894387782:16,Deployability,update,update,16,"Hi @maca8e . To update this issue, we have now made the change to DeepVariant_unfiltered GLnexus preset in the DeepTrio documentation. Thank you again for pointing this out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-894387782
https://github.com/google/deepvariant/issues/475#issuecomment-898812475:241,Modifiability,config,config,241,"Hi @AndrewCarroll . Thanks for pointing out the preprint, I hadn't come across that yet so it helped a great deal understanding the differences between the parent-child models and the training. Glad to hear that the `DeepVariant_unfiltered` config is now the recommended preset. Many thanks,; Macabe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475#issuecomment-898812475
https://github.com/google/deepvariant/issues/476#issuecomment-894671300:191,Availability,error,error,191,@williambrandler can you provide more detail regarding what you are trying to do?. Have you modified the existing dockerfile to replace the base image with the Databricks runtime image?. The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-894671300
https://github.com/google/deepvariant/issues/476#issuecomment-894671300:255,Availability,avail,available,255,@williambrandler can you provide more detail regarding what you are trying to do?. Have you modified the existing dockerfile to replace the base image with the Databricks runtime image?. The error appears to indicate that the `apt` package manager is not available in the Databricks runtime image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-894671300
https://github.com/google/deepvariant/issues/476#issuecomment-895630749:205,Deployability,install,install,205,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,; ARG FROM_IMAGE=ubuntu:20.04; with,; ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, ; `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-895630749
https://github.com/google/deepvariant/issues/476#issuecomment-896274508:96,Deployability,upgrade,upgrade,96,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896274508
https://github.com/google/deepvariant/issues/476#issuecomment-896286964:301,Deployability,install,installs,301,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964
https://github.com/google/deepvariant/issues/476#issuecomment-896286964:37,Integrability,depend,dependencies,37,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964
https://github.com/google/deepvariant/issues/476#issuecomment-896286964:321,Integrability,depend,dependencies,321,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964
https://github.com/google/deepvariant/issues/476#issuecomment-896286964:125,Testability,test,tested,125,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:268,Deployability,update,updated,268,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:456,Deployability,update,update,456,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:733,Deployability,install,installs,733,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:1175,Deployability,install,installs,1175,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:1321,Deployability,install,installed,1321,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:1486,Deployability,install,installed,1486,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:39,Integrability,depend,dependencies,39,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:753,Integrability,depend,dependencies,753,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:127,Testability,test,tested,127,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:297,Testability,test,tested,297,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896293199:1547,Usability,clear,clear,1547,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199
https://github.com/google/deepvariant/issues/476#issuecomment-896302178:170,Deployability,upgrade,upgraded,170,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. ; Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-896302178
https://github.com/google/deepvariant/issues/476#issuecomment-897826566:55,Deployability,install,installation,55,Hello! I also need to run DV on Ubuntu 18.04.; But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-897826566
https://github.com/google/deepvariant/issues/476#issuecomment-897826566:121,Deployability,install,installed,121,Hello! I also need to run DV on Ubuntu 18.04.; But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-897826566
https://github.com/google/deepvariant/issues/476#issuecomment-897826566:208,Deployability,install,installation,208,Hello! I also need to run DV on Ubuntu 18.04.; But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-897826566
https://github.com/google/deepvariant/issues/476#issuecomment-901422710:139,Deployability,install,installed,139,"Sorry for the delay.; I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-901422710
https://github.com/google/deepvariant/issues/476#issuecomment-901422710:177,Deployability,install,installation,177,"Sorry for the delay.; I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-901422710
https://github.com/google/deepvariant/issues/476#issuecomment-901424155:23,Deployability,install,installation,23,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-901424155
https://github.com/google/deepvariant/issues/476#issuecomment-901424155:104,Deployability,install,installation,104,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-901424155
https://github.com/google/deepvariant/issues/476#issuecomment-901424155:269,Deployability,install,installation,269,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-901424155
https://github.com/google/deepvariant/issues/476#issuecomment-902118100:360,Testability,log,log,360,"Hi @serge2016 ,; If you have a setup that works for Ubuntu20.04 as well as Ubuntu18.04, I'd be happy to use it as our default.; You can send a pull request with your changes that works for both Ubuntu20.04 and Ubuntu18.04. Even though we can't directly merge pull requests, I can make corresponding internal changes and mention your contribution in the commit log.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902118100
https://github.com/google/deepvariant/issues/476#issuecomment-902138384:223,Deployability,Install,Install,223,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384
https://github.com/google/deepvariant/issues/476#issuecomment-902138384:568,Testability,test,test,568,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384
https://github.com/google/deepvariant/issues/476#issuecomment-902138384:505,Usability,simpl,simple,505,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384
https://github.com/google/deepvariant/issues/476#issuecomment-902142885:105,Deployability,install,install,105,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902142885
https://github.com/google/deepvariant/issues/476#issuecomment-902142885:333,Deployability,install,installed,333,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902142885
https://github.com/google/deepvariant/issues/476#issuecomment-902142885:486,Deployability,install,installed,486,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476#issuecomment-902142885
https://github.com/google/deepvariant/issues/479#issuecomment-903356421:232,Safety,avoid,avoid,232,"Hi @DLPerf , thanks for looking into this.; I can look into your suggestions above, test out the changes, and report back.; If you want to just create a PR with your fixes suggested above, that will certainly make my job easier and avoid miscommunication. Note that our repo isn't quite setup to directly take external pull requests, but if you create one, I can create a corresponding internal commit and point to your PR when we commit it. Let me know if you want to create a PR, or just for me to try to follow your suggestion above. Either way works. If you'll create a PR, I'll wait for that first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-903356421
https://github.com/google/deepvariant/issues/479#issuecomment-903356421:84,Testability,test,test,84,"Hi @DLPerf , thanks for looking into this.; I can look into your suggestions above, test out the changes, and report back.; If you want to just create a PR with your fixes suggested above, that will certainly make my job easier and avoid miscommunication. Note that our repo isn't quite setup to directly take external pull requests, but if you create one, I can create a corresponding internal commit and point to your PR when we commit it. Let me know if you want to create a PR, or just for me to try to follow your suggestion above. Either way works. If you'll create a PR, I'll wait for that first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-903356421
https://github.com/google/deepvariant/issues/479#issuecomment-903990739:151,Deployability,update,update,151,"Thanks @DLPerf .; Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-903990739
https://github.com/google/deepvariant/issues/479#issuecomment-903990739:185,Performance,perform,performance,185,"Thanks @DLPerf .; Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-903990739
https://github.com/google/deepvariant/issues/479#issuecomment-905196306:549,Deployability,Update,Update,549,"Hi @DLPerf ,; I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. ; (Update: I might be making a bit more progress here.); But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-905196306
https://github.com/google/deepvariant/issues/479#issuecomment-905684335:36,Deployability,update,update,36,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335
https://github.com/google/deepvariant/issues/479#issuecomment-905684335:2117,Deployability,update,update,2117,", I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335
https://github.com/google/deepvariant/issues/479#issuecomment-905684335:1935,Energy Efficiency,efficient,efficient,1935,", I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335
https://github.com/google/deepvariant/issues/479#issuecomment-905684335:707,Testability,test,test,707,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335
https://github.com/google/deepvariant/issues/479#issuecomment-905684335:785,Testability,test,test,785,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:37,Availability,error,error,37,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:43,Integrability,message,message,43,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:125,Testability,test,tested,125,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:332,Testability,test,testdata,332,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:408,Testability,test,testdata,408,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:465,Testability,test,testdata,465,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:518,Testability,test,testdata,518,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-919633316:570,Testability,test,testdata,570,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316
https://github.com/google/deepvariant/issues/481#issuecomment-920082344:37,Testability,test,tested,37,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-920082344
https://github.com/google/deepvariant/issues/481#issuecomment-920082344:82,Testability,test,tested,82,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/481#issuecomment-920082344
https://github.com/google/deepvariant/issues/482#issuecomment-925047566:19,Testability,log,log,19,"@kirti141 from the log, I agree that it isn't quite clear. ; Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925047566
https://github.com/google/deepvariant/issues/482#issuecomment-925047566:52,Usability,clear,clear,52,"@kirti141 from the log, I agree that it isn't quite clear. ; Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925047566
https://github.com/google/deepvariant/issues/482#issuecomment-925379521:194,Availability,error,error,194,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521
https://github.com/google/deepvariant/issues/482#issuecomment-925379521:321,Testability,log,log,321,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521
https://github.com/google/deepvariant/issues/482#issuecomment-925379521:357,Usability,clear,clear,357,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521
https://github.com/google/deepvariant/issues/482#issuecomment-925383292:146,Deployability,release,release,146,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925383292
https://github.com/google/deepvariant/issues/482#issuecomment-925386459:294,Deployability,release,release,294,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>; wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with; > less RAM you had.; >; > Another known issue (which we will fix in the next release) is that if; > auxiliary fields are being read in, and if the BAM has many of them, it; > could unnecessarily increase the RAM usage. However, in this case given; > that you're using WGS model (which by default isn't parsing aux fields),; > that shouldn't be an issue...; >; > @kirti141 <https://github.com/kirti141> Another question for you - is; > there a BAM file that you can make public (with no sensitive information,; > of course) that we can attempt to reproduce this issue? Thanks again for; > reporting this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925386459
https://github.com/google/deepvariant/issues/482#issuecomment-925387861:512,Deployability,release,release,512,"I have no problem in making my .bam file public, but please help me where; to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM.; >; > Coverage is 46x; >; > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> Hmm, empirically we've been able to handle larger input BAM than 20GB; >> with less RAM you had.; >>; >> Another known issue (which we will fix in the next release) is that if; >> auxiliary fields are being read in, and if the BAM has many of them, it; >> could unnecessarily increase the RAM usage. However, in this case given; >> that you're using WGS model (which by default isn't parsing aux fields),; >> that shouldn't be an issue...; >>; >> @kirti141 <https://github.com/kirti141> Another question for you - is; >> there a BAM file that you can make public (with no sensitive information,; >> of course) that we can attempt to reproduce this issue? Thanks again for; >> reporting this.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>; >> .; >> Triage notifications on the go with GitHub Mobile for iOS; >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; >> or Android; >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925387861
https://github.com/google/deepvariant/issues/482#issuecomment-925387861:126,Testability,test,test,126,"I have no problem in making my .bam file public, but please help me where; to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM.; >; > Coverage is 46x; >; > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> Hmm, empirically we've been able to handle larger input BAM than 20GB; >> with less RAM you had.; >>; >> Another known issue (which we will fix in the next release) is that if; >> auxiliary fields are being read in, and if the BAM has many of them, it; >> could unnecessarily increase the RAM usage. However, in this case given; >> that you're using WGS model (which by default isn't parsing aux fields),; >> that shouldn't be an issue...; >>; >> @kirti141 <https://github.com/kirti141> Another question for you - is; >> there a BAM file that you can make public (with no sensitive information,; >> of course) that we can attempt to reproduce this issue? Thanks again for; >> reporting this.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>; >> .; >> Triage notifications on the go with GitHub Mobile for iOS; >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; >> or Android; >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925387861
https://github.com/google/deepvariant/issues/482#issuecomment-925428637:252,Testability,test,testdata,252,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; ```; on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925428637
https://github.com/google/deepvariant/issues/482#issuecomment-925428637:346,Testability,test,testdata,346,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; ```; on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925428637
https://github.com/google/deepvariant/issues/482#issuecomment-925432605:22,Testability,test,test,22,"It might work on your test data, but not working on all my samples...I; tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing); > turns out to be more complicated than I thought. I'll have to think about; > what's a best way for this.; >; > For now, I can try to run DV 1.2 on this large BAM file:; >; > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; >; > on a 64 cores CentOS machine, to see if I can reproduce the issue.; >; > I'll report back after I have a chance to run it.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925432605
https://github.com/google/deepvariant/issues/482#issuecomment-925432605:513,Testability,test,testdata,513,"It might work on your test data, but not working on all my samples...I; tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing); > turns out to be more complicated than I thought. I'll have to think about; > what's a best way for this.; >; > For now, I can try to run DV 1.2 on this large BAM file:; >; > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; >; > on a 64 cores CentOS machine, to see if I can reproduce the issue.; >; > I'll report back after I have a chance to run it.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925432605
https://github.com/google/deepvariant/issues/482#issuecomment-925432605:609,Testability,test,testdata,609,"It might work on your test data, but not working on all my samples...I; tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing); > turns out to be more complicated than I thought. I'll have to think about; > what's a best way for this.; >; > For now, I can try to run DV 1.2 on this large BAM file:; >; > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; >; > on a 64 cores CentOS machine, to see if I can reproduce the issue.; >; > I'll report back after I have a chance to run it.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482#issuecomment-925432605
https://github.com/google/deepvariant/issues/483#issuecomment-917319405:318,Integrability,protocol,protocol,318,"It might have something to do with the `--regions`. Can you try just running on just one small region, something like '--regions=""chr20:10000000-10010000""`?; What was the bedtools command you used for making that bed file?; The bed file to use for `--regions` in a WES run should be the exome regions from the capture protocol.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917319405
https://github.com/google/deepvariant/issues/483#issuecomment-917336418:721,Availability,error,error,721,"Hi Maria,; I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :); Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:; bedtools bamtobed [OPTIONS] -i <BAM>; It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me; (""idt_capture_novogene.grch38.bed""). Again I get the error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917336418
https://github.com/google/deepvariant/issues/483#issuecomment-917336418:631,Integrability,protocol,protocol,631,"Hi Maria,; I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :); Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:; bedtools bamtobed [OPTIONS] -i <BAM>; It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me; (""idt_capture_novogene.grch38.bed""). Again I get the error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917336418
https://github.com/google/deepvariant/issues/483#issuecomment-917342943:118,Availability,error,error,118,"Another test-; I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: ; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_res",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917342943
https://github.com/google/deepvariant/issues/483#issuecomment-917342943:645,Availability,Error,Error,645,"Another test-; I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: ; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_res",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917342943
https://github.com/google/deepvariant/issues/483#issuecomment-917342943:8,Testability,test,test,8,"Another test-; I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: ; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_res",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917342943
https://github.com/google/deepvariant/issues/483#issuecomment-917342943:61,Testability,test,test,61,"Another test-; I've ran the WES exactly like it is with your test files (fasta bam and bed) and again I get this same error: ; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3. In this example it also not working for a specific region ""chr20:10000000-10010000"" (some intermediate files are created but not the vcf file). The Error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 1; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions chr20:10000000-10010000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_res",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917342943
https://github.com/google/deepvariant/issues/483#issuecomment-917346497:97,Availability,error,error,97,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917346497
https://github.com/google/deepvariant/issues/483#issuecomment-917346497:103,Integrability,message,message,103,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917346497
https://github.com/google/deepvariant/issues/483#issuecomment-917353570:29,Availability,error,error,29,"You are right I've found the error: ; ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474; Fatal Python error: Aborted"" ; It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917353570
https://github.com/google/deepvariant/issues/483#issuecomment-917353570:321,Availability,error,error,321,"You are right I've found the error: ; ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474; Fatal Python error: Aborted"" ; It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917353570
https://github.com/google/deepvariant/issues/483#issuecomment-917353570:382,Availability,error,error,382,"You are right I've found the error: ; ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474; Fatal Python error: Aborted"" ; It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917353570
https://github.com/google/deepvariant/issues/483#issuecomment-917353570:400,Availability,error,error,400,"You are right I've found the error: ; ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474; Fatal Python error: Aborted"" ; It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917353570
https://github.com/google/deepvariant/issues/483#issuecomment-917353570:328,Safety,Abort,Aborted,328,"You are right I've found the error: ; ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474; Fatal Python error: Aborted"" ; It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917353570
https://github.com/google/deepvariant/issues/483#issuecomment-917355728:251,Availability,down,downloaded,251,"my command:; samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474""; I get this:; >chr1:4655405-4655474; CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC; CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. ; On the current fasta file it writes:. >chr1:4655405-4655474; [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917355728
https://github.com/google/deepvariant/issues/483#issuecomment-917355728:422,Availability,down,download,422,"my command:; samtools faidx GRCh38_no_alt_analysis_set.fasta ""chr1:4655405-4655474""; I get this:; >chr1:4655405-4655474; CCCGCTCCCCGAAACGTGACCATGTGGATTCAACAGCTTTTAGGACTCAGGTGAGCGACC; CGGCCGGCGC. sorry it writes the above on the second fasta file I've downloaded from the WES run. ; On the current fasta file it writes:. >chr1:4655405-4655474; [faidx] Failed to fetch sequence in chr1:4655405-4655474. Could it be that the download of the fasta file sometimes fails?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917355728
https://github.com/google/deepvariant/issues/483#issuecomment-917356380:105,Availability,error,error,105,"> I'm happy to hear you have enjoyed my YouTube videos :); > ; > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names.; > ; > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :); > ; > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?. for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917356380
https://github.com/google/deepvariant/issues/483#issuecomment-917356380:111,Integrability,message,message,111,"> I'm happy to hear you have enjoyed my YouTube videos :); > ; > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names.; > ; > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :); > ; > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?. for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917356380
https://github.com/google/deepvariant/issues/483#issuecomment-917356380:866,Testability,test,test,866,"> I'm happy to hear you have enjoyed my YouTube videos :); > ; > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names.; > ; > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :); > ; > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?. for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917356380
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:30,Availability,Down,Downloaded,30,"A summary of last trials:; 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474); 2. Ran again the WES run with the original files ; 3. Got again the ""parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:91,Availability,error,error,91,"A summary of last trials:; 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474); 2. Ran again the WES run with the original files ; 3. Got again the ""parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:564,Availability,Error,Error,564,"A summary of last trials:; 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474); 2. Ran again the WES run with the original files ; 3. Got again the ""parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:599,Availability,error,error,599,"A summary of last trials:; 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474); 2. Ran again the WES run with the original files ; 3. Got again the ""parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:1397,Availability,Error,Error,1397,"ermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:3047,Availability,error,error,3047,"output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op.; 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2); INFO:tensorflow:Reloading EMA...; I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]; ```. ```; real	0m53.331s; user	0m35.346s; sys	0m22.537s; ```; It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:605,Integrability,message,message,605,"A summary of last trials:; 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474); 2. Ran again the WES run with the original files ; 3. Got again the ""parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:1403,Integrability,message,message,1403,"ermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917442547:2514,Performance,Optimiz,Optimization,2514,"output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op.; 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2); INFO:tensorflow:Reloading EMA...; I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]; ```. ```; real	0m53.331s; user	0m35.346s; sys	0m22.537s; ```; It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547
https://github.com/google/deepvariant/issues/483#issuecomment-917501282:23,Availability,error,error,23,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282
https://github.com/google/deepvariant/issues/483#issuecomment-917501282:266,Availability,error,error,266,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282
https://github.com/google/deepvariant/issues/483#issuecomment-917501282:29,Integrability,message,message,29,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282
https://github.com/google/deepvariant/issues/483#issuecomment-917501282:272,Integrability,message,messages,272,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282
https://github.com/google/deepvariant/issues/483#issuecomment-917501282:380,Testability,test,testing,380,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282
https://github.com/google/deepvariant/issues/483#issuecomment-917606346:211,Availability,down,downloaded,211,"Thank you so much Maria! It finally worked. I've changed the docker memory settings to be 32 GB, it was indeed the problem. The fasta file still had problems of EOF each time at different position, however I've downloaded it again and again until it finally worked !. Thank you again for the support :) !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483#issuecomment-917606346
https://github.com/google/deepvariant/issues/485#issuecomment-920069873:100,Availability,error,error,100,"Hi, @MariaNattestad . Thank you very much for your kind advice. I have figured out what caused this error and it did work. Additionally, I don't understand how DeepVariant defines those candidate variants when making examples. I know it is implemented by the file ""allelecounter.cilf"", but I get limited information from this file. ; Could you please tell me more information about the candidate variants part?. Thank you again for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485#issuecomment-920069873
https://github.com/google/deepvariant/issues/485#issuecomment-1050345036:66,Availability,error,error,66,@leedchou can you share what you figured out was the cause of the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485#issuecomment-1050345036
https://github.com/google/deepvariant/issues/486#issuecomment-984133927:541,Availability,error,errors,541,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
https://github.com/google/deepvariant/issues/486#issuecomment-984133927:587,Deployability,pipeline,pipeline,587,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
https://github.com/google/deepvariant/issues/486#issuecomment-984133927:677,Modifiability,polymorphi,polymorphisms,677,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
https://github.com/google/deepvariant/issues/486#issuecomment-984133927:453,Safety,predict,predictors,453,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
https://github.com/google/deepvariant/issues/486#issuecomment-984133927:661,Safety,predict,predict,661,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
https://github.com/google/deepvariant/issues/486#issuecomment-984133927:793,Usability,clear,clear,793,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927
https://github.com/google/deepvariant/issues/487#issuecomment-927384413:185,Deployability,release,release,185,"Hi @edg1983 , thanks for bringing up this issue!; We have already been looking into this, and have already made a few internal fixes (done by @akolesnikov) that will be out in the next release.; I'm closing this issue for now. Feel free to comment or reopen if you have more questions or suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/487#issuecomment-927384413
https://github.com/google/deepvariant/issues/488#issuecomment-929837599:481,Availability,down,down,481,"Note that not all our DeepTrio models use the default height in the code (which is the 100+100+100=300 that @danielecook is referring to). See here:; https://github.com/google/deepvariant/blob/r1.2/scripts/run_deeptrio.py#L181-L184. @Suke-fudan please considering using the one-step `run_deeptrio.py` script for this. See this section:; https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command. And, if you really want to break it down to multiple steps, you can still use run_deeptrio.py and add `--dry_run=true` to see the command breakdown. So, basically for PACBIO DeepTrio, you'll want to set:; `--pileup_image_height_child=60` and `--pileup_image_height_parent=40`; in your make_examples stage.; ; Does this make sense? Let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-929837599
https://github.com/google/deepvariant/issues/488#issuecomment-929838628:225,Availability,robust,robust,225,"Oh, btw, the warning message in call_variants.py about image height might be confusing.; It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command; which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-929838628
https://github.com/google/deepvariant/issues/488#issuecomment-929838628:449,Availability,robust,robust,449,"Oh, btw, the warning message in call_variants.py about image height might be confusing.; It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command; which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-929838628
https://github.com/google/deepvariant/issues/488#issuecomment-929838628:21,Integrability,message,message,21,"Oh, btw, the warning message in call_variants.py about image height might be confusing.; It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command; which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-929838628
https://github.com/google/deepvariant/issues/488#issuecomment-929845497:132,Deployability,release,release,132,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-929845497
https://github.com/google/deepvariant/issues/488#issuecomment-929845497:455,Deployability,release,release,455,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-929845497
https://github.com/google/deepvariant/issues/488#issuecomment-930473071:159,Performance,perform,perform,159,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-930473071
https://github.com/google/deepvariant/issues/488#issuecomment-930473071:187,Performance,perform,performed,187,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-930473071
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1618,Availability,error,errors,1618,"put ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam""; --reference ${ref}; ${outdir2}/${sample}_deepvariant1.phased.vcf.gz; ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling; #DeepTrio version:1.2.0; /opt/deepvariant/bin/deeptrio/run_deeptrio; --model_type PACBIO; --ref ${ref}; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1684,Availability,error,error,1684,"put ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam""; --reference ${ref}; ${outdir2}/${sample}_deepvariant1.phased.vcf.gz; ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling; #DeepTrio version:1.2.0; /opt/deepvariant/bin/deeptrio/run_deeptrio; --model_type PACBIO; --ref ${ref}; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1985,Availability,error,error,1985,"; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:2481,Availability,error,error,2481,"t2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:2977,Availability,error,error,2977,"se are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:6575,Availability,reliab,reliable,6575,"g_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 655, in main; raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed. **I am very confusing :Are the result files reliable?Or should I rerun DeepTrio?**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:3234,Safety,sanity check,sanity check,3234,"29842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:3300,Safety,sanity check,sanity check,3300,"ot 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants; var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:5062,Safety,predict,predictions,5062,"n; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none; return next(iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:5292,Safety,sanity check,sanity check,5292,"ariants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none; return next(iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:5358,Safety,sanity check,sanity check,5358,"l.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none; return next(iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1582,Testability,log,log,1582,"put ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam""; --reference ${ref}; ${outdir2}/${sample}_deepvariant1.phased.vcf.gz; ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling; #DeepTrio version:1.2.0; /opt/deepvariant/bin/deeptrio/run_deeptrio; --model_type PACBIO; --ref ${ref}; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1671,Testability,log,log,1671,"put ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam""; --reference ${ref}; ${outdir2}/${sample}_deepvariant1.phased.vcf.gz; ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling; #DeepTrio version:1.2.0; /opt/deepvariant/bin/deeptrio/run_deeptrio; --model_type PACBIO; --ref ${ref}; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-939618673:3327,Testability,log,log,3327,"ouble-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/TMP_DIR/Bazel.runfiles_eyn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673
https://github.com/google/deepvariant/issues/488#issuecomment-940505896:262,Usability,clear,clear,262,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:; - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total).; - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940505896
https://github.com/google/deepvariant/issues/488#issuecomment-940586642:129,Availability,error,error,129,"Thanks for your help! I can ignore the warning about ""height"".; However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:; **raise ValueError('call_variants_outputs did not pass sanity check.'); ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed.**. How should I deal with these errors？",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940586642
https://github.com/google/deepvariant/issues/488#issuecomment-940586642:207,Availability,error,errors,207,"Thanks for your help! I can ignore the warning about ""height"".; However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:; **raise ValueError('call_variants_outputs did not pass sanity check.'); ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed.**. How should I deal with these errors？",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940586642
https://github.com/google/deepvariant/issues/488#issuecomment-940586642:561,Availability,error,errors,561,"Thanks for your help! I can ignore the warning about ""height"".; However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:; **raise ValueError('call_variants_outputs did not pass sanity check.'); ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed.**. How should I deal with these errors？",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940586642
https://github.com/google/deepvariant/issues/488#issuecomment-940586642:338,Safety,sanity check,sanity check,338,"Thanks for your help! I can ignore the warning about ""height"".; However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:; **raise ValueError('call_variants_outputs did not pass sanity check.'); ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed.**. How should I deal with these errors？",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940586642
https://github.com/google/deepvariant/issues/488#issuecomment-940586642:402,Safety,sanity check,sanity check,402,"Thanks for your help! I can ignore the warning about ""height"".; However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:; **raise ValueError('call_variants_outputs did not pass sanity check.'); ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed.**. How should I deal with these errors？",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-940586642
https://github.com/google/deepvariant/issues/488#issuecomment-953896379:46,Availability,error,error,46,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome?. Are you able to identify which region of the genome this error is occurring at?. Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-953896379
https://github.com/google/deepvariant/issues/488#issuecomment-953896379:193,Availability,error,error,193,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome?. Are you able to identify which region of the genome this error is occurring at?. Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-953896379
https://github.com/google/deepvariant/issues/488#issuecomment-953896379:289,Availability,error,error,289,@Suke-fudan - I am not entirely sure why this error has come up. Can you double check that you are using the correct reference genome?. Are you able to identify which region of the genome this error is occurring at?. Would you be comfortable sharing a file that allows us to reporduce the error?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-953896379
https://github.com/google/deepvariant/issues/488#issuecomment-964829927:102,Availability,error,error,102,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964829927
https://github.com/google/deepvariant/issues/488#issuecomment-964829927:108,Integrability,message,message,108,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964829927
https://github.com/google/deepvariant/issues/488#issuecomment-964833815:28,Deployability,update,update,28,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815
https://github.com/google/deepvariant/issues/488#issuecomment-964833815:106,Deployability,release,release,106,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815
https://github.com/google/deepvariant/issues/488#issuecomment-964833815:192,Deployability,update,update,192,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815
https://github.com/google/deepvariant/issues/488#issuecomment-964833815:84,Integrability,message,message,84,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815
https://github.com/google/deepvariant/issues/488#issuecomment-964833815:211,Integrability,message,message,211,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815
https://github.com/google/deepvariant/issues/488#issuecomment-964833815:203,Testability,log,logging,203,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815
https://github.com/google/deepvariant/issues/489#issuecomment-940166600:438,Deployability,INSTALL,INSTALL,438,"Thanks @Stikus , I noticed this and was just looking at it!. I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:; ```; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; ```. and then , right in front of the line of `./INSTALL.sh` , add this line:; ```; sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake; ```; Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-940166600
https://github.com/google/deepvariant/issues/489#issuecomment-940166600:604,Deployability,install,installation,604,"Thanks @Stikus , I noticed this and was just looking at it!. I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:; ```; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; ```. and then , right in front of the line of `./INSTALL.sh` , add this line:; ```; sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake; ```; Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-940166600
https://github.com/google/deepvariant/issues/489#issuecomment-940166600:172,Modifiability,Config,Configure,172,"Thanks @Stikus , I noticed this and was just looking at it!. I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:; ```; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; ```. and then , right in front of the line of `./INSTALL.sh` , add this line:; ```; sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake; ```; Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-940166600
https://github.com/google/deepvariant/issues/489#issuecomment-940357535:16,Deployability,patch,patch,16,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-940357535
https://github.com/google/deepvariant/issues/489#issuecomment-942094050:5,Testability,test,tested,5,I've tested your solution - it works on Ubuntu 20.04 but doesn't work on Ubuntu 18.04. Still looking for solution,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-942094050
https://github.com/google/deepvariant/issues/489#issuecomment-942303491:195,Deployability,install,installation,195,Finally fixed it - looks like fix was on LLVM side:; ![image](https://user-images.githubusercontent.com/41360525/137140712-84f012cb-b7b6-4d7a-a08b-22fd20ec93ed.png). I have no changes in our old installation and all working again (except pinning `jsonschema==3.2.0` as you've done [here](https://github.com/google/deepvariant/commit/fd02fa3ab8fa1d161e23d10c9931641d7ab1dcad) ),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-942303491
https://github.com/google/deepvariant/issues/489#issuecomment-943865449:245,Deployability,update,updated,245,Thanks @Stikus . Glad it's working. Internally I've made another change similar to https://github.com/google/clif/commit/83c7941aae9f01a575adc719b1baed3c6607b8f4 which should work for 18.04. I haven't pushed that to GitHub but will hopefully be updated to our r1.2 as well.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489#issuecomment-943865449
https://github.com/google/deepvariant/issues/490#issuecomment-948755113:43,Integrability,message,message,43,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem.; 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md.; I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948755113
https://github.com/google/deepvariant/issues/490#issuecomment-948869111:303,Availability,error,errors,303,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:; `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948869111
https://github.com/google/deepvariant/issues/490#issuecomment-948869111:365,Availability,error,errors,365,"You are definitely following the recommended path for someone starting with `reads.bam`: `reads.bam` -> `extracthifi` -> `hifi_reads.bam` -> `pbmm2` -> `hifi_reads.aligned.bam` -> DeepVariant. The consensus kinetics tags are relatively recent additions, but we have noticed that these seem to cause OOM errors with `make_examples`, and if we strip these tags these errors seem to go away. A short term fix would be to remove these tags to produce a BAM to be used as input for DeepVariant:; `samtools view -b -x fi -x fp -x ri -x rp hifi_reads.aligned.bam > hifi_reads.aligned.nokinetics.bam`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948869111
https://github.com/google/deepvariant/issues/490#issuecomment-948982282:58,Availability,error,error,58,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282
https://github.com/google/deepvariant/issues/490#issuecomment-948982282:149,Availability,error,error,149,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282
https://github.com/google/deepvariant/issues/490#issuecomment-948982282:292,Availability,error,error,292,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282
https://github.com/google/deepvariant/issues/490#issuecomment-948982282:370,Deployability,pipeline,pipeline,370,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282
https://github.com/google/deepvariant/issues/490#issuecomment-948982282:175,Testability,test,test,175,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282
https://github.com/google/deepvariant/issues/490#issuecomment-948986764:12,Deployability,release,release,12,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948986764
https://github.com/google/deepvariant/issues/490#issuecomment-948986764:39,Deployability,update,update,39,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490#issuecomment-948986764
https://github.com/google/deepvariant/issues/491#issuecomment-960053107:16,Deployability,update,update,16,"Thanks you!. An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960053107
https://github.com/google/deepvariant/issues/491#issuecomment-960053107:29,Energy Efficiency,allocate,allocated,29,"Thanks you!. An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960053107
https://github.com/google/deepvariant/issues/491#issuecomment-960065189:58,Availability,avail,available,58,"@SHuang-Broad , are you using a BAM file that is publicly available?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960065189
https://github.com/google/deepvariant/issues/491#issuecomment-960089166:491,Performance,load,loads,491,"Unfortunately, it's not... Providing what I can:; It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:; ```; chr10+chr14-p; chr4-p+chr5-p_chr11-p; chr7-q+chr16-q; ```; all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960089166
https://github.com/google/deepvariant/issues/491#issuecomment-960110290:150,Deployability,pipeline,pipeline,150,"@SHuang-Broad , yes, I'd suggest subsampling the BAM file with mapping quality of Q10 or Q20 if coverage is that high and then try again. I think the pipeline is getting stuck at the centromere. Do you happen to know the read N50?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960110290
https://github.com/google/deepvariant/issues/491#issuecomment-960130177:329,Availability,failure,failure,329,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960130177
https://github.com/google/deepvariant/issues/491#issuecomment-960130177:346,Testability,log,logging,346,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960130177
https://github.com/google/deepvariant/issues/491#issuecomment-960166365:55,Availability,error,errors,55,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365
https://github.com/google/deepvariant/issues/491#issuecomment-960166365:142,Availability,error,error,142,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365
https://github.com/google/deepvariant/issues/491#issuecomment-960166365:187,Availability,error,error,187,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365
https://github.com/google/deepvariant/issues/491#issuecomment-960166365:228,Availability,error,error,228,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365
https://github.com/google/deepvariant/issues/491#issuecomment-960166365:296,Availability,error,error,296,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365
https://github.com/google/deepvariant/issues/491#issuecomment-960166365:594,Energy Efficiency,allocate,allocated,594,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365
https://github.com/google/deepvariant/issues/491#issuecomment-960186921:77,Energy Efficiency,monitor,monitor,77,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960186921
https://github.com/google/deepvariant/issues/491#issuecomment-960911881:3,Deployability,update,update,3,"An update:. I've tried increasing both memory and disk, and it has worked!; Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960911881
https://github.com/google/deepvariant/issues/491#issuecomment-960911881:178,Deployability,pipeline,pipeline,178,"An update:. I've tried increasing both memory and disk, and it has worked!; Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960911881
https://github.com/google/deepvariant/issues/491#issuecomment-960911881:216,Deployability,pipeline,pipeline,216,"An update:. I've tried increasing both memory and disk, and it has worked!; Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960911881
https://github.com/google/deepvariant/issues/491#issuecomment-960911881:352,Deployability,pipeline,pipeline,352,"An update:. I've tried increasing both memory and disk, and it has worked!; Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-960911881
https://github.com/google/deepvariant/issues/491#issuecomment-961546511:100,Modifiability,layers,layers,100,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-961546511
https://github.com/google/deepvariant/issues/491#issuecomment-961546511:394,Usability,simpl,simple,394,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-961546511
https://github.com/google/deepvariant/issues/491#issuecomment-966527557:149,Deployability,release,release,149,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. ; I've made two changes internally (which will come out in the next release):; 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead.; 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-966527557
https://github.com/google/deepvariant/issues/491#issuecomment-966527557:555,Deployability,release,release,555,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. ; I've made two changes internally (which will come out in the next release):; 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead.; 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-966527557
https://github.com/google/deepvariant/issues/491#issuecomment-966527557:62,Testability,log,logging,62,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. ; I've made two changes internally (which will come out in the next release):; 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead.; 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-966527557
https://github.com/google/deepvariant/issues/491#issuecomment-966527557:454,Testability,test,test,454,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. ; I've made two changes internally (which will come out in the next release):; 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead.; 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-966527557
https://github.com/google/deepvariant/issues/491#issuecomment-966527557:591,Testability,log,log,591,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. ; I've made two changes internally (which will come out in the next release):; 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead.; 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-966527557
https://github.com/google/deepvariant/issues/491#issuecomment-1004469728:54,Testability,log,log,54,@pichuan thanks for the work!; I can confirm that the log file is indeed much smaller now with v1.3.0.; And it's running faster.; Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491#issuecomment-1004469728
https://github.com/google/deepvariant/issues/492#issuecomment-961288846:105,Availability,error,error,105,"I ran it with a different dataset with the same command just changed the files, I am getting a different error; This is the command I have; -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies"":""/input"" \; -v ""/media/eniac/WD1/HiFi_Dikaryon_data/Hifi_Assemblies/docker_out:/output"" \; google/deepvariant:1.2.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/G1-hifi.contigs.fasta \; --reads=/input/G1_sorted.bam \; --output_vcf=/output/output.vcf \; --num_shards=4. [E::hts_open_format] Failed to open file ""/input/G1_sorted.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 173, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 159, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 121, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 80, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 128, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ai032aw7/runfiles/com_google_deepvariant/th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492#issuecomment-961288846
https://github.com/google/deepvariant/issues/492#issuecomment-1821236290:68,Testability,test,test,68,"Hi @luciamayorf , can you share your command?. And, can you quickly test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md and see if that worked for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/492#issuecomment-1821236290
https://github.com/google/deepvariant/issues/494#issuecomment-1018054666:277,Safety,predict,prediction,277,"Hi @Suke-fudan , ; to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:; ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:; https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018054666
https://github.com/google/deepvariant/issues/494#issuecomment-1018054666:388,Safety,predict,prediction,388,"Hi @Suke-fudan , ; to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:; ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:; https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018054666
https://github.com/google/deepvariant/issues/494#issuecomment-1018054666:419,Testability,log,logic,419,"Hi @Suke-fudan , ; to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:; ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:; https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018054666
https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:3493,Safety,safe,safely,3493,"space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation; -- | -- | --; . | N/A | corresponding allele is called; M | Missing data | input (gVCF) had no data at this genome position; P | Partial data | input only partially covered this genome position; D | Depth | read depth too low to call; – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s).; L | Lost allele | ^ but other than deletion allele; U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s).; O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s).; 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->; </body>; </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:3699,Safety,safe,safely,3699,"space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation; -- | -- | --; . | N/A | corresponding allele is called; M | Missing data | input (gVCF) had no data at this genome position; P | Partial data | input only partially covered this genome position; D | Depth | read depth too low to call; – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s).; L | Lost allele | ^ but other than deletion allele; U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s).; O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s).; 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->; </body>; </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:1461,Testability,assert,assertion,1461,"st probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage.""; So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)?; When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1.; However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II.; In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)?. ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs; One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. ; <html>; <body>; <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:3931,Testability,assert,assertion,3931,"space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation; -- | -- | --; . | N/A | corresponding allele is called; M | Missing data | input (gVCF) had no data at this genome position; P | Partial data | input only partially covered this genome position; D | Depth | read depth too low to call; – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s).; L | Lost allele | ^ but other than deletion allele; U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s).; O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s).; 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->; </body>; </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:277,Usability,learn,learned,277,"Thank you!; You are right.The ""RNC"" comes from GLnexus. ; https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage.""; So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)?; When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1.; However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II.; In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)?. ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs; One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. ; <html>; <body>; <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-tran",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954
https://github.com/google/deepvariant/issues/494#issuecomment-1038521141:992,Deployability,pipeline,pipeline,992,"Hi @Suke-fudan ,; Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1038521141
https://github.com/google/deepvariant/issues/494#issuecomment-1038521141:566,Security,expose,exposed,566,"Hi @Suke-fudan ,; Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/494#issuecomment-1038521141
https://github.com/google/deepvariant/issues/495#issuecomment-981766539:106,Deployability,update,update,106,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do?. [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495#issuecomment-981766539
https://github.com/google/deepvariant/issues/495#issuecomment-981766539:741,Deployability,update,update,741,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do?. [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495#issuecomment-981766539
https://github.com/google/deepvariant/issues/495#issuecomment-981766539:394,Testability,log,logic,394,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do?. [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495#issuecomment-981766539
https://github.com/google/deepvariant/issues/495#issuecomment-981903611:288,Modifiability,flexible,flexible,288,"Hi @ed5152 ,; Thanks for your question.; Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes.; In the future we might support more flexible number of classes, but this is not currently on our roadmap.; Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/495#issuecomment-981903611
https://github.com/google/deepvariant/issues/496#issuecomment-990103583:283,Usability,learn,learning,283,"Hi!. DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing.; There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496#issuecomment-990103583
https://github.com/google/deepvariant/issues/496#issuecomment-992270390:50,Usability,clear,clear,50,"Hello Maria,. Thanks for your reply, this is very clear.; I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/496#issuecomment-992270390
https://github.com/google/deepvariant/issues/497#issuecomment-992114837:18,Availability,error,error,18,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-992114837
https://github.com/google/deepvariant/issues/497#issuecomment-992114837:24,Integrability,message,message,24,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-992114837
https://github.com/google/deepvariant/issues/497#issuecomment-993005113:66,Availability,error,error,66,"Thanks for getting back to me. I have attached the screenshot for error messages:; <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993005113
https://github.com/google/deepvariant/issues/497#issuecomment-993005113:72,Integrability,message,messages,72,"Thanks for getting back to me. I have attached the screenshot for error messages:; <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993005113
https://github.com/google/deepvariant/issues/497#issuecomment-993147588:26,Availability,error,error,26,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588
https://github.com/google/deepvariant/issues/497#issuecomment-993147588:61,Availability,error,error,61,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588
https://github.com/google/deepvariant/issues/497#issuecomment-993147588:701,Availability,mask,mask,701,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588
https://github.com/google/deepvariant/issues/497#issuecomment-993147588:956,Availability,mask,mask,956,"@myonaung I wonder if the error could be something else. The error seems to indicate that setting the thread to be on a specific CPU is the issue. Could you try a few things:. 1) Can you take this code and put it in a file called `affinity.c` and then compile with `gcc affinity.c -lpthread -o affinity`. Then run the program `./affinity` and tell us what you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588
https://github.com/google/deepvariant/issues/497#issuecomment-993147588:2332,Availability,avail,available,2332,"t you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588
https://github.com/google/deepvariant/issues/497#issuecomment-993147588:1813,Deployability,release,release,1813,"t you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:101,Availability,error,error,101,"Hi @pgrosu, thanks very much for the help with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:1459,Deployability,release,release,1459,"; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; Distributor ID: RedHatEnterpriseServer; Description: Red Hat Enterprise Linux Server release 7.9 (Maipo); Release: 7.9; Codename: Maipo; ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:2289,Deployability,release,release,2289,"; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; Distributor ID: RedHatEnterpriseServer; Description: Red Hat Enterprise Linux Server release 7.9 (Maipo); Release: 7.9; Codename: Maipo; ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:2310,Deployability,Release,Release,2310,"; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; Distributor ID: RedHatEnterpriseServer; Description: Red Hat Enterprise Linux Server release 7.9 (Maipo); Release: 7.9; Codename: Maipo; ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:1043,Energy Efficiency,monitor,monitor,1043,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:662,Performance,cache,cache,662,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:678,Performance,cache,cache,678,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:693,Performance,cache,cache,693,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-993155459:709,Performance,cache,cache,709,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459
https://github.com/google/deepvariant/issues/497#issuecomment-996960107:103,Deployability,patch,patched,103,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/497#issuecomment-996960107
https://github.com/google/deepvariant/issues/498#issuecomment-992798611:344,Integrability,Depend,Depending,344,"Hi @zhoudreames ,; Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/498#issuecomment-992798611
https://github.com/google/deepvariant/issues/499#issuecomment-1012212010:195,Deployability,install,installed,195,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010
https://github.com/google/deepvariant/issues/499#issuecomment-1012212010:461,Deployability,install,installed,461,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010
https://github.com/google/deepvariant/issues/499#issuecomment-1012212010:281,Security,access,access,281,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010
https://github.com/google/deepvariant/issues/499#issuecomment-1012497475:330,Deployability,install,installed,330,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7?. Is it possible for you to check (on both machines) what's the different between the protobuff version installed?; (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012497475
https://github.com/google/deepvariant/issues/499#issuecomment-1012497475:385,Security,access,access,385,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7?. Is it possible for you to check (on both machines) what's the different between the protobuff version installed?; (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012497475
https://github.com/google/deepvariant/issues/499#issuecomment-1012803228:15,Deployability,install,installed,15,"Protobuf isn't installed on either host machine. yum list installed | grep proto -> only returns ""xorg-x11-proto-devel"". (I guess it should be included in the Docker container, no?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012803228
https://github.com/google/deepvariant/issues/499#issuecomment-1012803228:58,Deployability,install,installed,58,"Protobuf isn't installed on either host machine. yum list installed | grep proto -> only returns ""xorg-x11-proto-devel"". (I guess it should be included in the Docker container, no?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012803228
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:62,Deployability,install,installed,62,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:204,Deployability,install,installed,204,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:235,Deployability,install,install,235,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:302,Deployability,install,install,302,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:451,Deployability,install,installs,451,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:384,Integrability,protocol,protocolbuffers,384,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:900,Performance,load,loading,900,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056
https://github.com/google/deepvariant/issues/499#issuecomment-1014178969:420,Deployability,install,install,420,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b""; ```. ssh into the machine. ```; gcloud compute ssh ${USER}-centos7 --zone us-west1-b; ```. On the machine, install singularity:; I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```; $ singularity --version; singularity-ce version 3.9.2; ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1014178969
https://github.com/google/deepvariant/issues/499#issuecomment-1014178969:519,Deployability,INSTALL,INSTALL,519,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b""; ```. ssh into the machine. ```; gcloud compute ssh ${USER}-centos7 --zone us-west1-b; ```. On the machine, install singularity:; I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```; $ singularity --version; singularity-ce version 3.9.2; ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1014178969
https://github.com/google/deepvariant/issues/499#issuecomment-1014178969:537,Deployability,install,installation,537,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b""; ```. ssh into the machine. ```; gcloud compute ssh ${USER}-centos7 --zone us-west1-b; ```. On the machine, install singularity:; I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```; $ singularity --version; singularity-ce version 3.9.2; ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/499#issuecomment-1014178969
https://github.com/google/deepvariant/issues/501#issuecomment-1003194651:48,Modifiability,config,config,48,"I've just discovered that when I change glnexus config from `DeepVariantWES` to `DeepVariant` (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity) which seems more probable in relation to 1kg 1.7 mln from 3500 samples. Anyway, would be nice to hear if it sounds reasonable for exome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501#issuecomment-1003194651
https://github.com/google/deepvariant/issues/501#issuecomment-1004215320:352,Modifiability,config,config,352,"Hi @kriestof ; Sorry for the delay of response, we are off work until tomorrow so we haven't responded. If you can help with a few clarifications that will be great:; 1. I don't expect the variants in the VCF from DeepVariant to be 10x less. Are you seeing 10x less variants even just from the VCF files?; 2. You later mentioned ""when I change glnexus config from DeepVariantWES to DeepVariant (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity)"" -- how many calls did you get from 75 samples when you used ""DeepVariantWES""?. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501#issuecomment-1004215320
https://github.com/google/deepvariant/issues/501#issuecomment-1004229605:220,Modifiability,config,config,220,"Hello @pichuan ,. Thank you for your response! Right now I get the right bed file from my data provider. It seems far better: ~320k calls in a single file, while ~420k after joint calling with glnexus and DeepVariantWES config. I remember the first time I got this data Deepvariant has much less calls than GATK, but then calling was made by our data provider and I don't possess any details on that. I can't remind the exact number, but I think it was far below 300k calls. Most of my comparison was based on that *legacy* data. Sorry for that unfair and unchecked comparison. In case you are interested I could try to generate a fresh GATK calling with the same bed file according to the GATK best practices. Otherwise I could let you know if I give a try to GATK in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/501#issuecomment-1004229605
https://github.com/google/deepvariant/issues/502#issuecomment-1007774645:226,Security,access,accessible,226,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md; You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0; You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/502#issuecomment-1007774645
https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:238,Energy Efficiency,Power,PowerPoint,238,"Hi!I did some PCR and Sanger sequencing.; The following mutation can be verified successfully：; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628
https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:297,Energy Efficiency,Power,PowerPoint,297,"Hi!I did some PCR and Sanger sequencing.; The following mutation can be verified successfully：; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628
https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:2406,Energy Efficiency,Power,PowerPoint,2406,"t;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 | 43,0,50; 38 | 38 | 91,0.41 | 38 | 38,0,50; 48 | 48 | 136,0.51 | 48 | 48,0,62; 43 | 43 | 31,0.42 | 19 | 19,0,46; 43 | 43 | 33,0.45 | 41 | 41,0,55; 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->; </body>. </html>. The following mutation validation failed:; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt so",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628
https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:2465,Energy Efficiency,Power,PowerPoint,2465,"to none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 | 43,0,50; 38 | 38 | 91,0.41 | 38 | 38,0,50; 48 | 48 | 136,0.51 | 48 | 48,0,62; 43 | 43 | 31,0.42 | 19 | 19,0,46; 43 | 43 | 33,0.45 | 41 | 41,0,55; 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->; </body>. </html>. The following mutation validation failed:; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:midd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628
https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:2244,Security,validat,validation,2244,"D5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 | 43,0,50; 38 | 38 | 91,0.41 | 38 | 38,0,50; 48 | 48 | 136,0.51 | 48 | 48,0,62; 43 | 43 | 31,0.42 | 19 | 19,0,46; 43 | 43 | 33,0.45 | 41 | 41,0,55; 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->; </body>. </html>. The following mutation validation failed:; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-rig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628
https://github.com/google/deepvariant/issues/503#issuecomment-1019658392:187,Availability,error,error,187,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1019658392
https://github.com/google/deepvariant/issues/503#issuecomment-1019658392:265,Availability,error,error,265,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1019658392
https://github.com/google/deepvariant/issues/503#issuecomment-1019658392:656,Availability,error,error,656,"Hi @Suke-fudan . The best value to filter on is the GQ value of the variant call itself. We know from empirical investigation that the GQ value is very well-calibrated with the empirical error rate (See Figure 2 of - https://www.nature.com/articles/nbt.4235). This error probability is in the PHRED scale, so a GQ of 10 means DeepVariant indicates a 90% probability the call is correct, while a GQ of 20 indicates a 99% probability the call is correct. The formula for PHRED and correctness probability can be [found here](https://en.wikipedia.org/wiki/Phred_quality_score). . Beyond this information about the relationship between estimated and empirical error, I can't help much more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/503#issuecomment-1019658392
https://github.com/google/deepvariant/issues/506#issuecomment-1017088492:69,Usability,guid,guides,69,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):; ```; -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list.; ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506#issuecomment-1017088492
https://github.com/google/deepvariant/issues/506#issuecomment-1017088492:85,Usability,guid,guide,85,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):; ```; -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list.; ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/506#issuecomment-1017088492
https://github.com/google/deepvariant/issues/510#issuecomment-1029076026:91,Deployability,pipeline,pipeline,91,Thanks a lot. We want to support both to run many patients as well as single ones within a pipeline.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/510#issuecomment-1029076026
https://github.com/google/deepvariant/issues/511#issuecomment-1021003424:5,Deployability,update,updated,5,"I've updated tensorflow to 2.7.0 with `sed -i ""s|2.5.0|2.7.0|"" ""deepvariant/settings.sh""` and everything working fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1021003424
https://github.com/google/deepvariant/issues/511#issuecomment-1021613047:82,Deployability,release,release,82,Thanks for letting us know. I've made a note of this to look into it for the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1021613047
https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:16,Deployability,update,update,16,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644
https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:44,Deployability,release,release,44,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644
https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:68,Deployability,update,update,68,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644
https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:183,Deployability,install,install,183,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644
https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:254,Deployability,update,update,254,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644
https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:312,Deployability,update,updates,312,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644
https://github.com/google/deepvariant/issues/513#issuecomment-1027495489:89,Availability,error,error,89,"Can you check whether you're able to pull other public images on this machine?. From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:; ```; singularity pull docker://google/deepvariant:""1.3.0""; ```; as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489
https://github.com/google/deepvariant/issues/513#issuecomment-1027495489:137,Availability,error,error,137,"Can you check whether you're able to pull other public images on this machine?. From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:; ```; singularity pull docker://google/deepvariant:""1.3.0""; ```; as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489
https://github.com/google/deepvariant/issues/513#issuecomment-1027495489:313,Safety,sanity check,sanity check,313,"Can you check whether you're able to pull other public images on this machine?. From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:; ```; singularity pull docker://google/deepvariant:""1.3.0""; ```; as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489
https://github.com/google/deepvariant/issues/513#issuecomment-1027495489:122,Security,Authenticat,Authentication,122,"Can you check whether you're able to pull other public images on this machine?. From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:; ```; singularity pull docker://google/deepvariant:""1.3.0""; ```; as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489
https://github.com/google/deepvariant/issues/513#issuecomment-1027691620:504,Availability,error,error,504,Thank you very much for your quick replay.; I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10; it returns the following error and stop; INFO: Convert SIF file to sandbox...; ERROR : Failed to create user namespace: user namespace not supported by your system; Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620
https://github.com/google/deepvariant/issues/513#issuecomment-1027691620:558,Availability,ERROR,ERROR,558,Thank you very much for your quick replay.; I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10; it returns the following error and stop; INFO: Convert SIF file to sandbox...; ERROR : Failed to create user namespace: user namespace not supported by your system; Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620
https://github.com/google/deepvariant/issues/513#issuecomment-1027691620:546,Modifiability,sandbox,sandbox,546,Thank you very much for your quick replay.; I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10; it returns the following error and stop; INFO: Convert SIF file to sandbox...; ERROR : Failed to create user namespace: user namespace not supported by your system; Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620
https://github.com/google/deepvariant/issues/513#issuecomment-1027691620:546,Testability,sandbox,sandbox,546,Thank you very much for your quick replay.; I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10; it returns the following error and stop; INFO: Convert SIF file to sandbox...; ERROR : Failed to create user namespace: user namespace not supported by your system; Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620
https://github.com/google/deepvariant/issues/514#issuecomment-1035209001:49,Availability,error,error,49,Removing the conda environment gives me the same error.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035209001
https://github.com/google/deepvariant/issues/514#issuecomment-1035263880:224,Availability,error,errors,224,"@Phillip-a-richmond Thanks for checking.; I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035263880
https://github.com/google/deepvariant/issues/514#issuecomment-1035263880:85,Deployability,release,release,85,"@Phillip-a-richmond Thanks for checking.; I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035263880
https://github.com/google/deepvariant/issues/514#issuecomment-1035492077:70,Testability,Test,Testing,70,I've got 1.1-gpu working so I don't think it's an issue with my CUDA. Testing 1.2 now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035492077
https://github.com/google/deepvariant/issues/514#issuecomment-1035509300:18,Availability,error,error,18,"1.2 produces same error. ```; 2022-02-10 12:57:29.123141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035509300
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:3903,Availability,error,error,3903,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src; ```; ```; pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow; ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory; ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:374,Deployability,Install,Install,374,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:; ```; pichuan@pichuan-gpu:~$ uname -a; Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux; ```. # Install GPU driver and Singularity on the machine:; ```; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash; ```. Singularity version:; ```; pichuan@pichuan-gpu:~$ singularity --version; singularity version 3.7.0; ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:2506,Performance,cache,cached,2506,"PUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:2909,Performance,cache,cached,2909,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src; ```; ```; pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow; ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory; ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:3438,Performance,cache,cached,3438,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src; ```; ```; pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow; ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory; ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:3679,Security,access,access,3679,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src; ```; ```; pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow; ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory; ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:757,Testability,test,test,757,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:; ```; pichuan@pichuan-gpu:~$ uname -a; Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux; ```. # Install GPU driver and Singularity on the machine:; ```; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash; ```. Singularity version:; ```; pichuan@pichuan-gpu:~$ singularity --version; singularity version 3.7.0; ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:902,Testability,test,test,902,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:; ```; pichuan@pichuan-gpu:~$ uname -a; Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux; ```. # Install GPU driver and Singularity on the machine:; ```; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash; ```. Singularity version:; ```; pichuan@pichuan-gpu:~$ singularity --version; singularity version 3.7.0; ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725
https://github.com/google/deepvariant/issues/514#issuecomment-1156819727:1221,Availability,error,error,1221,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:; ```; WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/; export SINGULARITY_CACHEDIR=$WORKING_DIR; export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/; mkdir -p $WORKING_DIR/tmp/. singularity exec \; 	-e \; 	-c \; 	-H $WORKING_DIR \; 	-B $WORKING_DIR/tmp:/tmp \; 	-B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/genomedir/$FASTA_FILE"" \; --reads=""/bamdir/$PROBAND_BAM"" \; --output_vcf=""/output/$PROBAND_VCF"" \; --output_gvcf=""/output/$PROBAND_GVCF"" \; --intermediate_results_dir=""/output/intermediate"" \; --num_shards=$NSLOTS ; ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727
https://github.com/google/deepvariant/issues/514#issuecomment-1156819727:1244,Deployability,deploy,deployment,1244,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:; ```; WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/; export SINGULARITY_CACHEDIR=$WORKING_DIR; export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/; mkdir -p $WORKING_DIR/tmp/. singularity exec \; 	-e \; 	-c \; 	-H $WORKING_DIR \; 	-B $WORKING_DIR/tmp:/tmp \; 	-B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/genomedir/$FASTA_FILE"" \; --reads=""/bamdir/$PROBAND_BAM"" \; --output_vcf=""/output/$PROBAND_VCF"" \; --output_gvcf=""/output/$PROBAND_GVCF"" \; --intermediate_results_dir=""/output/intermediate"" \; --num_shards=$NSLOTS ; ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727
https://github.com/google/deepvariant/issues/514#issuecomment-1156819727:1082,Modifiability,variab,variables,1082,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:; ```; WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/; export SINGULARITY_CACHEDIR=$WORKING_DIR; export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/; mkdir -p $WORKING_DIR/tmp/. singularity exec \; 	-e \; 	-c \; 	-H $WORKING_DIR \; 	-B $WORKING_DIR/tmp:/tmp \; 	-B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/genomedir/$FASTA_FILE"" \; --reads=""/bamdir/$PROBAND_BAM"" \; --output_vcf=""/output/$PROBAND_VCF"" \; --output_gvcf=""/output/$PROBAND_GVCF"" \; --intermediate_results_dir=""/output/intermediate"" \; --num_shards=$NSLOTS ; ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727
https://github.com/google/deepvariant/issues/515#issuecomment-1034110256:97,Security,access,accessible,97,I see that you don't bind a /scicore directory in docker command. Your local directories are not accessible from a docker. Please take a look at the FAQ [section](https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open) that explains docker binding.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/515#issuecomment-1034110256
https://github.com/google/deepvariant/issues/516#issuecomment-1034107962:68,Performance,perform,performs,68,"Hi Josh,. There are multiple possible explanations.; 1. DeepVariant performs a local realignment which may change how reads are aligned. There is a section in FAQ which describes how to output a realigned BAM file [here](https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work). For that you can run make_examples.py with --region flag that would generate examples for your specific region. I recommend to set the region to 1000 bases interval (for example chr1:2001-3000). Then you may examine a realigned BAM with IGV.; 2. DeepVariant does not take into account positions with base quality lower than the threshold. What can happen is that certain positions are low quality and therefore not counted towards allele depth. Hopefully this will help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/516#issuecomment-1034107962
https://github.com/google/deepvariant/issues/517#issuecomment-1050344301:108,Availability,error,error,108,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
https://github.com/google/deepvariant/issues/517#issuecomment-1050344301:449,Availability,error,error,449,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
https://github.com/google/deepvariant/issues/517#issuecomment-1050344301:114,Integrability,message,message,114,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
https://github.com/google/deepvariant/issues/517#issuecomment-1050344301:52,Usability,clear,clear,52,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301
https://github.com/google/deepvariant/issues/517#issuecomment-1055258649:285,Availability,error,errors,285,"Hi @MariaNattestad . Thanks for the offer, but it would be difficult to share the data without a DTA. So, I went back and reran the workflow (`--num_shards=5`) for a short region around the above coordinates and then again for the complete chr1, both the tests ran through without any errors. And some of the candidate variants called earlier are not present here.; ```; 1 160351251 . T <*> 0 . END=160351253 GT:GQ:MIN_DP:PL 0/0:50:80:0,261,2609; 1 160351254 . GTTTT G,<*> 9.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:9:79:18,33,0:0.417722,0:8,0,26,990,990,990; ```; Not sure how it's resolved. But, I will close this issue for now and will reopen it if a similar error pops up during the rerun of all the chrs. Thank you for looking into the issue,; Naga",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1055258649
https://github.com/google/deepvariant/issues/517#issuecomment-1055258649:653,Availability,error,error,653,"Hi @MariaNattestad . Thanks for the offer, but it would be difficult to share the data without a DTA. So, I went back and reran the workflow (`--num_shards=5`) for a short region around the above coordinates and then again for the complete chr1, both the tests ran through without any errors. And some of the candidate variants called earlier are not present here.; ```; 1 160351251 . T <*> 0 . END=160351253 GT:GQ:MIN_DP:PL 0/0:50:80:0,261,2609; 1 160351254 . GTTTT G,<*> 9.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:9:79:18,33,0:0.417722,0:8,0,26,990,990,990; ```; Not sure how it's resolved. But, I will close this issue for now and will reopen it if a similar error pops up during the rerun of all the chrs. Thank you for looking into the issue,; Naga",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1055258649
https://github.com/google/deepvariant/issues/517#issuecomment-1055258649:255,Testability,test,tests,255,"Hi @MariaNattestad . Thanks for the offer, but it would be difficult to share the data without a DTA. So, I went back and reran the workflow (`--num_shards=5`) for a short region around the above coordinates and then again for the complete chr1, both the tests ran through without any errors. And some of the candidate variants called earlier are not present here.; ```; 1 160351251 . T <*> 0 . END=160351253 GT:GQ:MIN_DP:PL 0/0:50:80:0,261,2609; 1 160351254 . GTTTT G,<*> 9.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:9:79:18,33,0:0.417722,0:8,0,26,990,990,990; ```; Not sure how it's resolved. But, I will close this issue for now and will reopen it if a similar error pops up during the rerun of all the chrs. Thank you for looking into the issue,; Naga",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/517#issuecomment-1055258649
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:2768,Availability,avail,available,2768,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1971,Energy Efficiency,reduce,reduced,1971,"g section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1855,Modifiability,inherit,inherited,1855,"and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1235,Performance,perform,perform,1235,"to run separate calling on the non-PAR regions of ChrX and ChrY, where only the mother sample is provided as the parent for calling of the son, and (less importantly as it is unclear whether this is an issue with chrY) only the father sample is provided for calling chrY on the son. In the documentation, this is expressed in the following statement: ""Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be perfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1885,Performance,perform,performed,1885,"and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:2277,Performance,perform,performed,2277,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:190,Testability,benchmark,benchmarked,190,"Hi @Phillip-a-richmond . Thank you for the report. You are correct, that the behavior out-of-the box for DeepTrio can currently be sub-optimal for the sex chromosomes in male samples. We've benchmarked some strategies for dealing with this. In the short term, our recommendation is to run separate calling on the non-PAR regions of ChrX and ChrY, where only the mother sample is provided as the parent for calling of the son, and (less importantly as it is unclear whether this is an issue with chrY) only the father sample is provided for calling chrY on the son. In the documentation, this is expressed in the following statement: ""Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:941,Testability,benchmark,benchmarks,941,"Hi @Phillip-a-richmond . Thank you for the report. You are correct, that the behavior out-of-the box for DeepTrio can currently be sub-optimal for the sex chromosomes in male samples. We've benchmarked some strategies for dealing with this. In the short term, our recommendation is to run separate calling on the non-PAR regions of ChrX and ChrY, where only the mother sample is provided as the parent for calling of the son, and (less importantly as it is unclear whether this is an issue with chrY) only the father sample is provided for calling chrY on the son. In the documentation, this is expressed in the following statement: ""Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:3066,Usability,feedback,feedback,3066,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025
https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:386,Availability,down,downstream,386,"Hi Andrew,. Thanks for your response. There are a few items here which I'll address. 1. Is your main request that we have the ability to output a hemizygous representation of the VCF? This is something we are starting to think about for VCF and gVCF as we prepare to work with the T2T assemblies. Yes proper representation on X chromosome is important, and it's caused this problem for downstream analyses of pathogenic hemizygous variants. Likely would be true for de novo hemizygous variants on male non-PAR chromosome X. If this were fixed sooner than later that would increase confidence in DeepTrio. Until that's fixed, I'll be shifting back to DeepVariant GVCF->GLNexus for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100
https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:1023,Availability,down,downstream,1023,"Hi Andrew,. Thanks for your response. There are a few items here which I'll address. 1. Is your main request that we have the ability to output a hemizygous representation of the VCF? This is something we are starting to think about for VCF and gVCF as we prepare to work with the T2T assemblies. Yes proper representation on X chromosome is important, and it's caused this problem for downstream analyses of pathogenic hemizygous variants. Likely would be true for de novo hemizygous variants on male non-PAR chromosome X. If this were fixed sooner than later that would increase confidence in DeepTrio. Until that's fixed, I'll be shifting back to DeepVariant GVCF->GLNexus for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100
https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2580,Deployability,configurat,configuration,2580,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100
https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2258,Modifiability,parameteriz,parameterized,2258,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100
https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2580,Modifiability,config,configuration,2580,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100
https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2643,Testability,test,test,2643,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100
https://github.com/google/deepvariant/issues/518#issuecomment-1050404831:716,Deployability,update,update,716,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831
https://github.com/google/deepvariant/issues/518#issuecomment-1050404831:431,Testability,test,test,431,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831
https://github.com/google/deepvariant/issues/518#issuecomment-1050404831:479,Usability,feedback,feedback,479,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831
https://github.com/google/deepvariant/issues/518#issuecomment-1082326802:36,Deployability,update,update,36,"Hi @Phillip-a-richmond . Just as an update on this issue, we have the truth sets and have trained some experimental models. We're still in the process of refining these models, as well as how to use the T2T truth sets. So we do have progress, but it's still going to take time before we're ready with something for you to look at.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1082326802
https://github.com/google/deepvariant/issues/518#issuecomment-1086175908:32,Testability,test,test,32,Thanks @AndrewCarroll! Happy to test beta version when you produce them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1086175908
https://github.com/google/deepvariant/issues/518#issuecomment-1145399411:44,Deployability,update,update,44,"Hi @Phillip-a-richmond ,; I want to give an update on this issue:. As part of working on v1.4.0, we did some experiments on this, which is still work in progress. If you can reach out to me and @AndrewCarroll (you can email me at pichuan@google.com), we can follow up on an experimental model for you to test, if you're still interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1145399411
https://github.com/google/deepvariant/issues/518#issuecomment-1145399411:304,Testability,test,test,304,"Hi @Phillip-a-richmond ,; I want to give an update on this issue:. As part of working on v1.4.0, we did some experiments on this, which is still work in progress. If you can reach out to me and @AndrewCarroll (you can email me at pichuan@google.com), we can follow up on an experimental model for you to test, if you're still interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1145399411
https://github.com/google/deepvariant/issues/518#issuecomment-1284750172:82,Deployability,update,update,82,"Hi @Phillip-a-richmond and everyone else who might be following this thread,; One update:. one post-processing trick that you can do now is: take the probability distribution, and ignore the 0/1 one, and just renormalize the other two. From there, you can decide whether it should be a 0/0 or 1/1.; (This idea came from our collaborator @doron-st at Ultima Genomics. They actually verified this on a dataset and showed good precision. They did this on DeepVariant, not DeepTrio. But I think the same trick can be applied). Our team is currently considering building this into an option in `postprocess_variants`, so that we can do this re-normalization ourselves. (Another more principled approach will be to build this knowledge into the modeling part. We're also considering that, but that will be an even longer-term solution.). Thanks @Phillip-a-richmond again for reporting this. We have filed an internal issue to track this work. I'll close this bug for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1284750172
https://github.com/google/deepvariant/issues/518#issuecomment-1286018716:170,Deployability,pipeline,pipelines,170,"Follow up from my previous comment, @doron-st has shared his script here:; https://github.com/Ultimagen/VariantCalling/blob/c2634d8c08db4473e61b786eed06afc2bb8fccd1/ugvc/pipelines/convert_haploid_regions.py; (Thanks Doron!)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1286018716
https://github.com/google/deepvariant/issues/518#issuecomment-1691400894:4,Deployability,update,updates,4,Any updates on this? The link for the script is broken now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1691400894
https://github.com/google/deepvariant/issues/518#issuecomment-1691796096:62,Deployability,release,release,62,"@nurmians Still looking through the code - and looked at the [release notes](https://github.com/google/deepvariant/releases) - though still reasoning through it in case it might have been implemented via some alternate logic. In any case, here's a link to the script for converting genotypes of regions to haploid calls:. https://github.com/Ultimagen/VariantCalling/blob/master/ugvc/pipelines/convert_haploid_regions.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096
https://github.com/google/deepvariant/issues/518#issuecomment-1691796096:115,Deployability,release,releases,115,"@nurmians Still looking through the code - and looked at the [release notes](https://github.com/google/deepvariant/releases) - though still reasoning through it in case it might have been implemented via some alternate logic. In any case, here's a link to the script for converting genotypes of regions to haploid calls:. https://github.com/Ultimagen/VariantCalling/blob/master/ugvc/pipelines/convert_haploid_regions.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096
https://github.com/google/deepvariant/issues/518#issuecomment-1691796096:383,Deployability,pipeline,pipelines,383,"@nurmians Still looking through the code - and looked at the [release notes](https://github.com/google/deepvariant/releases) - though still reasoning through it in case it might have been implemented via some alternate logic. In any case, here's a link to the script for converting genotypes of regions to haploid calls:. https://github.com/Ultimagen/VariantCalling/blob/master/ugvc/pipelines/convert_haploid_regions.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096
https://github.com/google/deepvariant/issues/518#issuecomment-1691796096:219,Testability,log,logic,219,"@nurmians Still looking through the code - and looked at the [release notes](https://github.com/google/deepvariant/releases) - though still reasoning through it in case it might have been implemented via some alternate logic. In any case, here's a link to the script for converting genotypes of regions to haploid calls:. https://github.com/Ultimagen/VariantCalling/blob/master/ugvc/pipelines/convert_haploid_regions.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096
https://github.com/google/deepvariant/issues/518#issuecomment-1692451997:204,Deployability,release,release,204,"Hi @nurmians ,; The link the @pgrosu link to seems like the right one from Ultima!. Our team is working on incorporating this as part of postprocess_variants. The official version will be out in the next release (before end of 2023). ; You can get a preview of these 2 flags that we'll be adding:; https://github.com/google/deepvariant/blob/dev/deepvariant/postprocess_variants.py#L170-L188. (Note that we don't officially support the code in ""dev"" branch. If you want an official documentation and usage of this, please wait for the release later. But please feel free to take a look at the code if you like)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1692451997
https://github.com/google/deepvariant/issues/518#issuecomment-1692451997:534,Deployability,release,release,534,"Hi @nurmians ,; The link the @pgrosu link to seems like the right one from Ultima!. Our team is working on incorporating this as part of postprocess_variants. The official version will be out in the next release (before end of 2023). ; You can get a preview of these 2 flags that we'll be adding:; https://github.com/google/deepvariant/blob/dev/deepvariant/postprocess_variants.py#L170-L188. (Note that we don't officially support the code in ""dev"" branch. If you want an official documentation and usage of this, please wait for the release later. But please feel free to take a look at the code if you like)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1692451997
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1752,Availability,avail,available,1752,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1871,Energy Efficiency,adapt,adapting,1871,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1871,Modifiability,adapt,adapting,1871,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:157,Performance,perform,performed,157,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:810,Performance,optimiz,optimization,810,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:956,Performance,optimiz,optimize,956,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1018,Performance,perform,performs,1018,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1099,Performance,optimiz,optimized,1099,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1385,Security,validat,validation,1385,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:793,Usability,simpl,simplified,793,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838
https://github.com/google/deepvariant/issues/519#issuecomment-1053728841:41,Security,access,access,41,"Hey, thanks for the response! I may have access to a high-quality dataset in the near future. Also, training multiple models for each level of ploidy makes sense. I've got some experience with TF and programming in general (Python and Rust, lately), so curious if you can give some feedback on specific areas that may need to be adjusted for ploidy? Or, if it would be too much for one person I understand too. I've done re-training of DV models for non-model species and have seen great improvements so far, so I'm familiar with that as well. Thanks again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841
https://github.com/google/deepvariant/issues/519#issuecomment-1053728841:282,Usability,feedback,feedback,282,"Hey, thanks for the response! I may have access to a high-quality dataset in the near future. Also, training multiple models for each level of ploidy makes sense. I've got some experience with TF and programming in general (Python and Rust, lately), so curious if you can give some feedback on specific areas that may need to be adjusted for ploidy? Or, if it would be too much for one person I understand too. I've done re-training of DV models for non-model species and have seen great improvements so far, so I'm familiar with that as well. Thanks again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841
https://github.com/google/deepvariant/issues/519#issuecomment-1054575852:551,Energy Efficiency,adapt,adapt,551,"I consulted with the team to get an idea of the effort required, and we came to the conclusion that the DeepVariant code is full of diploid assumptions, so this would likely take a lot of effort, and unfortunately it isn't something that we have enough bandwidth to advise anyone else through either. You're of course welcome to fork DeepVariant and play around with it, but it will likely require many changes to the code to make it work. It is definitely possible though. You could also consider whether other ML-based variant callers are easier to adapt to polyploid, or if any polyploid callers exist already, which could be fun to add ML on top of as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1054575852
https://github.com/google/deepvariant/issues/519#issuecomment-1054575852:551,Modifiability,adapt,adapt,551,"I consulted with the team to get an idea of the effort required, and we came to the conclusion that the DeepVariant code is full of diploid assumptions, so this would likely take a lot of effort, and unfortunately it isn't something that we have enough bandwidth to advise anyone else through either. You're of course welcome to fork DeepVariant and play around with it, but it will likely require many changes to the code to make it work. It is definitely possible though. You could also consider whether other ML-based variant callers are easier to adapt to polyploid, or if any polyploid callers exist already, which could be fun to add ML on top of as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1054575852
https://github.com/google/deepvariant/issues/519#issuecomment-1054593204:45,Usability,guid,guidance,45,"Ah, understood, and thanks very much for the guidance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/519#issuecomment-1054593204
https://github.com/google/deepvariant/issues/520#issuecomment-1048481061:262,Deployability,update,update,262,"Thanks for clarifying! Hopefully this can be addressed soon. The particular example I have shown here actually has fairly severe consequences for interpretation (MNP = stop gained). So this is not a minor issue. Closing this for now, but will keep an eye on the update notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1048481061
https://github.com/google/deepvariant/issues/520#issuecomment-1541965936:41,Deployability,update,update,41,"Hi,. I am re-opening in hope to get some update on this issue. I think adding support for MNP calling has been a community request dating back to at least 2019. And as this particular issue highlights, the lack of MNP support really does have implications for how the data can be used. . I found in several discussions that people are entirely unaware of this particular limitation in Deepvariant. . So please, at the very least make this a very visible disclaimer, or preferably implement support for MNP calling in your otherwise excellent tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1541965936
https://github.com/google/deepvariant/issues/520#issuecomment-1542578964:328,Testability,benchmark,benchmarks,328,"Hi @marchoeppner . We are currently working on some approaches to better call MNPs and locations with complex variants. Those are in an intermediate stage of investigation, and I can't estimate at present when they will complete (nor give with certainty a guarantee that it will improve accuracy). For now, I will generate some benchmarks for MNP calls which might ground discussions in some quantifiable numbers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1542578964
https://github.com/google/deepvariant/issues/520#issuecomment-1558724808:1862,Usability,learn,learned,1862,"g the union of the Truth Set with any MNP variant called in either GATK or DeepVariant, here is the Hap.py that I see:. <google-sheets-html-origin><style type=""text/css""><!--td {border: 1px solid #cccccc;}br {mso-data-placement:same-cell;}--></style>.   | Filter | TRUTH.TOTAL | TRUTH.TP | TRUTH.FN | QUERY.FP | FP.gt | FP.al | METRIC.Recall | METRIC.Precision | METRIC.F1_Score; -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --; HG001-DV | PASS | 27307 | 26199 | 1108 | 510 | 95 | 10 | 0.9594 | 0.9809 | 0.9700; HG001-GATK | PASS | 27307 | 26258 | 1049 | 1746 | 153 | 17 | 0.9616 | 0.9377 | 0.9495; HG002-DV | PASS | 28662 | 27505 | 1157 | 613 | 77 | 3 | 0.9596 | 0.9782 | 0.9688; HG002-GATK | PASS | 28662 | 27539 | 1123 | 2073 | 133 | 7 | 0.9608 | 0.9300 | 0.9452; HG003-DV | PASS | 28274 | 27234 | 1040 | 588 | 73 | 7 | 0.9632 | 0.9789 | 0.9710; HG003-GATK | PASS | 28274 | 27188 | 1086 | 2060 | 156 | 8 | 0.9616 | 0.9296 | 0.9453. A few observations from these metrics:. 1. For both GATK and DeepVariant, MNPs are notably harder to call. F1 is 0.945-0.97, while we expect F1 of around 0.9945 for DeepVariant at 30x with Illumina.; 2. DeepVariant and GATK4 have fairly similar recall, with DeepVariant being higher in recall in 1 sample and GATK in the other 2.; 3. DeepVariant has noticeably higher precision (0.98 vs 0.93). From this, I suspect that there is at least a few factors that make MNPs more difficult to call. I suspect that DeepVariant has learned some of those factors and is more conservative to call MNPs, even when they might look like real calls. There are several other possible explanations (e.g. that hap.py has trouble in the comparison process correctly annotating sites). Following this current set of metrics, I plan to inspect several of the correctly and incorrectly called variants in IGV with their support and see if I can better understand what factors are making MNPs more difficult to call, and if it seems like there are any specifically addressable issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1558724808
https://github.com/google/deepvariant/issues/520#issuecomment-1558729655:331,Availability,reliab,reliably,331,"Thanks for looking into this. I was under the impression that Deepvariant cannot call MNPs period. It sounded like a technical decision inside the code to look at each variant site independently and break potential MNPs up into individual SNPs. Did I get that wrong and would that mean that DV can infact call MNPs, but not always reliably?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1558729655
https://github.com/google/deepvariant/issues/520#issuecomment-1558742268:339,Availability,avail,available,339,"Gottcha. Basically, my wish would be for DV to emit MNPs as single VCF line. This is because basically all effect prediction algorithms operate that way. It is technically possible to analyse phased VCFs and get protein-level effect predictions that will look at in-phase SNPs within a coding sequence (Haplosaurus, Bcftools CSQ), but the available tools for that are very limited in what they can annotate and report back. And they are generally not used (much) in (clinical) variant interpretation for that reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1558742268
https://github.com/google/deepvariant/issues/520#issuecomment-1558742268:114,Safety,predict,prediction,114,"Gottcha. Basically, my wish would be for DV to emit MNPs as single VCF line. This is because basically all effect prediction algorithms operate that way. It is technically possible to analyse phased VCFs and get protein-level effect predictions that will look at in-phase SNPs within a coding sequence (Haplosaurus, Bcftools CSQ), but the available tools for that are very limited in what they can annotate and report back. And they are generally not used (much) in (clinical) variant interpretation for that reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1558742268
https://github.com/google/deepvariant/issues/520#issuecomment-1558742268:233,Safety,predict,predictions,233,"Gottcha. Basically, my wish would be for DV to emit MNPs as single VCF line. This is because basically all effect prediction algorithms operate that way. It is technically possible to analyse phased VCFs and get protein-level effect predictions that will look at in-phase SNPs within a coding sequence (Haplosaurus, Bcftools CSQ), but the available tools for that are very limited in what they can annotate and report back. And they are generally not used (much) in (clinical) variant interpretation for that reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1558742268
https://github.com/google/deepvariant/issues/520#issuecomment-1559853651:244,Availability,error,errors,244,"Hi @marchoeppner . Converting the representation from adjacent lines to single events is something we will look at. It won't necessarily be an easy change. I think the next thing I will do is to look at why both DeepVariant and GATK are making errors on this set and see if it looks like changing the representation is likely to help with those errors, or if something else is going on. If the errors can also be reduced, then this looks more interesting as something to do sooner rather than later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1559853651
https://github.com/google/deepvariant/issues/520#issuecomment-1559853651:345,Availability,error,errors,345,"Hi @marchoeppner . Converting the representation from adjacent lines to single events is something we will look at. It won't necessarily be an easy change. I think the next thing I will do is to look at why both DeepVariant and GATK are making errors on this set and see if it looks like changing the representation is likely to help with those errors, or if something else is going on. If the errors can also be reduced, then this looks more interesting as something to do sooner rather than later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1559853651
https://github.com/google/deepvariant/issues/520#issuecomment-1559853651:394,Availability,error,errors,394,"Hi @marchoeppner . Converting the representation from adjacent lines to single events is something we will look at. It won't necessarily be an easy change. I think the next thing I will do is to look at why both DeepVariant and GATK are making errors on this set and see if it looks like changing the representation is likely to help with those errors, or if something else is going on. If the errors can also be reduced, then this looks more interesting as something to do sooner rather than later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1559853651
https://github.com/google/deepvariant/issues/520#issuecomment-1559853651:413,Energy Efficiency,reduce,reduced,413,"Hi @marchoeppner . Converting the representation from adjacent lines to single events is something we will look at. It won't necessarily be an easy change. I think the next thing I will do is to look at why both DeepVariant and GATK are making errors on this set and see if it looks like changing the representation is likely to help with those errors, or if something else is going on. If the errors can also be reduced, then this looks more interesting as something to do sooner rather than later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1559853651
https://github.com/google/deepvariant/issues/520#issuecomment-1561873437:260,Modifiability,layers,layers,260,"The thing is that InceptionV3 works on one combined tensor (using all channels), while slicing them together across different convolutions. Spatial information and cross-channel information can be important if you want to train on those types of inputs, where layers of [depthwise separable convolutions (Xception architecture)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) across inputs and [squeeze-excitation networks (SEnet) ](https://arxiv.org/pdf/1709.01507.pdf) become useful in boosting features that travel together. The thing is that the network would need to be tweaked by applying separate input convolutions for only channels that contain the spatial information and not others, as currently DeepVariant models are trained with a collection of channels of which some are only binary possibly only having some effect with more data as the other channels normalize -- though they might become more purposeful as filters. Using additional attention models or custom filters at specific locations across the network can highlight features of interest like variants that possibly travel together, that trigger a separate model flow for the resultant output of interest.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/520#issuecomment-1561873437
https://github.com/google/deepvariant/issues/521#issuecomment-1074112393:38,Deployability,update,update,38,"Hi @GuillaumeHolley ,; to give you an update, I haven't had time to go search for an example. But I'll keep this open in case you want to share an example (or when I have a chance to find one). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521#issuecomment-1074112393
https://github.com/google/deepvariant/issues/521#issuecomment-1100316870:267,Availability,error,error,267,"@GuillaumeHolley ,. After working on this for a while, it looks like the solution isn't that easy. On your end, can you please try filtering all the `NoCall` variants from the gvcf file as a post-processing and see if that helps? I was able to get rid of the GLNexus error by filtering `NoCall` variants with `bcftools`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521#issuecomment-1100316870
https://github.com/google/deepvariant/issues/521#issuecomment-1102489422:35,Usability,feedback,feedback,35,"Hi @kishwarshafin,. Thanks for the feedback, I actually forgot to get back to you about this but I have indeed filtered out the `NoCall` for the small cohort I have after your early feedback and it indeed solved this issue. Good luck with solving this!. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521#issuecomment-1102489422
https://github.com/google/deepvariant/issues/521#issuecomment-1102489422:182,Usability,feedback,feedback,182,"Hi @kishwarshafin,. Thanks for the feedback, I actually forgot to get back to you about this but I have indeed filtered out the `NoCall` for the small cohort I have after your early feedback and it indeed solved this issue. Good luck with solving this!. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/521#issuecomment-1102489422
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:32,Availability,error,error,32,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:106,Availability,error,error,106,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:258,Availability,down,download,258,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:318,Availability,down,download,318,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:112,Integrability,message,message,112,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:213,Safety,safe,safety,213,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:365,Testability,log,login,365,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579
https://github.com/google/deepvariant/pull/523#issuecomment-1069241340:53,Deployability,Update,Update,53,"Hi @dkurt , sorry it took me a while to get to this. Update: I've run internally once with your updated code. So far I've only run on one WES example so I don't have full timing stats yet. I want to confirm that it's expected that I'm seeing:. ```; Please install required versions of components or run pip installation; pip install openvino-dev[tensorflow]; ```. I'll continue to look into the runtime and let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340
https://github.com/google/deepvariant/pull/523#issuecomment-1069241340:96,Deployability,update,updated,96,"Hi @dkurt , sorry it took me a while to get to this. Update: I've run internally once with your updated code. So far I've only run on one WES example so I don't have full timing stats yet. I want to confirm that it's expected that I'm seeing:. ```; Please install required versions of components or run pip installation; pip install openvino-dev[tensorflow]; ```. I'll continue to look into the runtime and let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340
https://github.com/google/deepvariant/pull/523#issuecomment-1069241340:256,Deployability,install,install,256,"Hi @dkurt , sorry it took me a while to get to this. Update: I've run internally once with your updated code. So far I've only run on one WES example so I don't have full timing stats yet. I want to confirm that it's expected that I'm seeing:. ```; Please install required versions of components or run pip installation; pip install openvino-dev[tensorflow]; ```. I'll continue to look into the runtime and let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340
https://github.com/google/deepvariant/pull/523#issuecomment-1069241340:307,Deployability,install,installation,307,"Hi @dkurt , sorry it took me a while to get to this. Update: I've run internally once with your updated code. So far I've only run on one WES example so I don't have full timing stats yet. I want to confirm that it's expected that I'm seeing:. ```; Please install required versions of components or run pip installation; pip install openvino-dev[tensorflow]; ```. I'll continue to look into the runtime and let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340
https://github.com/google/deepvariant/pull/523#issuecomment-1069241340:325,Deployability,install,install,325,"Hi @dkurt , sorry it took me a while to get to this. Update: I've run internally once with your updated code. So far I've only run on one WES example so I don't have full timing stats yet. I want to confirm that it's expected that I'm seeing:. ```; Please install required versions of components or run pip installation; pip install openvino-dev[tensorflow]; ```. I'll continue to look into the runtime and let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340
https://github.com/google/deepvariant/pull/523#issuecomment-1069288915:17,Testability,test,test,17,Got it. My first test run did finish successfully and the hap.py numbers are the same as before.; I'm now running a few bigger runs to take a look at the runtime.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/523#issuecomment-1069288915
https://github.com/google/deepvariant/issues/524#issuecomment-1067583182:279,Energy Efficiency,monitor,monitor,279,"Hi,. As reported above I can see a bunch of small files (30-40 Mb total) written in TMPDIR. If I look in the folder when deepvariant is running I can see files like these; `Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx`. I will try another run to monitor when exactly they are created, but the job fails at very early stage when I submit it to the cluster, so I assume these are written during make_examples which is the first step in run deepvariant I think. Thanks for support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067583182
https://github.com/google/deepvariant/issues/524#issuecomment-1067587398:52,Integrability,message,message,52,"Ah I see. Sorry I missed that part in your original message.; And, I think I understand your question better now. . --intermediate_results_dir isn't designed to capture all temp files from DeepVariant. It's for capturing the intermediate outputs (from make_examples, call_variants) in case that users need to re-use them later on. In your case, using your workaround of setting TMPDIR actually makes sense to me. From your description, it also seems like it's related to your system setting. If you think this is going to be a common issue, please share your command and I'm happy to add it to our documentation as a workaround for other users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067587398
https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:701,Deployability,configurat,configuration,701,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987
https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:139,Modifiability,config,configures,139,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987
https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:701,Modifiability,config,configuration,701,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987
https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:973,Modifiability,variab,variable,973,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987
https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:447,Testability,log,login,447,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987
https://github.com/google/deepvariant/issues/524#issuecomment-1079508369:377,Deployability,release,release,377,"Thank you @edg1983 . I will plan to add this section to our FAQ.md:. ---. ## Singularity related questions:. ### `TMPDIR`. If your run with Singularity is having issues with `TMPDIR`, try adding this to your command:; ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ---. This should show up in our next release. Thanks for providing this information!; If you have more suggestions, let me know. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1079508369
https://github.com/google/deepvariant/issues/524#issuecomment-1091034842:180,Security,access,access,180,"Hi @splaisan ; Your question is using docker, which is a bit different from the discussion earlier, I believe. To use --intermediate_results_dir, it indicates you probably want to access the content there later. So, I'd recommend that you write it to an output file that you mounted with `-v`. For example, given that you have `-v ""$PWD/tmp_dir"":""/tmp_dir""`, maybe try seeting:; ` --intermediate_results_dir /tmp_dir`, which should write output to `$PWD/tmp_dir` once you're done?; (I noticed you wrote `--intermediate_results_dir /temp_dir`, which was not actually mounted. Not sure if that's a typo or not.). Hope this helps. I'm going to close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/524#issuecomment-1091034842
https://github.com/google/deepvariant/issues/526#issuecomment-1067196676:631,Security,access,access,631,"Hi Aman,. Thank you for reaching out to us. As you've read in our [Cohort Best Practices](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration), generating a cohort variant callset using DeepVariant includes two separate steps:. 1. Running DeepVariant on all sample reads to generate gVCF files, one per each sample.; 2. Running GLnexus on the gVCFs using the provided `DeepVariantWGS` or `DeepVariantWES`. For the first step, the best way to parallelize computation would be using cloud compute resources if you have access to them. You can find instructions on how to run DeepVariant on Google Cloud Platform [here](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant), or you can use any other cloud service providers using our DeepVariant docker [images](https://github.com/google/deepvariant#how-to-run-deepvariant), to run DeepVariant on each sample (or a batch of samples) separately in multiple machines. I would not recommend parallelizing DeepVariant runs over samples in a single machine though, since a single run of DeepVariant already uses multiple cores in the `make_examples` step - the number of cores to use is controlled by the `--num_shards` parameter. Once you've generated all ~200 gVCFs, one for each sample, the second step should be pretty simple. A single machine with relatively good CPU/RAM capacity should be able to handle merging 200 gVCFs using GLnexus. I hope this helps. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676
https://github.com/google/deepvariant/issues/526#issuecomment-1067196676:1392,Usability,simpl,simple,1392,"Hi Aman,. Thank you for reaching out to us. As you've read in our [Cohort Best Practices](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration), generating a cohort variant callset using DeepVariant includes two separate steps:. 1. Running DeepVariant on all sample reads to generate gVCF files, one per each sample.; 2. Running GLnexus on the gVCFs using the provided `DeepVariantWGS` or `DeepVariantWES`. For the first step, the best way to parallelize computation would be using cloud compute resources if you have access to them. You can find instructions on how to run DeepVariant on Google Cloud Platform [here](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant), or you can use any other cloud service providers using our DeepVariant docker [images](https://github.com/google/deepvariant#how-to-run-deepvariant), to run DeepVariant on each sample (or a batch of samples) separately in multiple machines. I would not recommend parallelizing DeepVariant runs over samples in a single machine though, since a single run of DeepVariant already uses multiple cores in the `make_examples` step - the number of cores to use is controlled by the `--num_shards` parameter. Once you've generated all ~200 gVCFs, one for each sample, the second step should be pretty simple. A single machine with relatively good CPU/RAM capacity should be able to handle merging 200 gVCFs using GLnexus. I hope this helps. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676
https://github.com/google/deepvariant/issues/527#issuecomment-1067162845:40,Modifiability,extend,extend,40,"@amyhouseman Sometimes people decide to extend the regions in the BED files a bit, something like: https://bedtools.readthedocs.io/en/latest/content/tools/slop.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067162845
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:95,Availability,down,download,95,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:680,Availability,error,error,680,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:712,Availability,error,error,712,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:1605,Availability,error,error,1605,"ad parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz --reads Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/NG0921010_EKDN210018950-1A_HN2MGDSX2_L2_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis/deepvariant/intermediateresults/NG0921010_EKDN210018950-1A_HN2MGDSX2_L2_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis/deepvariant/intermediateresults/NG0921010_EKDN210018950-1A_HN2MGDSX2_L2_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:2701,Availability,error,error,2701,"nalysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz --reads Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/NG0921010_EKDN210018950-1A_HN2MGDSX2_L2_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis/deepvariant/intermediateresults/NG0921010_EKDN210018950-1A_HN2MGDSX2_L2_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis/deepvariant/intermediateresults/NG0921010_EKDN210018950-1A_HN2MGDSX2_L2_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed --task 0; ```. The error is quite descriptive, but I'm not sure how my file would be empty?; Thanks for all your help!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:629,Performance,load,load,629,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:651,Performance,load,load,651,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:197,Testability,log,login,197,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214
https://github.com/google/deepvariant/issues/527#issuecomment-1073019775:133,Usability,guid,guide,133,"Hi sorry for the delay,. I reran my sequences with a new copy of the human reference genome from https://www.ncbi.nlm.nih.gov/genome/guide/human/ up until running deepvariant. . I get the same headers as above, it doesn't seem like the bam, sam or even the reference start with the word 'chr1':. sam file header: @SQ	SN:NC_000001.11	LN:248956422; bam file (sorted picard): @HD	VN:1.6	SO:coordinate; @SQ	SN:NC_000001.11	LN:248956422; Original fasta reference file header: >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly; Bed file: chr1	12080	12251. All the code and steps I've done before are on my page under Exome_Pipeline/PE read analysis. I'm not sure! Ah, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1073019775
https://github.com/google/deepvariant/issues/527#issuecomment-1078989925:104,Security,access,accession,104,For anyone with the same problem: my bed file had the IDs labelled as chr1 whereas other files used the accession number (e.g NC_000001.11) - I used dplyr on R to recode the IDs to the accession number and then deepvariant worked :) Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1078989925
https://github.com/google/deepvariant/issues/527#issuecomment-1078989925:185,Security,access,accession,185,For anyone with the same problem: my bed file had the IDs labelled as chr1 whereas other files used the accession number (e.g NC_000001.11) - I used dplyr on R to recode the IDs to the accession number and then deepvariant worked :) Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/527#issuecomment-1078989925
https://github.com/google/deepvariant/issues/528#issuecomment-1067120303:84,Deployability,release,release,84,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303
https://github.com/google/deepvariant/issues/528#issuecomment-1067120303:813,Safety,predict,prediction,813,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303
https://github.com/google/deepvariant/issues/528#issuecomment-1067120303:838,Usability,intuit,intuitive,838,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303
https://github.com/google/deepvariant/issues/528#issuecomment-1067565656:701,Availability,error,error,701,"Hi @Qianwangwoo . We don't do additional filtering beyond the probabilities from the classifier. In this case, DeepVariant does not have a high confidence in the correct genotype between HET and HOM-ALT (a GQ of 4 corresponds to a ~60% confidence in a correct genotype call). The QUAL value of 36.1 suggests that DeepVariant is at least pretty confident that the position is not REF. A few other points to keep in mind - first, are you using the two-pass DeepVariant-WhatsHap-DeepVariant method? If so, then DeepVariant may be using additional information about the phasing from longer range. Second, this variant is at a junction between homopolymers (poly-T and poly-G) This represents the dominant error mode for PacBio HiFi, so it may nit be straightforward for a human to assess the probability of a G->T variant here as opposed to a sequencing error of Insertion T and deletion G. . If you want to for sure have a higher precision, you can additionally filter for GQ value (e.g. 10 for a 90% confidence in the genotype call). However, if you do so, you will lose variant positions like this which are very likely not reference, but difficult to genotype.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067565656
https://github.com/google/deepvariant/issues/528#issuecomment-1067565656:850,Availability,error,error,850,"Hi @Qianwangwoo . We don't do additional filtering beyond the probabilities from the classifier. In this case, DeepVariant does not have a high confidence in the correct genotype between HET and HOM-ALT (a GQ of 4 corresponds to a ~60% confidence in a correct genotype call). The QUAL value of 36.1 suggests that DeepVariant is at least pretty confident that the position is not REF. A few other points to keep in mind - first, are you using the two-pass DeepVariant-WhatsHap-DeepVariant method? If so, then DeepVariant may be using additional information about the phasing from longer range. Second, this variant is at a junction between homopolymers (poly-T and poly-G) This represents the dominant error mode for PacBio HiFi, so it may nit be straightforward for a human to assess the probability of a G->T variant here as opposed to a sequencing error of Insertion T and deletion G. . If you want to for sure have a higher precision, you can additionally filter for GQ value (e.g. 10 for a 90% confidence in the genotype call). However, if you do so, you will lose variant positions like this which are very likely not reference, but difficult to genotype.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1067565656
https://github.com/google/deepvariant/issues/528#issuecomment-1068868000:220,Deployability,release,release,220,"Hi @Qianwangwoo . Yes, the two-pass method generally improves accuracy with PacBio small variant calling, especially for Indels. Whether it is likely to improve this call, I am not sure. Note that we anticipate a future release of DeepVariant for PacBio in the near future which will have comparable accuracy with a single pass of variant calling, so you may prefer to keep your current workflow and wait for that version if you don't mind updating.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/528#issuecomment-1068868000
https://github.com/google/deepvariant/issues/530#issuecomment-1076920454:1511,Deployability,update,updated,1511,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454
https://github.com/google/deepvariant/issues/530#issuecomment-1076920454:1531,Deployability,release,release,1531,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454
https://github.com/google/deepvariant/issues/530#issuecomment-1076920454:1457,Usability,feedback,feedback,1457,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454
https://github.com/google/deepvariant/issues/530#issuecomment-1076923302:102,Usability,guid,guides,102,"@japhill But for now, I also wonder if you can use the `-B` option in Singularity?; https://sylabs.io/guides/3.1/user-guide/bind_paths_and_mounts.html#user-defined-bind-paths; Can you try it and let me know if it works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076923302
https://github.com/google/deepvariant/issues/530#issuecomment-1076923302:118,Usability,guid,guide,118,"@japhill But for now, I also wonder if you can use the `-B` option in Singularity?; https://sylabs.io/guides/3.1/user-guide/bind_paths_and_mounts.html#user-defined-bind-paths; Can you try it and let me know if it works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076923302
https://github.com/google/deepvariant/issues/530#issuecomment-1076926666:58,Security,access,accessible,58,Just to add to the previous post. Your file system is not accessible from the container. Using -B option you may map (mount) some directories from your file system to be accessible from within the container. . There is a similar issue with the similar content. https://github.com/google/deepvariant/issues/506#issuecomment-1017088492,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076926666
https://github.com/google/deepvariant/issues/530#issuecomment-1076926666:170,Security,access,accessible,170,Just to add to the previous post. Your file system is not accessible from the container. Using -B option you may map (mount) some directories from your file system to be accessible from within the container. . There is a similar issue with the similar content. https://github.com/google/deepvariant/issues/506#issuecomment-1017088492,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1076926666
https://github.com/google/deepvariant/issues/530#issuecomment-1079509951:29,Deployability,update,update,29,"Hi @japhill , to give you an update, I plan to add this section to our FAQ in the next release:. ---. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for more details. ---. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951
https://github.com/google/deepvariant/issues/530#issuecomment-1079509951:87,Deployability,release,release,87,"Hi @japhill , to give you an update, I plan to add this section to our FAQ in the next release:. ---. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for more details. ---. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951
https://github.com/google/deepvariant/issues/530#issuecomment-1079509951:294,Safety,avoid,avoid,294,"Hi @japhill , to give you an update, I plan to add this section to our FAQ in the next release:. ---. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for more details. ---. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951
https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:375,Availability,error,errors,375,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356
https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:453,Deployability,update,update,453,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356
https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:511,Deployability,release,release,511,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356
https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:1325,Integrability,Message,Message,1325,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356
https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:737,Safety,avoid,avoid,737,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356
https://github.com/google/deepvariant/issues/531#issuecomment-1081798812:92,Usability,guid,guidelines,92,"thanks for the great piece of software!; I agree with Jordi and would add that some general guidelines on how to use the numerous INFO fields would be nice, do they live somewhere?; it is a pity to have so many metrics and ignore how to put them to good use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1081798812
https://github.com/google/deepvariant/issues/531#issuecomment-1082175599:119,Usability,learn,learning,119,"Hi @jordimaggi ; For anything that is `RefCall`, that means: even though a candidate variant was proposed, our machine learning classifier decided the most likely class is 0 (which means reference). ; You can read this section to get a bit more background on this: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. You could potentially take the finer-grained information (like `PL`) and try to adjust your own threshold. This could increase the sensitivity, but will likely hurt the specificity of DeepVariant's results. I'll ask @AndrewCarroll to add his thoughts here as well. Hi @splaisan , for the existing fields we have in our VCF file, we follow the standard definitions you can find on https://en.wikipedia.org/wiki/Variant_Call_Format#Common_FORMAT_fields (and we only fill in a subset of them). Let us know if there's anything specific that is not clear to you. Happy to explain more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1082175599
https://github.com/google/deepvariant/issues/531#issuecomment-1082175599:930,Usability,clear,clear,930,"Hi @jordimaggi ; For anything that is `RefCall`, that means: even though a candidate variant was proposed, our machine learning classifier decided the most likely class is 0 (which means reference). ; You can read this section to get a bit more background on this: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. You could potentially take the finer-grained information (like `PL`) and try to adjust your own threshold. This could increase the sensitivity, but will likely hurt the specificity of DeepVariant's results. I'll ask @AndrewCarroll to add his thoughts here as well. Hi @splaisan , for the existing fields we have in our VCF file, we follow the standard definitions you can find on https://en.wikipedia.org/wiki/Variant_Call_Format#Common_FORMAT_fields (and we only fill in a subset of them). Let us know if there's anything specific that is not clear to you. Happy to explain more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1082175599
https://github.com/google/deepvariant/issues/531#issuecomment-1082325659:1218,Performance,tune,tune,1218,"Hi @jordimaggi @splaisan . I will add on a little to Pi-Chuan's answer with respect to filtering and quality scores. We consistently find that the genotype quality (GQ and PL fields) are extremely well-calibrated with the empirical probability of a call being correct. This is quantified in Figure 2 of the [original DeepVariant paper](https://www.biorxiv.org/content/10.1101/092890v6). This value is the best to use when determining whether a call is likely correct. Both ourselves and other external groups who we work with have tried to identify other metrics of standard INFO and FORMAT fields which are more predictive of call quality or even additionally informative in a subset of contexts and cases. For basically everything we and these groups have looked at, GQ is more predictive of call correctness. . If you are able to identify an annotation which is additionally informative beyond GQ (and also not already perfectly captured in the GQ field), it would be quite interesting to know, and we could consider incorporating it as an output field, or providing the annotation as an input during calling. . In general, I'd encourage you to look at GQ and PL as the most informative fields if you would like to tune between sensitivity and specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659
https://github.com/google/deepvariant/issues/531#issuecomment-1082325659:613,Safety,predict,predictive,613,"Hi @jordimaggi @splaisan . I will add on a little to Pi-Chuan's answer with respect to filtering and quality scores. We consistently find that the genotype quality (GQ and PL fields) are extremely well-calibrated with the empirical probability of a call being correct. This is quantified in Figure 2 of the [original DeepVariant paper](https://www.biorxiv.org/content/10.1101/092890v6). This value is the best to use when determining whether a call is likely correct. Both ourselves and other external groups who we work with have tried to identify other metrics of standard INFO and FORMAT fields which are more predictive of call quality or even additionally informative in a subset of contexts and cases. For basically everything we and these groups have looked at, GQ is more predictive of call correctness. . If you are able to identify an annotation which is additionally informative beyond GQ (and also not already perfectly captured in the GQ field), it would be quite interesting to know, and we could consider incorporating it as an output field, or providing the annotation as an input during calling. . In general, I'd encourage you to look at GQ and PL as the most informative fields if you would like to tune between sensitivity and specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659
https://github.com/google/deepvariant/issues/531#issuecomment-1082325659:780,Safety,predict,predictive,780,"Hi @jordimaggi @splaisan . I will add on a little to Pi-Chuan's answer with respect to filtering and quality scores. We consistently find that the genotype quality (GQ and PL fields) are extremely well-calibrated with the empirical probability of a call being correct. This is quantified in Figure 2 of the [original DeepVariant paper](https://www.biorxiv.org/content/10.1101/092890v6). This value is the best to use when determining whether a call is likely correct. Both ourselves and other external groups who we work with have tried to identify other metrics of standard INFO and FORMAT fields which are more predictive of call quality or even additionally informative in a subset of contexts and cases. For basically everything we and these groups have looked at, GQ is more predictive of call correctness. . If you are able to identify an annotation which is additionally informative beyond GQ (and also not already perfectly captured in the GQ field), it would be quite interesting to know, and we could consider incorporating it as an output field, or providing the annotation as an input during calling. . In general, I'd encourage you to look at GQ and PL as the most informative fields if you would like to tune between sensitivity and specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659
https://github.com/google/deepvariant/issues/532#issuecomment-1086245019:148,Energy Efficiency,schedul,scheduler,148,"Hello jessieshen97,. In order to train a good model you need to run make_examples on multiple sets on different coverages. Internally we use Google scheduler with thousands of hosts. You may try to do it on one machine but it may be challenging. ; To answer you question, yes you can train a DeepTrio model of the diploid organism. Although, we don't have a document with detailed steps on training a DeepTrio model you can try following the steps for DeepVariant toy model training. The only difference is that you need to use DeepTrio make_examples. All other sets would be identical.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/532#issuecomment-1086245019
https://github.com/google/deepvariant/issues/533#issuecomment-1088051366:139,Availability,Error,Error,139,"Hi @pichuan,. Thanks for replying so quickly. I have tried both of those things. Doing `export TMPDIR=""$PWD/tmp_dir""` causes the same as **Error 1** above. . ```; INFO: Using cached SIF image; I0404 17:45:45.958600 23016861075264 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp_ui_t3bd. ***** Intermediate results will be written to /tmp/tmp_ui_t3bd in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmp_ui_t3bd/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /scratch1/rrautsa/deepvariant_test/tmp_dir/parXXXXX.par: Parent directory (/scratch1/rrautsa/deepvariant_test/tmp_dir/) does not exist at /usr/bin/parallel line 3889. real	0m0.257s; user	0m0.121s; sys	0m0.125s; ```. And perhaps I don't understand singularity enough, but it seems that no matter how I change the `--bind` input it gives me the following error:. ```; INFO: Using cached SIF image; FATAL: failed to open /bin/sh for inspection: failed to open elf binary /bin/sh: open /bin/sh: no such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366
https://github.com/google/deepvariant/issues/533#issuecomment-1088051366:908,Availability,Error,Error,908,"Hi @pichuan,. Thanks for replying so quickly. I have tried both of those things. Doing `export TMPDIR=""$PWD/tmp_dir""` causes the same as **Error 1** above. . ```; INFO: Using cached SIF image; I0404 17:45:45.958600 23016861075264 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp_ui_t3bd. ***** Intermediate results will be written to /tmp/tmp_ui_t3bd in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmp_ui_t3bd/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /scratch1/rrautsa/deepvariant_test/tmp_dir/parXXXXX.par: Parent directory (/scratch1/rrautsa/deepvariant_test/tmp_dir/) does not exist at /usr/bin/parallel line 3889. real	0m0.257s; user	0m0.121s; sys	0m0.125s; ```. And perhaps I don't understand singularity enough, but it seems that no matter how I change the `--bind` input it gives me the following error:. ```; INFO: Using cached SIF image; FATAL: failed to open /bin/sh for inspection: failed to open elf binary /bin/sh: open /bin/sh: no such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366
https://github.com/google/deepvariant/issues/533#issuecomment-1088051366:1296,Availability,error,error,1296,"Hi @pichuan,. Thanks for replying so quickly. I have tried both of those things. Doing `export TMPDIR=""$PWD/tmp_dir""` causes the same as **Error 1** above. . ```; INFO: Using cached SIF image; I0404 17:45:45.958600 23016861075264 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp_ui_t3bd. ***** Intermediate results will be written to /tmp/tmp_ui_t3bd in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmp_ui_t3bd/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /scratch1/rrautsa/deepvariant_test/tmp_dir/parXXXXX.par: Parent directory (/scratch1/rrautsa/deepvariant_test/tmp_dir/) does not exist at /usr/bin/parallel line 3889. real	0m0.257s; user	0m0.121s; sys	0m0.125s; ```. And perhaps I don't understand singularity enough, but it seems that no matter how I change the `--bind` input it gives me the following error:. ```; INFO: Using cached SIF image; FATAL: failed to open /bin/sh for inspection: failed to open elf binary /bin/sh: open /bin/sh: no such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366
https://github.com/google/deepvariant/issues/533#issuecomment-1088051366:175,Performance,cache,cached,175,"Hi @pichuan,. Thanks for replying so quickly. I have tried both of those things. Doing `export TMPDIR=""$PWD/tmp_dir""` causes the same as **Error 1** above. . ```; INFO: Using cached SIF image; I0404 17:45:45.958600 23016861075264 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp_ui_t3bd. ***** Intermediate results will be written to /tmp/tmp_ui_t3bd in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmp_ui_t3bd/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /scratch1/rrautsa/deepvariant_test/tmp_dir/parXXXXX.par: Parent directory (/scratch1/rrautsa/deepvariant_test/tmp_dir/) does not exist at /usr/bin/parallel line 3889. real	0m0.257s; user	0m0.121s; sys	0m0.125s; ```. And perhaps I don't understand singularity enough, but it seems that no matter how I change the `--bind` input it gives me the following error:. ```; INFO: Using cached SIF image; FATAL: failed to open /bin/sh for inspection: failed to open elf binary /bin/sh: open /bin/sh: no such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366
https://github.com/google/deepvariant/issues/533#issuecomment-1088051366:1321,Performance,cache,cached,1321,"Hi @pichuan,. Thanks for replying so quickly. I have tried both of those things. Doing `export TMPDIR=""$PWD/tmp_dir""` causes the same as **Error 1** above. . ```; INFO: Using cached SIF image; I0404 17:45:45.958600 23016861075264 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp_ui_t3bd. ***** Intermediate results will be written to /tmp/tmp_ui_t3bd in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmp_ui_t3bd/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /scratch1/rrautsa/deepvariant_test/tmp_dir/parXXXXX.par: Parent directory (/scratch1/rrautsa/deepvariant_test/tmp_dir/) does not exist at /usr/bin/parallel line 3889. real	0m0.257s; user	0m0.121s; sys	0m0.125s; ```. And perhaps I don't understand singularity enough, but it seems that no matter how I change the `--bind` input it gives me the following error:. ```; INFO: Using cached SIF image; FATAL: failed to open /bin/sh for inspection: failed to open elf binary /bin/sh: open /bin/sh: no such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366
https://github.com/google/deepvariant/issues/534#issuecomment-1102014578:2577,Availability,avail,available,2577,"eature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further training. This highlights one key point where T2T may help with training - that the current training is limited by the truth set. Training with v4.2.1 truth sets is still constrained by the confident regions of the genome. If we can get fully complete, 100% accurate truth sets covering the genome, this will provide more training examples of difficult regions in the training process, and I think this could further improve a model (whether it's on the T2T reference or on GRCh38). I think there will be an opportunity for this as complete T2T assemblies become available for more samples. Finally, from a practical perspective, the current v4.2.1 truth sets are relative to GRCh38, so in order to train we'd need to first be able to generate truth variants and confident regions for some sample on T2T. That's certainly doable, but it is tricky to do correctly. Hopefully this has answered more questions than it has opened. If this is an area you have ideas about or are interested in collaborating on, we'd certainly be happy to explore those together. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578
https://github.com/google/deepvariant/issues/534#issuecomment-1102014578:972,Usability,learn,learned,972,"oad . This is a good question and the best answer I can give to it is both complicated and not conclusive. TL;DR - for long reads, retraining will (probably) not change things much, but there might be other future opportunities to use T2T truth in training strategies regardless of the reference used. . Generally, I think DeepVariant will give good results on T2T without retraining specifically for it, and given the better completeness of the T2T reference, probably just using this will give generally better results. In the past, we have trained with both GRCh37 and GRCh38, and we don't see the model behaving very differently with either reference. It's possible that re-training with the T2T could lead to marginally better accuracy in some areas, especially in segmental duplications, which are better resolved in T2T. At present, GRCh38 will have some segmental duplications that are collapsed, or where a copy is missing. DeepVariant seems to have learned some of the patterns for this, and is can sometimes reject variants in regions that look like segdups. This feature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578
https://github.com/google/deepvariant/issues/534#issuecomment-1102014578:1997,Usability,learn,learn,1997,"re collapsed, or where a copy is missing. DeepVariant seems to have learned some of the patterns for this, and is can sometimes reject variants in regions that look like segdups. This feature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further training. This highlights one key point where T2T may help with training - that the current training is limited by the truth set. Training with v4.2.1 truth sets is still constrained by the confident regions of the genome. If we can get fully complete, 100% accurate truth sets covering the genome, this will provide more training examples of difficult regions in the training process, and I think this could further improve a model (whether it's on the T2T reference or on GRCh38). I think there will be an opportunity for this as complete T2T assemblies become available for more samples. Finally, from a practical perspective, the current v4.2.1 truth sets are relative to GRCh38, so in order to train we'd need to first be able to generate truth variants and confident regions for some sample on T2T. That's certainly doable, but it is tricky to do correctly. Hopefully this has answere",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578
https://github.com/google/deepvariant/issues/534#issuecomment-2261084300:961,Integrability,protocol,protocol,961,"I'm working on a project related to T2T variant calling. @AndrewCarroll, has your group examined this question in more detail? I am currently documenting the impact of T2T vs GRCh38 alignment on variant calling. I can tell you already that it has a [large effect](https://www.waisman.wisc.edu/2024/06/07/werling-slide-of-the-week-2024/) on alignment quality. Documenting the impact is one thing, but retraining DeepVariant is another. I *could* do it, but I don't look forward to it. If interested, I can provide my cram files. I have your published HG002-HG007 illumina reads at 20X and 30X depth aligned to:. - GRCh38 (w/ BWA-mem); - T2Tv2.0 (w/ BWA-mem; - GRCh38 (aligned to HPRCv1.1 w/ vg giraffe, surjected to GRCh38); - T2T (aligned to HPRCv1.1 w/ vg giraffe, surjected to T2T); - GRCh38 (aligned to personalized graph* created from HPRCv1.1, surjected to GRCh38); - T2T (aligned to personalized graph* created from HPRCv1.1, surjected to T2T). * per the protocol outlined in this paper: https://www.biorxiv.org/content/10.1101/2023.12.13.571553v2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2261084300
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:109,Availability,avail,available,109,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:186,Availability,avail,available,186,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:500,Availability,avail,available,500,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:200,Deployability,release,released,200,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:59,Integrability,depend,depends,59,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:358,Modifiability,extend,extend,358,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:343,Usability,simpl,simply,343,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106
https://github.com/google/deepvariant/issues/534#issuecomment-2263721491:398,Deployability,update,updates,398,"Thanks! I don't know if I agree that a lifted over variant set would not provide value - after all, even if the variants themselves are the same, we'd expect there to be fewer pileup regions due to differences in how off-target reads align. Even if you're right, there's only one way to know for sure - try it out and see!. That being said, your reasoning makes sense. I'll keep an eye out for any updates. PS - I wonder if someone has begun working with the HG002 Q100 assembly? That can be aligned to GRCh38 or T2T reference and, in theory, used as a ground truth variant set for HG002 reads. (Although, I believe that Zook is still evaluating the viability of that option.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2263721491
https://github.com/google/deepvariant/issues/534#issuecomment-2266085916:334,Availability,error,errors,334,"Hi @JosephLalli . It's a good question. Overall, I agree with what Kishar said about liftover. With historical truth sets (e.g. v3.3.2 which had GRCh38 variants lifted over from GRCh37), which observed artifacts from the liftover process. One factor to keep in mind is that the truth sets have such high label quality that even a few errors makes a big difference. . We've been talking with Justin Zook about the T2T Q100 assembly. My expectation is that this will represent the highest quality mechanism to get labels on the T2T assembly. My understanding is that a GRCh38 investigation of this assembly will come first. So we haven't yet worked on it, and I think it won't be very imminent, but I believe it is something that we will eventually investigate as the resources become more available for it. What timeframe do you think is required for your purposes?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2266085916
https://github.com/google/deepvariant/issues/534#issuecomment-2266085916:788,Availability,avail,available,788,"Hi @JosephLalli . It's a good question. Overall, I agree with what Kishar said about liftover. With historical truth sets (e.g. v3.3.2 which had GRCh38 variants lifted over from GRCh37), which observed artifacts from the liftover process. One factor to keep in mind is that the truth sets have such high label quality that even a few errors makes a big difference. . We've been talking with Justin Zook about the T2T Q100 assembly. My expectation is that this will represent the highest quality mechanism to get labels on the T2T assembly. My understanding is that a GRCh38 investigation of this assembly will come first. So we haven't yet worked on it, and I think it won't be very imminent, but I believe it is something that we will eventually investigate as the resources become more available for it. What timeframe do you think is required for your purposes?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2266085916
https://github.com/google/deepvariant/issues/534#issuecomment-2266268721:350,Availability,error,errors,350,"> Hi @JosephLalli ; > ; > ; > ; > It's a good question. Overall, I agree with what Kishar said about liftover. With historical truth sets (e.g. v3.3.2 which had GRCh38 variants lifted over from GRCh37), which observed artifacts from the liftover process. One factor to keep in mind is that the truth sets have such high label quality that even a few errors makes a big difference. ; > ; > ; > ; > We've been talking with Justin Zook about the T2T Q100 assembly. My expectation is that this will represent the highest quality mechanism to get labels on the T2T assembly. My understanding is that a GRCh38 investigation of this assembly will come first.; > ; > ; > ; > So we haven't yet worked on it, and I think it won't be very imminent, but I believe it is something that we will eventually investigate as the resources become more available for it.; > ; > ; > ; > What timeframe do you think is required for your purposes?; > ; > ; > ; > Thank you,; > ; > Andrew. I agree re: lifting over. I think waiting for HG002-Q100 makes sense. . As far as timeframe goes, that's really up to Justin Zook. I hope to submit for publication by the end of the year, but we'll see... Thanks for reaching out,; Joe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2266268721
https://github.com/google/deepvariant/issues/534#issuecomment-2266268721:833,Availability,avail,available,833,"> Hi @JosephLalli ; > ; > ; > ; > It's a good question. Overall, I agree with what Kishar said about liftover. With historical truth sets (e.g. v3.3.2 which had GRCh38 variants lifted over from GRCh37), which observed artifacts from the liftover process. One factor to keep in mind is that the truth sets have such high label quality that even a few errors makes a big difference. ; > ; > ; > ; > We've been talking with Justin Zook about the T2T Q100 assembly. My expectation is that this will represent the highest quality mechanism to get labels on the T2T assembly. My understanding is that a GRCh38 investigation of this assembly will come first.; > ; > ; > ; > So we haven't yet worked on it, and I think it won't be very imminent, but I believe it is something that we will eventually investigate as the resources become more available for it.; > ; > ; > ; > What timeframe do you think is required for your purposes?; > ; > ; > ; > Thank you,; > ; > Andrew. I agree re: lifting over. I think waiting for HG002-Q100 makes sense. . As far as timeframe goes, that's really up to Justin Zook. I hope to submit for publication by the end of the year, but we'll see... Thanks for reaching out,; Joe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/534#issuecomment-2266268721
https://github.com/google/deepvariant/issues/535#issuecomment-1104708449:62,Testability,test,testing,62,"Hi @MariaNattestad , thanks so much for your reply!; Yes, I'm testing for germline variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/535#issuecomment-1104708449
https://github.com/google/deepvariant/issues/536#issuecomment-1106635811:300,Availability,down,downsampling,300,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811
https://github.com/google/deepvariant/issues/536#issuecomment-1106635811:414,Deployability,update,updated,414,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811
https://github.com/google/deepvariant/issues/536#issuecomment-1106635811:181,Usability,learn,learning,181,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811
https://github.com/google/deepvariant/issues/536#issuecomment-1110491714:264,Deployability,release,release,264,"Hi @paupaiz , I'll close this issue now. Hopefully Maria's response answered your question.; One thing I'll add is that when you use make_examples, you can use the `pileup_image_height` to change the height of images you create. ; But, be careful that our default release models won't work directly if you change the pileup_image_height, so if you create images of different heights, you'll likely need to retrain your own model.; I'll close this issue now. Feel free to reopen or ask another question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/536#issuecomment-1110491714
https://github.com/google/deepvariant/issues/537#issuecomment-1136530780:273,Security,access,accessible,273,```; Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); ``` . Could you try to save the model on the cloud (path should start with gs://)? It looks that model is not accessible from the TPU host.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1136530780
https://github.com/google/deepvariant/issues/537#issuecomment-1136578198:449,Availability,error,error,449,"Sure, I am trying to use the default WGS model. After hosting the model on the cloud, how do I point deepvariant to it through the Docker solution?. This is what I see in the local docker container's models directory when running the image:; ```bash; root@8368b35e9c34:/# ls /opt/models/wgs/; model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.input_shape model.ckpt.meta; ```. I am using the google/deepvariant:1.3.0 docker image. The same error occurs for me with the GPU version. Is there a different model expected for the TPU implementation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1136578198
https://github.com/google/deepvariant/issues/537#issuecomment-1139165175:111,Security,access,accessible,111,"When you do ```ls /opt/models/wgs/``` you see the local content of the mounted directory which is probably not accessible from TPU host. Although, we don't officially support running on TPU there is an older version case study that shows how to run training on TPU [here](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md#start-a-cloud-tpu). In particular, there is a [link](https://cloud.google.com/tpu/docs/storage-buckets#storage_access) with instructions how to make storage bucket accessible from the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1139165175
https://github.com/google/deepvariant/issues/537#issuecomment-1139165175:528,Security,access,accessible,528,"When you do ```ls /opt/models/wgs/``` you see the local content of the mounted directory which is probably not accessible from TPU host. Although, we don't officially support running on TPU there is an older version case study that shows how to run training on TPU [here](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md#start-a-cloud-tpu). In particular, there is a [link](https://cloud.google.com/tpu/docs/storage-buckets#storage_access) with instructions how to make storage bucket accessible from the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1139165175
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:1098,Availability,error,error,1098,"necessary for the TPU Node. I am still having an issue pointing deepvariant to the model hosted in the cloud. . I have tried using a model in the deepvariant bucket with the following command and model: ; gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. But I get the following error:; ```bash; I0527 20:42:08.331003 139757477517120 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:2326,Availability,error,error,2326,"t.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. I also get the same error when hosting the model (renamed model.ckpt) in my personal GS bucket -- I have made the storage bucket read accessible to all users so the TPU should have access:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. I0527 21:26:03.381308 140127359940416 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:4432,Availability,error,error,4432," run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. However, if I shorten the model name in the deepvariant bucket (model.ckpt.data-00000-of-00001 -> model.ckpt), the file is found and processing continues until the previous error is met because the checkpoint file does not actually exist under the name model.ckpt in the deepvariant bucket. ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. INFO:tensorflow:Done calling model_fn.; I0527 21:33:10.817516 139926144051008 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0527 21:33:11.115715 139926144051008 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:4457,Availability,checkpoint,checkpoint,4457," run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. However, if I shorten the model name in the deepvariant bucket (model.ckpt.data-00000-of-00001 -> model.ckpt), the file is found and processing continues until the previous error is met because the checkpoint file does not actually exist under the name model.ckpt in the deepvariant bucket. ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. INFO:tensorflow:Done calling model_fn.; I0527 21:33:10.817516 139926144051008 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0527 21:33:11.115715 139926144051008 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph wa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:6055,Availability,error,error,6055,"-output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. INFO:tensorflow:Done calling model_fn.; I0527 21:33:10.817516 139926144051008 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0527 21:33:11.115715 139926144051008 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph was finalized.; I0527 21:33:11.664746 139926144051008 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; I0527 21:33:11.801618 139926144051008 saver.py:1298] Restoring parameters from gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; INFO:tensorflow:prediction_loop marked as finished; I0527 21:33:13.662127 139926144051008 error_handling.py:115] prediction_loop marked as finished; WARNING:tensorflow:Reraising captured error; W0527 21:33:13.662372 139926144051008 error_handling.py:149] Reraising captured error; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call; return fn(*args); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn; return self._call_tf_sessionrun(options, feed_dict, fetch_list,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun; return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,; tensorflow.python.framework.errors_impl.NotFoundError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[{{node save_1/RestoreV2}}]]. During handling of the above exception,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:6142,Availability,error,error,6142,"_dir. INFO:tensorflow:Done calling model_fn.; I0527 21:33:10.817516 139926144051008 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0527 21:33:11.115715 139926144051008 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph was finalized.; I0527 21:33:11.664746 139926144051008 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; I0527 21:33:11.801618 139926144051008 saver.py:1298] Restoring parameters from gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; INFO:tensorflow:prediction_loop marked as finished; I0527 21:33:13.662127 139926144051008 error_handling.py:115] prediction_loop marked as finished; WARNING:tensorflow:Reraising captured error; W0527 21:33:13.662372 139926144051008 error_handling.py:149] Reraising captured error; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call; return fn(*args); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn; return self._call_tf_sessionrun(options, feed_dict, fetch_list,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun; return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,; tensorflow.python.framework.errors_impl.NotFoundError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:12923,Availability,Checkpoint,CheckpointReader,12923,"ow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal; ret = Operation(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 69, in get_tensor; return CheckpointReader.CheckpointReader_GetTensor(; RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1314, in restore; names_to_keys = object_graph_key_mapping(save_path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1632, in object_graph_key_mapping; object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor; error_translator(e); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator; raise errors_impl.NotFoundError(None, None, error_message); tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJEC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:13029,Availability,checkpoint,checkpoint,13029,"e_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal; ret = Operation(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 69, in get_tensor; return CheckpointReader.CheckpointReader_GetTensor(; RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1314, in restore; names_to_keys = object_graph_key_mapping(save_path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1632, in object_graph_key_mapping; object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor; error_translator(e); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator; raise errors_impl.NotFoundError(None, None, error_message); tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, ano",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:13935,Availability,checkpoint,checkpoint,13935,"nsor; return CheckpointReader.CheckpointReader_GetTensor(; RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1314, in restore; names_to_keys = object_graph_key_mapping(save_path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1632, in object_graph_key_mapping; object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor; error_translator(e); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator; raise errors_impl.NotFoundError(None, None, error_message); tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17460,Availability,checkpoint,checkpoint,17460,"f.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17566,Availability,checkpoint,checkpoint,17566,"python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17650,Availability,checkpoint,checkpoint,17650,"e_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17671,Availability,error,error,17671,"are_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9478,Energy Efficiency,Monitor,MonitoredSession,9478,"ib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9621,Energy Efficiency,Monitor,MonitoredSession,9621,"arser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in fina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15624,Energy Efficiency,Monitor,MonitoredSession,15624,"nt/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15768,Energy Efficiency,Monitor,MonitoredSession,15768,"om_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:19222,Energy Efficiency,Monitor,MonitoredSession,19222,"ib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:19365,Energy Efficiency,Monitor,MonitoredSession,19365,"arser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in fina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:7870,Integrability,message,message,7870,"gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore; sess.run(self.saver_def.restore_op_name,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.NotFoundError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17508,Modifiability,Variab,Variable,17508,"python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9125,Safety,predict,prediction,9125,"_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9143,Safety,predict,predictions,9143,"_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9276,Safety,predict,predict,9276,"]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9325,Safety,predict,predict,9325," ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9445,Safety,predict,predict,9445,"3, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:14843,Safety,predict,prediction,14843,"work.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:14861,Safety,predict,predictions,14861,"work.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:14995,Safety,predict,predict,14995,"exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15421,Safety,predict,predict,15421,"299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15470,Safety,predict,predict,15470,"runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.cr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15591,Safety,predict,predict,15591,"/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_ses",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:18869,Safety,predict,prediction,18869,"_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:18887,Safety,predict,predictions,18887,"_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:19020,Safety,predict,predict,19020,"]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:19069,Safety,predict,predict,19069," ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:19189,Safety,predict,predict,19189,"3, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:2440,Security,access,accessible,2440,"; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. I also get the same error when hosting the model (renamed model.ckpt) in my personal GS bucket -- I have made the storage bucket read accessible to all users so the TPU should have access:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. I0527 21:26:03.381308 140127359940416 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:2487,Security,access,access,2487,"; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. I also get the same error when hosting the model (renamed model.ckpt) in my personal GS bucket -- I have made the storage bucket read accessible to all users so the TPU should have access:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. I0527 21:26:03.381308 140127359940416 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:10703,Security,access,access,10703,"ages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:11342,Security,access,access,11342,"create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:20447,Security,access,access,20447,"ages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:21086,Security,access,access,21086,"create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:22457,Usability,simpl,simple,22457,"low/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal; ret = Operation(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op); ```. Is there something simple I am missing here? Thanks for the support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874
https://github.com/google/deepvariant/issues/537#issuecomment-1140073910:101,Availability,error,error,101,"Sorry, @akolesnikov pointed out that you tried both.; I don't have an immediate answer to the second error then.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1140073910
https://github.com/google/deepvariant/issues/537#issuecomment-1147942215:54,Deployability,update,updates,54,"Hi, I just wanted to check in to see if there are any updates on this thread? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1147942215
https://github.com/google/deepvariant/issues/537#issuecomment-1148060981:10,Testability,benchmark,benchmarking,10,"I am just benchmarking TPU usage on DeepVariant to see if there is a significant speedup as compared to GPU. There were supporting flags in the call_variants step, so I wanted to test with TPU. If TPU is not recommended for inference, then I will switch over to training and try from there, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1148060981
https://github.com/google/deepvariant/issues/537#issuecomment-1148060981:179,Testability,test,test,179,"I am just benchmarking TPU usage on DeepVariant to see if there is a significant speedup as compared to GPU. There were supporting flags in the call_variants step, so I wanted to test with TPU. If TPU is not recommended for inference, then I will switch over to training and try from there, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1148060981
https://github.com/google/deepvariant/issues/537#issuecomment-1148070760:34,Availability,error,error,34,"Is there a solution to the second error that occurs when renaming (model.ckpt.data-00000-of-00001 -> model.ckpt), or is this not supported for TPU usage?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1148070760
https://github.com/google/deepvariant/issues/537#issuecomment-1149310849:152,Security,access,access,152,"Unfortunately, we don't officially support running on TPU at the moment. The way you ran it when using a short model name looks correct. It could be an access control issue (there is no read access to the bucket containing the model from TPU host).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1149310849
https://github.com/google/deepvariant/issues/537#issuecomment-1149310849:191,Security,access,access,191,"Unfortunately, we don't officially support running on TPU at the moment. The way you ran it when using a short model name looks correct. It could be an access control issue (there is no read access to the bucket containing the model from TPU host).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/537#issuecomment-1149310849
https://github.com/google/deepvariant/issues/538#issuecomment-1138272184:863,Availability,avail,available,863,"Hi @avilella . DeepVariant has been used on MGI datasets, both using the standard Illumina model, as well as retrained models. There is some complexity that the MGI/BGI technologies have evolved over time, so some demonstrations may not reflect the newest methods. The general finding is that the Illumina models tend to work well for MGI data, though we find examples of retraining for certain datasets improve further. Our [advanced training tutorial ](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md) walks through retraining an Illumina model for data from BGISEQ 500 and [this comparison](https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/) was conducted several years ago using the out-of-the-box Illumina model. If you know of any genome in a bottle sequencing datasets that are available from more recent MGI platforms, I'd be interested in pointers to those locations. I would be quite curious to see how the technology has evolved over the last several years.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138272184
https://github.com/google/deepvariant/issues/538#issuecomment-1138272184:187,Modifiability,evolve,evolved,187,"Hi @avilella . DeepVariant has been used on MGI datasets, both using the standard Illumina model, as well as retrained models. There is some complexity that the MGI/BGI technologies have evolved over time, so some demonstrations may not reflect the newest methods. The general finding is that the Illumina models tend to work well for MGI data, though we find examples of retraining for certain datasets improve further. Our [advanced training tutorial ](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md) walks through retraining an Illumina model for data from BGISEQ 500 and [this comparison](https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/) was conducted several years ago using the out-of-the-box Illumina model. If you know of any genome in a bottle sequencing datasets that are available from more recent MGI platforms, I'd be interested in pointers to those locations. I would be quite curious to see how the technology has evolved over the last several years.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138272184
https://github.com/google/deepvariant/issues/538#issuecomment-1138272184:1010,Modifiability,evolve,evolved,1010,"Hi @avilella . DeepVariant has been used on MGI datasets, both using the standard Illumina model, as well as retrained models. There is some complexity that the MGI/BGI technologies have evolved over time, so some demonstrations may not reflect the newest methods. The general finding is that the Illumina models tend to work well for MGI data, though we find examples of retraining for certain datasets improve further. Our [advanced training tutorial ](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md) walks through retraining an Illumina model for data from BGISEQ 500 and [this comparison](https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/) was conducted several years ago using the out-of-the-box Illumina model. If you know of any genome in a bottle sequencing datasets that are available from more recent MGI platforms, I'd be interested in pointers to those locations. I would be quite curious to see how the technology has evolved over the last several years.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138272184
https://github.com/google/deepvariant/issues/538#issuecomment-1138380160:1050,Availability,avail,available,1050,"Fantastic information, thank you. On Thu, May 26, 2022 at 9:01 AM Andrew Carroll ***@***.***>; wrote:. > Hi @avilella <https://github.com/avilella>; >; > DeepVariant has been used on MGI datasets, both using the standard; > Illumina model, as well as retrained models. There is some complexity that; > the MGI/BGI technologies have evolved over time, so some demonstrations may; > not reflect the newest methods.; >; > The general finding is that the Illumina models tend to work well for MGI; > data, though we find examples of retraining for certain datasets improve; > further.; >; > Our advanced training tutorial; > <https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md>; > walks through retraining an Illumina model for data from BGISEQ 500 and this; > comparison; > <https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/>; > was conducted several years ago using the out-of-the-box Illumina model.; >; > If you know of any genome in a bottle sequencing datasets that are; > available from more recent MGI platforms, I'd be interested in pointers to; > those locations. I would be quite curious to see how the technology has; > evolved over the last several years.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/538#issuecomment-1138272184>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABGSN3EDTSIAXWBYLGQ3PDVL4VV7ANCNFSM5W4SRYCA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160
https://github.com/google/deepvariant/issues/538#issuecomment-1138380160:1554,Integrability,Message,Message,1554,"Fantastic information, thank you. On Thu, May 26, 2022 at 9:01 AM Andrew Carroll ***@***.***>; wrote:. > Hi @avilella <https://github.com/avilella>; >; > DeepVariant has been used on MGI datasets, both using the standard; > Illumina model, as well as retrained models. There is some complexity that; > the MGI/BGI technologies have evolved over time, so some demonstrations may; > not reflect the newest methods.; >; > The general finding is that the Illumina models tend to work well for MGI; > data, though we find examples of retraining for certain datasets improve; > further.; >; > Our advanced training tutorial; > <https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md>; > walks through retraining an Illumina model for data from BGISEQ 500 and this; > comparison; > <https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/>; > was conducted several years ago using the out-of-the-box Illumina model.; >; > If you know of any genome in a bottle sequencing datasets that are; > available from more recent MGI platforms, I'd be interested in pointers to; > those locations. I would be quite curious to see how the technology has; > evolved over the last several years.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/538#issuecomment-1138272184>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABGSN3EDTSIAXWBYLGQ3PDVL4VV7ANCNFSM5W4SRYCA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160
https://github.com/google/deepvariant/issues/538#issuecomment-1138380160:332,Modifiability,evolve,evolved,332,"Fantastic information, thank you. On Thu, May 26, 2022 at 9:01 AM Andrew Carroll ***@***.***>; wrote:. > Hi @avilella <https://github.com/avilella>; >; > DeepVariant has been used on MGI datasets, both using the standard; > Illumina model, as well as retrained models. There is some complexity that; > the MGI/BGI technologies have evolved over time, so some demonstrations may; > not reflect the newest methods.; >; > The general finding is that the Illumina models tend to work well for MGI; > data, though we find examples of retraining for certain datasets improve; > further.; >; > Our advanced training tutorial; > <https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md>; > walks through retraining an Illumina model for data from BGISEQ 500 and this; > comparison; > <https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/>; > was conducted several years ago using the out-of-the-box Illumina model.; >; > If you know of any genome in a bottle sequencing datasets that are; > available from more recent MGI platforms, I'd be interested in pointers to; > those locations. I would be quite curious to see how the technology has; > evolved over the last several years.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/538#issuecomment-1138272184>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABGSN3EDTSIAXWBYLGQ3PDVL4VV7ANCNFSM5W4SRYCA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160
https://github.com/google/deepvariant/issues/538#issuecomment-1138380160:1203,Modifiability,evolve,evolved,1203,"Fantastic information, thank you. On Thu, May 26, 2022 at 9:01 AM Andrew Carroll ***@***.***>; wrote:. > Hi @avilella <https://github.com/avilella>; >; > DeepVariant has been used on MGI datasets, both using the standard; > Illumina model, as well as retrained models. There is some complexity that; > the MGI/BGI technologies have evolved over time, so some demonstrations may; > not reflect the newest methods.; >; > The general finding is that the Illumina models tend to work well for MGI; > data, though we find examples of retraining for certain datasets improve; > further.; >; > Our advanced training tutorial; > <https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md>; > walks through retraining an Illumina model for data from BGISEQ 500 and this; > comparison; > <https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/>; > was conducted several years ago using the out-of-the-box Illumina model.; >; > If you know of any genome in a bottle sequencing datasets that are; > available from more recent MGI platforms, I'd be interested in pointers to; > those locations. I would be quite curious to see how the technology has; > evolved over the last several years.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/538#issuecomment-1138272184>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABGSN3EDTSIAXWBYLGQ3PDVL4VV7ANCNFSM5W4SRYCA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160
https://github.com/google/deepvariant/issues/538#issuecomment-1817559748:452,Usability,feedback,feedback,452,"Hi @PlatonB ,; In v1.6, we have added documentation on how to run on MGI data. Please see the links ""Complete Genomics data: [T7 case study](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md); [G400 case study](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-g400-case-study.md)"" which was also linked from our GitHub main page. And let us know if you encounter any issues or have any feedback. Hopefully the customized models will give you better results!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/538#issuecomment-1817559748
https://github.com/google/deepvariant/issues/539#issuecomment-1139723040:597,Availability,error,errors,597,"@ziphra ,. Reads without any quality score provided to DeepVariant would cause this issue. One thing you can quickly see from your `samtools view` output is the flag of this read is `0x16`, you can go to this website: https://broadinstitute.github.io/picard/explain-flags.html and put in `0x16` and it'd say the read is unmapped. . A quick way to remove any improper reads would be to run this:; ```; INPUT_BAM=/path/to/input.bam; OUTPUT_BAM=/path/to/input.F0x904.bam; samtools view -@42 -F 0x904 ${INPUT_BAM} > ${OUTPUT_BAM}; ```; This will remove all non-primary reads and should get rid of the errors for you. You can put `0x904` on the explain-flags website to see which reads you will only keep.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539#issuecomment-1139723040
https://github.com/google/deepvariant/issues/539#issuecomment-1141201135:705,Usability,clear,clear,705,"Hello @kishwarshafin, . Thank you for your response and for the useful tips!. It appears that I have several types of flags for my reads presenting no quality score: ; ```; 0; 16; 2048; 2064; 4; ```; All the above flags are also present in my reads having a quality score sequence, except for the flag `4` (= read unmapped), which is absent. Also, it seems that in this case flags should be written without the `0x` prefix, which converts them to hexadecimal when they are written in decimal in the sam file, as I understand. ; `0x16` in https://broadinstitute.github.io/picard/explain-flags.html output a flag that cannot be set when read is not paired, and my read are not paired. However, it now seems clear that something went wrong with the alignment since I have all types of reads with no quality score sequence, not only `0x904` type reads (which are read unmapped `0x4`, not primary alignment `0x100`, and supplementary alignment `0x800`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539#issuecomment-1141201135
https://github.com/google/deepvariant/issues/539#issuecomment-1144255541:225,Usability,clear,clearly,225,"@ziphra ,. You are correct. It does seem like something went wrong with your mapping and you have reads without base-qualities. My suspicion was that the aligner was removing base-qualities from non-primary reads, but that's clearly not the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/539#issuecomment-1144255541
https://github.com/google/deepvariant/issues/541#issuecomment-1152836865:336,Deployability,release,release,336,"Hi @ASLeonard ,. Thanks for reporting this issue. We actually made a deliberate decision to not include OpenVINO this time, because in our test setting we were not able to get faster runtime. We did talk to @dkurt about this and tried https://github.com/google/deepvariant/pull/523. We will still plan to try OpenVINO again in the next release. But given that we didn't see faster runtime, we decided to leave it out of the default. If you would like to use it, please use our Dockerfile and build with the option on. I'm curious - were you seeing a speedup by using OpenVINO in DeepVariant v1.3.0? If so, what is the type of machine you're using?. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1152836865
https://github.com/google/deepvariant/issues/541#issuecomment-1152836865:139,Testability,test,test,139,"Hi @ASLeonard ,. Thanks for reporting this issue. We actually made a deliberate decision to not include OpenVINO this time, because in our test setting we were not able to get faster runtime. We did talk to @dkurt about this and tried https://github.com/google/deepvariant/pull/523. We will still plan to try OpenVINO again in the next release. But given that we didn't see faster runtime, we decided to leave it out of the default. If you would like to use it, please use our Dockerfile and build with the option on. I'm curious - were you seeing a speedup by using OpenVINO in DeepVariant v1.3.0? If so, what is the type of machine you're using?. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1152836865
https://github.com/google/deepvariant/issues/541#issuecomment-1153777690:164,Testability,benchmark,benchmark,164,"I was using OpenVINO from v1.1 when it was still faster, and mainly haven't bothered to remove the flags since at worst it doesn't seem to make it slower. I didn't benchmark in v1.3 when not including the flag, so it may well be the case it isn't as helpful anymore. It would get tricky since users can easily build their own image with `DV_OPENVINO_BUILD=1`, so the flags to run with OpenVINO can't be easily removed. But if the default google/deepvariant image doesn't have OpenVINO support, it would be nice if there was more of a user warning+exit or a warning+ignore the flag and run as if `--use_openvino=False` anyway since it is currently a legitimate flag to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1153777690
https://github.com/google/deepvariant/issues/541#issuecomment-1154454592:372,Deployability,release,releases,372,"Yeah, we could remove `use_openvino` completely from call_variants.py. But given that I do still want to try it again in the future, it would be nice to keep it as a flag. (And like you said, users who build their own binaries or own Docker images, they can still enable it). @ASLeonard Will it be helpful if we add a bullet point in https://github.com/google/deepvariant/releases/tag/v1.4.0 to specifically call out that we're not building OpenVINO into out default Docker image?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1154454592
https://github.com/google/deepvariant/issues/541#issuecomment-1154742774:622,Deployability,release,release,622,"Yeah I think that would be fine. This case was more unfortunate as the missing OpenVINO leads to the missing `from tensorflow.python.tools import optimize_for_inference_lib` dependency, and so I assumed I had really messed something up as I hadn't changed anything with tensorflow. On a slightly different note, the memory usage for call_variants (open_vino=False) in v1.4 is much better than v1.3 (open_vino=True) at ~ 7gb compared to ~20gb RAM for a lot of medium coverage samples. Also postprocessing seems to be about 70% faster (although the previous runs had IO issues so probably inflated). Great improvements this release!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1154742774
https://github.com/google/deepvariant/issues/541#issuecomment-1154742774:174,Integrability,depend,dependency,174,"Yeah I think that would be fine. This case was more unfortunate as the missing OpenVINO leads to the missing `from tensorflow.python.tools import optimize_for_inference_lib` dependency, and so I assumed I had really messed something up as I hadn't changed anything with tensorflow. On a slightly different note, the memory usage for call_variants (open_vino=False) in v1.4 is much better than v1.3 (open_vino=True) at ~ 7gb compared to ~20gb RAM for a lot of medium coverage samples. Also postprocessing seems to be about 70% faster (although the previous runs had IO issues so probably inflated). Great improvements this release!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1154742774
https://github.com/google/deepvariant/issues/541#issuecomment-1156752742:44,Deployability,update,updated,44,"Thanks @ASLeonard , a few follow ups:; 1. I updated https://github.com/google/deepvariant/releases/tag/v1.4.0 to mention that this time we didn't build in OpenVINO by default.; 2. Thanks for the `call_variants` observation. Now I wonder how much of the RAM difference is OpenVINO and how much of it is TensorFlow version updates...; 3. Great to hear that you're also seeing good postprocess_variants improvements! Our 20%er @MosheWagner worked really hard on this, so I'm sure he'll be happy to hear that too!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1156752742
https://github.com/google/deepvariant/issues/541#issuecomment-1156752742:90,Deployability,release,releases,90,"Thanks @ASLeonard , a few follow ups:; 1. I updated https://github.com/google/deepvariant/releases/tag/v1.4.0 to mention that this time we didn't build in OpenVINO by default.; 2. Thanks for the `call_variants` observation. Now I wonder how much of the RAM difference is OpenVINO and how much of it is TensorFlow version updates...; 3. Great to hear that you're also seeing good postprocess_variants improvements! Our 20%er @MosheWagner worked really hard on this, so I'm sure he'll be happy to hear that too!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1156752742
https://github.com/google/deepvariant/issues/541#issuecomment-1156752742:321,Deployability,update,updates,321,"Thanks @ASLeonard , a few follow ups:; 1. I updated https://github.com/google/deepvariant/releases/tag/v1.4.0 to mention that this time we didn't build in OpenVINO by default.; 2. Thanks for the `call_variants` observation. Now I wonder how much of the RAM difference is OpenVINO and how much of it is TensorFlow version updates...; 3. Great to hear that you're also seeing good postprocess_variants improvements! Our 20%er @MosheWagner worked really hard on this, so I'm sure he'll be happy to hear that too!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/541#issuecomment-1156752742
https://github.com/google/deepvariant/issues/543#issuecomment-1228240854:29,Availability,error,error,29,How did you solve this? Same error here.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/543#issuecomment-1228240854
https://github.com/google/deepvariant/issues/544#issuecomment-1179387163:97,Performance,optimiz,optimized,97,"Merging VCFs can be done using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for use with DeepVariant gVCFs. The process is described in the DeepTrio case studies ([DeepTrio whole genome sequencing case study](https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-wgs-case-study.md) and [Using DeepTrio for small variant calling from the trio sequenced with PacBio HiFi](https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md)), and in the manuscript, [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). --output_gvcf_merged flag in run_deeptrio.py script is not supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544#issuecomment-1179387163
https://github.com/google/deepvariant/issues/544#issuecomment-1179387163:530,Performance,scalab,scalable,530,"Merging VCFs can be done using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for use with DeepVariant gVCFs. The process is described in the DeepTrio case studies ([DeepTrio whole genome sequencing case study](https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-wgs-case-study.md) and [Using DeepTrio for small variant calling from the trio sequenced with PacBio HiFi](https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md)), and in the manuscript, [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). --output_gvcf_merged flag in run_deeptrio.py script is not supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/544#issuecomment-1179387163
https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:88,Availability,error,error,88,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896
https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:265,Availability,ERROR,ERROR,265,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896
https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:680,Availability,avail,available,680,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896
https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:1139,Deployability,configurat,configuration,1139,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896
https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:560,Energy Efficiency,schedul,scheduler,560,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896
https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:1139,Modifiability,config,configuration,1139,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896
https://github.com/google/deepvariant/issues/546#issuecomment-1180880714:1008,Energy Efficiency,monitor,monitoring,1008,"ah good catch sorry I missed the image!. It has been a while since I have used slurm, but you could try:. ```; #SBATCH --cpus-per-task=28; #SBATCH --ntasks-per-node=1 # Or don't set this.; ```. See the explanation for `--ntasks-per-node`:. `--ntasks-per-node=<ntasks>`; > Request that ntasks be invoked on each node. If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. Meant to be used with the --nodes option. This is related to --cpus-per-task=ncpus, but does not require knowledge of the actual number of cpus on each node. In some cases, it is more convenient to be able to request that no more than a specific number of tasks be invoked on each node. Examples of this include submitting a hybrid MPI/OpenMP app where only one MPI ""task/rank"" should be assigned to each node while allowing the OpenMP portion to utilize all of the parallelism present in the node, or submitting a single setup/cleanup/monitoring job to each node of a pre-existing allocation as one step in a larger job script. I think the issue is that `--ntasks-per-node` is not allocating CPUs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/546#issuecomment-1180880714
https://github.com/google/deepvariant/issues/547#issuecomment-1194472227:459,Usability,clear,clearly,459,"Hi @elcortegano . Just to clarify, are you referring to the QUAL field of the VCF (the 6th column of tab-file itself), or the GQ field (the 10th column of a single sample). If you are referring to QUAL as the 6th column, this observation is expected. QUAL measures the probability that the ALT field has at least one allele with the ALT base. So you can think of it as p(HET) + p(HOM), or alternatively as 1 - p(REF). For homozygous positions, they look more clearly non-reference as in many cases they may not have any reference bases. . Heterozygous positions likely have at least some evidence for the Ref allele, which suggests a higher probability that the position might be Ref. If you are interested in filtering, we often recommend that the GQ field in the samples is preferable, as this is a measure of the genotype call itself being correct. There may be some differences between HET and HOM for this due to differences in difficulty in making those types of calls. However, it should be lower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/547#issuecomment-1194472227
https://github.com/google/deepvariant/issues/548#issuecomment-1194463806:5,Availability,error,error,5,This error is most likely due to the corrupted tf records file which is the output of call_variants step. In your set up the intermediate results are dumped into /tmp. Could it be that you ran multiple instances of deepvariant simultaneously with the same output directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/548#issuecomment-1194463806
https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:2397,Integrability,Message,Message,2397,"A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FANUP4ZC4YG3AONVCKJ7RT53VW27C7ANCNFSM55E3G3QA&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Ci6wL9MBJUn9xtmb4eLWiMAZi%2BFEFI03EvFd2jZp0rc%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320
https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:376,Safety,safe,safelinks,376,"Amazing, thank you!; Amy. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: Kishwar Shafin ***@***.***>; Sent: Sunday, July 31, 2022 7:15:11 PM; To: google/deepvariant ***@***.***>; Cc: Amy Houseman ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] What hg38 would you suggest? (Issue #549). Hi @amyhouseman<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320
https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:987,Safety,safe,safelinks,987,"Amazing, thank you!; Amy. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: Kishwar Shafin ***@***.***>; Sent: Sunday, July 31, 2022 7:15:11 PM; To: google/deepvariant ***@***.***>; Cc: Amy Houseman ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] What hg38 would you suggest? (Issue #549). Hi @amyhouseman<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320
https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:1465,Safety,safe,safelinks,1465,"A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FANUP4ZC4YG3AONVCKJ7RT53VW27C7ANCNFSM55E3G3QA&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Ci6wL9MBJUn9xtmb4eLWiMAZi%2BFEFI03EvFd2jZp0rc%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320
https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:1911,Safety,safe,safelinks,1911,"A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FANUP4ZC4YG3AONVCKJ7RT53VW27C7ANCNFSM55E3G3QA&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Ci6wL9MBJUn9xtmb4eLWiMAZi%2BFEFI03EvFd2jZp0rc%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320
https://github.com/google/deepvariant/issues/550#issuecomment-1205591500:123,Safety,avoid,avoid,123,"hi @IndyHouseGuy ,. You can add ; ```; docker run -it -v /data:/data \; -u `id -u`:`id -g`; ```; to your docker command to avoid this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1205591500
https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:1818,Availability,echo,echo,1818,"/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/A2LCPRQWWLAYOZXICW5LXSDVXQAGNANCNFSM55QXIB6A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158
https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:2515,Integrability,Message,Message,2515,"/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/A2LCPRQWWLAYOZXICW5LXSDVXQAGNANCNFSM55QXIB6A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158
https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:2183,Safety,avoid,avoid,2183,"/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/A2LCPRQWWLAYOZXICW5LXSDVXQAGNANCNFSM55QXIB6A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158
https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:361,Security,secur,security,361,"Thank you for the quick response. I tried that and still got files by root. I tried setting the uid and; groupid explicitly and using the dynamic method you recommended. Please advise. command:. deep_dir=deep_variant_id80429g20. mkdir -p XXXXXXXXXXXXXXXXXX/$deep_dir. docker pull google/deepvariant:0.9.0. # This was ran, XXXXXXX are directories marked out for security purposes. docker run -it -u 80429:20 -v; XXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; XXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXXXXXXXXXXX/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@*",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158
https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:1252,Security,secur,security,1252,"or security purposes. docker run -it -u 80429:20 -v; XXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; XXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXXXXXXXXXXX/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158
https://github.com/google/deepvariant/issues/550#issuecomment-1206552255:46,Testability,test,test,46,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255
https://github.com/google/deepvariant/issues/550#issuecomment-1206552255:333,Testability,test,test,333,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255
https://github.com/google/deepvariant/issues/550#issuecomment-1206552255:38,Usability,simpl,simpler,38,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255
https://github.com/google/deepvariant/issues/550#issuecomment-1207586205:178,Security,access,access,178,"I've got it working now. What I reported earlier was correct and I'm happy to explore this further. Oddly enough, I now have a bed file owned by root as well. I do not have root access on this machine which explains why some HPC's have banned docker. I'll investigate the differences and report back. Good new is I have it working so there is no action need on your end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/550#issuecomment-1207586205
https://github.com/google/deepvariant/issues/552#issuecomment-1213590530:40,Availability,error,error,40,"@Argonvi here is some background on the error:. https://stackoverflow.com/questions/28185844/do-all-64-bit-intel-architectures-support-ssse3-sse4-1-sse4-2-instructions. My guess here is that the processor being used is either (a) older, or (b) incompatible with the dockerized version of DeepVariant. One way around this might be to try to build DeepVariant yourself.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1213590530
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:168,Availability,error,error,168,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3767,Availability,ERROR,ERROR,3767,"n: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4268,Availability,ERROR,ERROR,4268,"/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4787,Availability,ERROR,ERROR,4787,"t all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5212,Availability,ERROR,ERROR,5212,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6768,Availability,Down,Download,6768,"g; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7231,Availability,down,download,7231,"===== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7306,Availability,down,downloaded,7306,"===== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10285,Availability,ERROR,ERROR,10285,"8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13110,Availability,ERROR,ERROR,13110,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:369,Deployability,Install,Install,369,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:1009,Deployability,Update,Update,1009,"than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNIN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:1488,Deployability,Install,Install,1488," 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:1607,Deployability,Install,Install,1607,"a conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2686,Deployability,Install,Installing,2686,"oad Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2764,Deployability,install,installation,2764,"oad Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2899,Deployability,install,installed,2899,"packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3082,Deployability,install,installed,3082,"kages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3731,Deployability,Install,Install,3731,"n: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3863,Deployability,install,installed,3863,"n: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4364,Deployability,install,installed,4364,"/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4689,Deployability,Install,Install,4689,"22 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4731,Deployability,Install,Installing,4731,"22 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4883,Deployability,install,installed,4883,"t all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5104,Deployability,Install,Install,5104,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5178,Deployability,Install,Install,5178,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5308,Deployability,install,installed,5308,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5847,Deployability,Update,Update,5847,"ke into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/ke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6328,Deployability,Install,Install,6328,"owing dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6447,Deployability,Install,Install,6447,"atible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you lik",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6568,Deployability,install,installed,6568,"buf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6662,Deployability,Install,Install,6662,"buf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6706,Deployability,install,installed,6706,"buf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6954,Deployability,update,update-relnotes,6954,"BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6992,Deployability,install,installed,6992,"81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dyna",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7248,Deployability,release,release,7248,"===== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8294,Deployability,Configurat,Configuration,8294,"zation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8955,Deployability,install,installation,8955,a 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:9979,Deployability,Install,Installing,9979,"ly uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10381,Deployability,install,installed,10381,"8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10629,Deployability,install,installed,10629,"cting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:11780,Deployability,install,installation,11780,ge manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:12804,Deployability,Install,Installing,12804,"ly uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13206,Deployability,install,installed,13206,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13454,Deployability,install,installed,13454,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:186,Integrability,message,messages,186,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:209,Integrability,depend,dependencies,209,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3780,Integrability,depend,dependency,3780,"n: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3920,Integrability,depend,dependency,3920,"d in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4281,Integrability,depend,dependency,4281,"/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4421,Integrability,depend,dependency,4421,"ckages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4800,Integrability,depend,dependency,4800,"t all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4940,Integrability,depend,dependency,4940,"ncy conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5225,Integrability,depend,dependency,5225,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5365,Integrability,depend,dependency,5365,"es that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10298,Integrability,depend,dependency,10298,"8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10438,Integrability,depend,dependency,10438,"b/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13123,Integrability,depend,dependency,13123,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13263,Integrability,depend,dependency,13263,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:301,Modifiability,config,config,301,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:486,Modifiability,config,config,486,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6781,Modifiability,config,configure,6781,"g; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7399,Modifiability,config,config,7399,"kg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7490,Modifiability,config,configure,7490,": Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7543,Modifiability,config,configuring,7543,"n.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -ypa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7615,Modifiability,config,configs,7615,"lling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Igno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7666,Modifiability,config,config,7666,"'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7735,Modifiability,config,config,7735,":53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7777,Modifiability,config,config,7777," M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7873,Modifiability,config,config,7873,"071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7894,Modifiability,Config,Config,7894,"071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7941,Modifiability,config,config,7941,"te-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7985,Modifiability,config,config,7985," TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Success",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8073,Modifiability,config,config,8073,"sorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8162,Modifiability,config,configs,8162," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manage",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8206,Modifiability,config,config,8206," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manage",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8247,Modifiability,config,config,8247,"Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8294,Modifiability,Config,Configuration,8294,"zation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:296,Performance,Load,Load,296,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:481,Performance,Load,Load,481,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2377,Performance,cache,cached,2377,"an omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distributio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7333,Performance,optimiz,optimization,7333,"kg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:9662,Performance,cache,cached,9662,"ckages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:12487,Performance,cache,cached,12487,"ckages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279
https://github.com/google/deepvariant/issues/552#issuecomment-1219769574:61,Availability,avail,available,61,@Argonvi What hardware do you have? SSE4.1 instructions were available on Intel CPUs since 2009 (https://en.wikipedia.org/wiki/SSE4). If you have a very old hardware it might not be possible compile DeepVariant. Could you try to run it on a Cloud virtual machine?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/552#issuecomment-1219769574
https://github.com/google/deepvariant/issues/554#issuecomment-1223132769:112,Availability,avail,available,112,"hello @imdanique ,. The hybrid model is trained on PacBio HiFi + Illumina data, currently there's no model/mode available that supports ONT+Illumina data in the manner this hybrid mode is used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/554#issuecomment-1223132769
https://github.com/google/deepvariant/issues/555#issuecomment-1223153383:421,Availability,error,error,421,"Hi @tahashmi ,. It does look like a TensorFlow issue as described here: https://github.com/tensorflow/text/issues/385. You can check the tensorflow version of DeepVariant this way:; ```bash; singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu python3 -c 'import tensorflow as tf; print(tf.__version__)'; ```; And then please try to install the tensorflow version locally to see if the error gets fixed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555#issuecomment-1223153383
https://github.com/google/deepvariant/issues/555#issuecomment-1223153383:368,Deployability,install,install,368,"Hi @tahashmi ,. It does look like a TensorFlow issue as described here: https://github.com/tensorflow/text/issues/385. You can check the tensorflow version of DeepVariant this way:; ```bash; singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu python3 -c 'import tensorflow as tf; print(tf.__version__)'; ```; And then please try to install the tensorflow version locally to see if the error gets fixed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555#issuecomment-1223153383
https://github.com/google/deepvariant/issues/555#issuecomment-1229403857:254,Performance,cache,cached,254,"Thanks @kishwarshafin . ; But I think I am not even able to import TF in Python environment through DV container. . ```; (base) [tahmad@gcn4 ~]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu python3; INFO: Using cached SIF image; Python 3.8.10 (default, Mar 15 2022, 12:22:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tensorflow as tf; 2022-08-28 09:48:47.608744: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE; ```. on my local system, I have TF 2.5.2; ```; >>> import tensorflow as tf; 2022-08-28 09:51:55.901400: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; >>> print(tf.__version__); 2.5.2; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555#issuecomment-1229403857
https://github.com/google/deepvariant/issues/555#issuecomment-1229404701:119,Deployability,install,install,119,"For the time being, this issue has been resolved by updating my local TF to 2.7.0 which is used in DV container.; `pip install -U tensorflow==2.7.0`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/555#issuecomment-1229404701
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:67,Deployability,release,release,67,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:145,Deployability,pipeline,pipeline,145,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:109,Energy Efficiency,reduce,reduce,109,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:332,Energy Efficiency,reduce,reduce,332,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:869,Performance,perform,performs,869,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:1113,Performance,perform,performance,1113,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:132,Usability,simpl,simplify,132,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407
https://github.com/google/deepvariant/issues/559#issuecomment-1230609842:126,Deployability,install,installed,126,"@tahashmi,. It looks like [nucleus](https://github.com/google/nucleus) is having issues with the newer tensorflow version you installed. I'm not exactly sure why your installing tensorflow locally fixed the issue. Can you please see if you install nucleus locally, it fixes it:; ```; pip install --user google-nucleus; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559#issuecomment-1230609842
https://github.com/google/deepvariant/issues/559#issuecomment-1230609842:167,Deployability,install,installing,167,"@tahashmi,. It looks like [nucleus](https://github.com/google/nucleus) is having issues with the newer tensorflow version you installed. I'm not exactly sure why your installing tensorflow locally fixed the issue. Can you please see if you install nucleus locally, it fixes it:; ```; pip install --user google-nucleus; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559#issuecomment-1230609842
https://github.com/google/deepvariant/issues/559#issuecomment-1230609842:240,Deployability,install,install,240,"@tahashmi,. It looks like [nucleus](https://github.com/google/nucleus) is having issues with the newer tensorflow version you installed. I'm not exactly sure why your installing tensorflow locally fixed the issue. Can you please see if you install nucleus locally, it fixes it:; ```; pip install --user google-nucleus; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559#issuecomment-1230609842
https://github.com/google/deepvariant/issues/559#issuecomment-1230609842:288,Deployability,install,install,288,"@tahashmi,. It looks like [nucleus](https://github.com/google/nucleus) is having issues with the newer tensorflow version you installed. I'm not exactly sure why your installing tensorflow locally fixed the issue. Can you please see if you install nucleus locally, it fixes it:; ```; pip install --user google-nucleus; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/559#issuecomment-1230609842
https://github.com/google/deepvariant/issues/561#issuecomment-1234749060:216,Usability,simpl,simply,216,"@aalfi ,. Please follow the instructions here: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. I see that you have `--dry_run=true` which would not run anything and would simply print out the commands:; ```; --[no]dry_run: Optional. If True, only prints out commands without executing them.; (default: 'false'); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/561#issuecomment-1234749060
https://github.com/google/deepvariant/issues/562#issuecomment-1234776784:188,Modifiability,extend,extend,188,"Hi @RaphaelSanchesUSP . It will likely be very difficult to do so regardless of the approach. However, I'd like to better understand what sort of output you would look for? Would it be to extend DeepVariant's output classes beyond REF, HET, HOM into more than diploid output, or is it instead to train on polyploid species and repurpose HET to be (some number of non-ref and non-hom alleles without further specification)?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562#issuecomment-1234776784
https://github.com/google/deepvariant/issues/562#issuecomment-1234855822:254,Safety,predict,predictions,254,"Hi @AndrewCarroll . Thank you very much for your promptness and attention.; I verified that DeepVariant has in several parts of the code, parameters for diploid organisms and two alleles. Like the small example function below:. `def most_likely_genotype(predictions, ploidy=2, n_alleles=2):`. The mentioned function is used in the post-processing process of calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562#issuecomment-1234855822
https://github.com/google/deepvariant/issues/562#issuecomment-1235851209:565,Availability,error,error,565,"Hi @RaphaelSanchesUSP . Between the two approaches, this - **repurpose HET to be (some number of non-ref and non-hom alleles without further specification)?** is the one which would require much less work. Of the approaches, this might not require large changes and might be possible with training alone. One component that could be limiting for this approach is the ability to identify extremely well characterized samples that could be used as truth examples. Is there a polyploid species where the variants relative to the reference are known without almost any error, as is the case for the genome in a bottle samples for humans?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/562#issuecomment-1235851209
https://github.com/google/deepvariant/issues/564#issuecomment-1251331616:47,Availability,error,error,47,"@yangyxt was this resolved?; From the original error message, it seems to me that the input to call_variants was truncated. Which means that your make_examples run might have not been fully succeeded. Another possible issue is: If you happen to have multiple make_examples running and overwriting the same files, you also might have corrupted output from make_examples (which will cause the call_variants step to err out.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/564#issuecomment-1251331616
https://github.com/google/deepvariant/issues/564#issuecomment-1251331616:53,Integrability,message,message,53,"@yangyxt was this resolved?; From the original error message, it seems to me that the input to call_variants was truncated. Which means that your make_examples run might have not been fully succeeded. Another possible issue is: If you happen to have multiple make_examples running and overwriting the same files, you also might have corrupted output from make_examples (which will cause the call_variants step to err out.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/564#issuecomment-1251331616
https://github.com/google/deepvariant/issues/564#issuecomment-1253259881:342,Usability,feedback,feedback,342,"> I'm not very familiar with SINGULARITY_CACHEDIR. But, in your command, if you're running it 3 times, you should use a different --intermediate_results_dir. Output of `make_examples` will be written to that directory. So, if you use the same intermediate_results_dir, that might explain why your data is corrupted. Thank you. I will try and feedback to you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/564#issuecomment-1253259881
https://github.com/google/deepvariant/issues/565#issuecomment-1251305150:390,Usability,intuit,intuition,390,"Hi @SHuang-Broad . By default, DeepVariant only looks at the content of the QUAL field (column 11) in order to populate the quality values. DeepVariant is able to look at and read in arbitrary additional tags (e.g. we have used the HP tag for phasing in the past). We have not previously experimented with BAQ, but with the framework above it would not be hard to look at it if you have an intuition that it might help. If you think it is promising, we could either do this investigation ourselves, or we could try to give you some instructions on how to do an experimental training if you are interested. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/565#issuecomment-1251305150
https://github.com/google/deepvariant/issues/565#issuecomment-1252802626:194,Deployability,pipeline,pipeline,194,"@AndrewCarroll ; Thanks for the answer.; We're in a bit of a time crunch now (operations), but knowing how to run that experiment will be super helpful!. @amwenger ; Right, `pbmm2` (used in our pipeline) doesn't provide the `BQ:Z` tag.; However, we also run `samtools calmd` on the BAM to generate the `MD:Z` tag. And `calmd` allows one to compute the BAQ by turning on the `-r` flag (off in our pipeline now). But as you can imagine, it will not be negligible compute.; Hence we are interested in doing some experiments to see if DV can benefit from this tag. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/565#issuecomment-1252802626
https://github.com/google/deepvariant/issues/565#issuecomment-1252802626:396,Deployability,pipeline,pipeline,396,"@AndrewCarroll ; Thanks for the answer.; We're in a bit of a time crunch now (operations), but knowing how to run that experiment will be super helpful!. @amwenger ; Right, `pbmm2` (used in our pipeline) doesn't provide the `BQ:Z` tag.; However, we also run `samtools calmd` on the BAM to generate the `MD:Z` tag. And `calmd` allows one to compute the BAQ by turning on the `-r` flag (off in our pipeline now). But as you can imagine, it will not be negligible compute.; Hence we are interested in doing some experiments to see if DV can benefit from this tag. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/565#issuecomment-1252802626
https://github.com/google/deepvariant/issues/566#issuecomment-1253229584:393,Integrability,message,messages,393,"> Hi @yangyxt , can you try a command like: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity ?. Thanks for the response. I tried a command without --env argument in the very beginning and the warning logs were still what is in the screenshot above. Then I started to alter the env variable with --env argument and still got the same warning messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584
https://github.com/google/deepvariant/issues/566#issuecomment-1253229584:333,Modifiability,variab,variable,333,"> Hi @yangyxt , can you try a command like: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity ?. Thanks for the response. I tried a command without --env argument in the very beginning and the warning logs were still what is in the screenshot above. Then I started to alter the env variable with --env argument and still got the same warning messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584
https://github.com/google/deepvariant/issues/566#issuecomment-1253229584:252,Testability,log,logs,252,"> Hi @yangyxt , can you try a command like: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity ?. Thanks for the response. I tried a command without --env argument in the very beginning and the warning logs were still what is in the screenshot above. Then I started to alter the env variable with --env argument and still got the same warning messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584
https://github.com/google/deepvariant/issues/566#issuecomment-1253261845:59,Integrability,message,messages,59,"> . Yes. I didn't get unexpected results when only warning messages are shown. I just wonder whether the warning message would affect the results in some hidden way that I'm not aware of. So I tried to remove the warning message and ended up finding out it's somehow due to the lack of a dependency, locale, in the docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253261845
https://github.com/google/deepvariant/issues/566#issuecomment-1253261845:113,Integrability,message,message,113,"> . Yes. I didn't get unexpected results when only warning messages are shown. I just wonder whether the warning message would affect the results in some hidden way that I'm not aware of. So I tried to remove the warning message and ended up finding out it's somehow due to the lack of a dependency, locale, in the docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253261845
https://github.com/google/deepvariant/issues/566#issuecomment-1253261845:221,Integrability,message,message,221,"> . Yes. I didn't get unexpected results when only warning messages are shown. I just wonder whether the warning message would affect the results in some hidden way that I'm not aware of. So I tried to remove the warning message and ended up finding out it's somehow due to the lack of a dependency, locale, in the docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253261845
https://github.com/google/deepvariant/issues/566#issuecomment-1253261845:288,Integrability,depend,dependency,288,"> . Yes. I didn't get unexpected results when only warning messages are shown. I just wonder whether the warning message would affect the results in some hidden way that I'm not aware of. So I tried to remove the warning message and ended up finding out it's somehow due to the lack of a dependency, locale, in the docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1253261845
https://github.com/google/deepvariant/issues/566#issuecomment-1284747096:92,Integrability,message,messages,92,Thanks @yangyxt . Good to know that you're not seeing strange behaviors despite the warning messages. I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/566#issuecomment-1284747096
https://github.com/google/deepvariant/issues/568#issuecomment-1251633143:595,Deployability,update,update-if-youre-using-our-,595,"Hi @jkalleberg ,; Thank you for reaching out. The model ckpt you're using was older than v1.4. And you're right: in v1.4 we added an extra channel, and we haven't trained a new allele frequency model with v1.4. So we actually don't yet have a model that has both insert_size as well allele_frequency!. Two things:. 1. If you want to run v1.4.0 code with the older model (which didn't have the insert_size channel), you can add `,channels=''` to the end of your make_examples_extra_args. I added a section to my public gist here:; https://gist.github.com/pichuan/64d73bc965300645470eb29a66116593#update-if-youre-using-our-v140-docker-codebase; 2. I'm currently training a new WGS AF model that will have both the insert_size channel, as well as the allele_frequency channel. So, stay tuned! I can give you an update when I have it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143
https://github.com/google/deepvariant/issues/568#issuecomment-1251633143:808,Deployability,update,update,808,"Hi @jkalleberg ,; Thank you for reaching out. The model ckpt you're using was older than v1.4. And you're right: in v1.4 we added an extra channel, and we haven't trained a new allele frequency model with v1.4. So we actually don't yet have a model that has both insert_size as well allele_frequency!. Two things:. 1. If you want to run v1.4.0 code with the older model (which didn't have the insert_size channel), you can add `,channels=''` to the end of your make_examples_extra_args. I added a section to my public gist here:; https://gist.github.com/pichuan/64d73bc965300645470eb29a66116593#update-if-youre-using-our-v140-docker-codebase; 2. I'm currently training a new WGS AF model that will have both the insert_size channel, as well as the allele_frequency channel. So, stay tuned! I can give you an update when I have it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143
https://github.com/google/deepvariant/issues/568#issuecomment-1251633143:783,Performance,tune,tuned,783,"Hi @jkalleberg ,; Thank you for reaching out. The model ckpt you're using was older than v1.4. And you're right: in v1.4 we added an extra channel, and we haven't trained a new allele frequency model with v1.4. So we actually don't yet have a model that has both insert_size as well allele_frequency!. Two things:. 1. If you want to run v1.4.0 code with the older model (which didn't have the insert_size channel), you can add `,channels=''` to the end of your make_examples_extra_args. I added a section to my public gist here:; https://gist.github.com/pichuan/64d73bc965300645470eb29a66116593#update-if-youre-using-our-v140-docker-codebase; 2. I'm currently training a new WGS AF model that will have both the insert_size channel, as well as the allele_frequency channel. So, stay tuned! I can give you an update when I have it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143
https://github.com/google/deepvariant/issues/568#issuecomment-1251646879:788,Availability,error,error,788,"@pichuan Thank you for the quick reply. That's a good tip to bypass the insert_size channel by default. Looking forward to your update, much appreciated. If I understand correctly, each additional channel requires building a model using examples containing those channels. As a hypothetical example, to use `insert_size` + `allele_frequency` + `avg_base_quality` during variant calling would require re-training, correct? . I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251646879
https://github.com/google/deepvariant/issues/568#issuecomment-1251646879:128,Deployability,update,update,128,"@pichuan Thank you for the quick reply. That's a good tip to bypass the insert_size channel by default. Looking forward to your update, much appreciated. If I understand correctly, each additional channel requires building a model using examples containing those channels. As a hypothetical example, to use `insert_size` + `allele_frequency` + `avg_base_quality` during variant calling would require re-training, correct? . I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251646879
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:1175,Availability,error,error,1175,"annel requires building a model using examples containing those channels. As a hypothetical example, to use `insert_size` + `allele_frequency` + `avg_base_quality` during variant calling would require re-training, correct?. Yes. The make_examples stage will need to create examples that are consistent with the model.ckpt used in call_variants. Because the model.ckpt was already trained with a specific list of channels and shape. So, if you create different examples - even if it's just to remove a channel, you're suppose to retrain on examples that are made with that channel removed as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:1519,Availability,error,errors,1519,"hat are consistent with the model.ckpt used in call_variants. Because the model.ckpt was already trained with a specific list of channels and shape. So, if you create different examples - even if it's just to remove a channel, you're suppose to retrain on examples that are made with that channel removed as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and if the examples you're making ends up having 7 channels but different ones, then it should still give you an error. But, if you're using an older model (such as the AF model you're using), and if it didn't have a `model.ckpt.example_info.json` file with it, then I believe the curren",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:1625,Availability,checkpoint,checkpoint,1625," So, if you create different examples - even if it's just to remove a channel, you're suppose to retrain on examples that are made with that channel removed as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and if the examples you're making ends up having 7 channels but different ones, then it should still give you an error. But, if you're using an older model (such as the AF model you're using), and if it didn't have a `model.ckpt.example_info.json` file with it, then I believe the current behavior is that it will try to use the 7 channels directly. Which will cause the issue that it might use the weights for allele_frequency channe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:1658,Availability,checkpoint,checkpoint,1658," So, if you create different examples - even if it's just to remove a channel, you're suppose to retrain on examples that are made with that channel removed as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and if the examples you're making ends up having 7 channels but different ones, then it should still give you an error. But, if you're using an older model (such as the AF model you're using), and if it didn't have a `model.ckpt.example_info.json` file with it, then I believe the current behavior is that it will try to use the 7 channels directly. Which will cause the issue that it might use the weights for allele_frequency channe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:2318,Availability,error,error,2318,"mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and if the examples you're making ends up having 7 channels but different ones, then it should still give you an error. But, if you're using an older model (such as the AF model you're using), and if it didn't have a `model.ckpt.example_info.json` file with it, then I believe the current behavior is that it will try to use the 7 channels directly. Which will cause the issue that it might use the weights for allele_frequency channel for the insert_size channel. Does that make sense?. By the way, in case you haven't seen it, we have a nice blog post that talks about channels: https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:130,Deployability,update,update,130,"> @pichuan Thank you for the quick reply. That's a good tip to bypass the insert_size channel by default. Looking forward to your update, much appreciated.; > ; > If I understand correctly, each additional channel requires building a model using examples containing those channels. As a hypothetical example, to use `insert_size` + `allele_frequency` + `avg_base_quality` during variant calling would require re-training, correct?. Yes. The make_examples stage will need to create examples that are consistent with the model.ckpt used in call_variants. Because the model.ckpt was already trained with a specific list of channels and shape. So, if you create different examples - even if it's just to remove a channel, you're suppose to retrain on examples that are made with that channel removed as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:1810,Integrability,depend,depends,1810,"d as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and if the examples you're making ends up having 7 channels but different ones, then it should still give you an error. But, if you're using an older model (such as the AF model you're using), and if it didn't have a `model.ckpt.example_info.json` file with it, then I believe the current behavior is that it will try to use the 7 channels directly. Which will cause the issue that it might use the weights for allele_frequency channel for the insert_size channel. Does that make sense?. By the way, in case you haven't seen it, we have a nice blog post that talks about channels: https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213
https://github.com/google/deepvariant/issues/568#issuecomment-1252420669:354,Availability,Avail,Available,354,"@pichuan thank you for the discussion. And yes, that does make sense. That was my expectation, but I hadn't seen that blog post yet, so again, thanks!. Final question: besides `insert_size,` do any of these channels listed have model.ckpt files on GCP, like the old PopVCF one I used? ; ```; --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. I assume they do not, or they'd be listed [here](https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant/1.4.0?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false), correct? I want to confirm I'm keeping an eye out in the right place if/when any new checkpoints become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1252420669
https://github.com/google/deepvariant/issues/568#issuecomment-1252420669:851,Availability,checkpoint,checkpoints,851,"@pichuan thank you for the discussion. And yes, that does make sense. That was my expectation, but I hadn't seen that blog post yet, so again, thanks!. Final question: besides `insert_size,` do any of these channels listed have model.ckpt files on GCP, like the old PopVCF one I used? ; ```; --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. I assume they do not, or they'd be listed [here](https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant/1.4.0?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false), correct? I want to confirm I'm keeping an eye out in the right place if/when any new checkpoints become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1252420669
https://github.com/google/deepvariant/issues/568#issuecomment-1252420669:870,Availability,avail,available,870,"@pichuan thank you for the discussion. And yes, that does make sense. That was my expectation, but I hadn't seen that blog post yet, so again, thanks!. Final question: besides `insert_size,` do any of these channels listed have model.ckpt files on GCP, like the old PopVCF one I used? ; ```; --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. I assume they do not, or they'd be listed [here](https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant/1.4.0?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false), correct? I want to confirm I'm keeping an eye out in the right place if/when any new checkpoints become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1252420669
https://github.com/google/deepvariant/issues/568#issuecomment-1253235153:10,Availability,checkpoint,checkpoints,10,"New model checkpoints associated with new releases will be under gs://deepvariant/models/DeepVariant as you noticed. I mentioned that starting from v1.4.0, you can see this file:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. The ""channels"" values are enums. You can look them up in this proto:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/protos/deepvariant.proto#L1048. From the example above, it's saying that DeepVariant v1.4.0 WGS model has 7 channels, and they are:. ```; CH_READ_BASE = 1;; CH_BASE_QUALITY = 2;; CH_MAPPING_QUALITY = 3;; CH_STRAND = 4;; CH_READ_SUPPORTS_VARIANT = 5;; CH_BASE_DIFFERS_FROM_REF = 6;; CH_INSERT_SIZE = 19;; ```. Note that the allele frequency model isn't part of our regular release process yet. It's made public as part of our preprint https://doi.org/10.1101/2021.01.06.425550. Right now, we're retraining it when users request it. We're certainly hoping to see more uses cases (thank you for letting us know!). If it's become more mature, we can consider building it into part of our regular release process. (Adding more regular supports also means more overhead for each release, so we need to balance this carefully.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153
https://github.com/google/deepvariant/issues/568#issuecomment-1253235153:42,Deployability,release,releases,42,"New model checkpoints associated with new releases will be under gs://deepvariant/models/DeepVariant as you noticed. I mentioned that starting from v1.4.0, you can see this file:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. The ""channels"" values are enums. You can look them up in this proto:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/protos/deepvariant.proto#L1048. From the example above, it's saying that DeepVariant v1.4.0 WGS model has 7 channels, and they are:. ```; CH_READ_BASE = 1;; CH_BASE_QUALITY = 2;; CH_MAPPING_QUALITY = 3;; CH_STRAND = 4;; CH_READ_SUPPORTS_VARIANT = 5;; CH_BASE_DIFFERS_FROM_REF = 6;; CH_INSERT_SIZE = 19;; ```. Note that the allele frequency model isn't part of our regular release process yet. It's made public as part of our preprint https://doi.org/10.1101/2021.01.06.425550. Right now, we're retraining it when users request it. We're certainly hoping to see more uses cases (thank you for letting us know!). If it's become more mature, we can consider building it into part of our regular release process. (Adding more regular supports also means more overhead for each release, so we need to balance this carefully.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153
https://github.com/google/deepvariant/issues/568#issuecomment-1253235153:909,Deployability,release,release,909,"New model checkpoints associated with new releases will be under gs://deepvariant/models/DeepVariant as you noticed. I mentioned that starting from v1.4.0, you can see this file:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. The ""channels"" values are enums. You can look them up in this proto:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/protos/deepvariant.proto#L1048. From the example above, it's saying that DeepVariant v1.4.0 WGS model has 7 channels, and they are:. ```; CH_READ_BASE = 1;; CH_BASE_QUALITY = 2;; CH_MAPPING_QUALITY = 3;; CH_STRAND = 4;; CH_READ_SUPPORTS_VARIANT = 5;; CH_BASE_DIFFERS_FROM_REF = 6;; CH_INSERT_SIZE = 19;; ```. Note that the allele frequency model isn't part of our regular release process yet. It's made public as part of our preprint https://doi.org/10.1101/2021.01.06.425550. Right now, we're retraining it when users request it. We're certainly hoping to see more uses cases (thank you for letting us know!). If it's become more mature, we can consider building it into part of our regular release process. (Adding more regular supports also means more overhead for each release, so we need to balance this carefully.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153
https://github.com/google/deepvariant/issues/568#issuecomment-1253235153:1229,Deployability,release,release,1229,"New model checkpoints associated with new releases will be under gs://deepvariant/models/DeepVariant as you noticed. I mentioned that starting from v1.4.0, you can see this file:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. The ""channels"" values are enums. You can look them up in this proto:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/protos/deepvariant.proto#L1048. From the example above, it's saying that DeepVariant v1.4.0 WGS model has 7 channels, and they are:. ```; CH_READ_BASE = 1;; CH_BASE_QUALITY = 2;; CH_MAPPING_QUALITY = 3;; CH_STRAND = 4;; CH_READ_SUPPORTS_VARIANT = 5;; CH_BASE_DIFFERS_FROM_REF = 6;; CH_INSERT_SIZE = 19;; ```. Note that the allele frequency model isn't part of our regular release process yet. It's made public as part of our preprint https://doi.org/10.1101/2021.01.06.425550. Right now, we're retraining it when users request it. We're certainly hoping to see more uses cases (thank you for letting us know!). If it's become more mature, we can consider building it into part of our regular release process. (Adding more regular supports also means more overhead for each release, so we need to balance this carefully.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153
https://github.com/google/deepvariant/issues/568#issuecomment-1253235153:1310,Deployability,release,release,1310,"New model checkpoints associated with new releases will be under gs://deepvariant/models/DeepVariant as you noticed. I mentioned that starting from v1.4.0, you can see this file:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. The ""channels"" values are enums. You can look them up in this proto:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/protos/deepvariant.proto#L1048. From the example above, it's saying that DeepVariant v1.4.0 WGS model has 7 channels, and they are:. ```; CH_READ_BASE = 1;; CH_BASE_QUALITY = 2;; CH_MAPPING_QUALITY = 3;; CH_STRAND = 4;; CH_READ_SUPPORTS_VARIANT = 5;; CH_BASE_DIFFERS_FROM_REF = 6;; CH_INSERT_SIZE = 19;; ```. Note that the allele frequency model isn't part of our regular release process yet. It's made public as part of our preprint https://doi.org/10.1101/2021.01.06.425550. Right now, we're retraining it when users request it. We're certainly hoping to see more uses cases (thank you for letting us know!). If it's become more mature, we can consider building it into part of our regular release process. (Adding more regular supports also means more overhead for each release, so we need to balance this carefully.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153
https://github.com/google/deepvariant/issues/568#issuecomment-1256836258:190,Usability,feedback,feedback,190,"Hi @jkalleberg ,; please see See: https://gist.github.com/pichuan/7ad09bf1fa8f519facf6806eca835ea6. I'll close this issue for now. Feel free to open more issues if you have any questions or feedback for us.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/568#issuecomment-1256836258
https://github.com/google/deepvariant/issues/569#issuecomment-1264952905:639,Usability,learn,learned,639,"Hi @guandailu . We have investigated the use of the MarkDuplicates step. In WGS and exome samples we looked at, we observe no measurable difference at coverages 30x and higher. For coverages lower than 30x, we observe a very slight advantage for MarkDuplicates. For this reason in our [best practices recommendation](https://github.com/google/deepvariant/blob/r1.4/docs/trio-merge-case-study.md), we indicate duplicate marking as optional. . In terms of other preprocessing steps, we observe that running Base Quality Score Recalibrator (BQSR) consistently results in a slight reduction of accuracy (likely because DeepVariant has already learned how to use the underlying concept to recalibrate qualities as it considers calling). We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants. If you would like to filter for higher precision than the default calls, we recommend using the Genotype Quality (GQ) field, setting a higher threshold based on your desire for specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/569#issuecomment-1264952905
https://github.com/google/deepvariant/issues/571#issuecomment-1273885517:85,Deployability,update,update,85,"Hello,; I created a tracking bug to add this feature to DeepVariant. We will post an update once we finalize the plan for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1273885517
https://github.com/google/deepvariant/issues/571#issuecomment-1273885517:131,Deployability,release,release,131,"Hello,; I created a tracking bug to add this feature to DeepVariant. We will post an update once we finalize the plan for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1273885517
https://github.com/google/deepvariant/issues/571#issuecomment-1274112172:47,Integrability,depend,depend,47,"Hi @mdriller . My answer to your question will depend on what exactly you will need from Plink and what sort of cohort approach you have. . If you just want to be able to run Plink on the joint genotype results, I wonder if you can try following the process which was performed for UKBiobank to convert their DeepVariant exome joint calls into PLINK format. That is the section **Conversion of pVCF to PLINK and BGEN files** [from the UKBiobank WES Protocol](https://biobank.ctsu.ox.ac.uk/crystal/ukb/docs/UKB_WES_Protocol.pdf). I hope this will work, as it is not generally our preference to replicate the functionality of -ERC BP_RESOLUTION, and this is likely to make writing output much slower. If, instead, you want calls at specific sites (similar to a genotyping chip approach but with NGS data), I would say that is is possible to force genotyping at a given set of alleles with one of the modules of DeepVariant (VCF candidate importer). I suspect this isn't what you want though. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172
https://github.com/google/deepvariant/issues/571#issuecomment-1274112172:449,Integrability,Protocol,Protocol,449,"Hi @mdriller . My answer to your question will depend on what exactly you will need from Plink and what sort of cohort approach you have. . If you just want to be able to run Plink on the joint genotype results, I wonder if you can try following the process which was performed for UKBiobank to convert their DeepVariant exome joint calls into PLINK format. That is the section **Conversion of pVCF to PLINK and BGEN files** [from the UKBiobank WES Protocol](https://biobank.ctsu.ox.ac.uk/crystal/ukb/docs/UKB_WES_Protocol.pdf). I hope this will work, as it is not generally our preference to replicate the functionality of -ERC BP_RESOLUTION, and this is likely to make writing output much slower. If, instead, you want calls at specific sites (similar to a genotyping chip approach but with NGS data), I would say that is is possible to force genotyping at a given set of alleles with one of the modules of DeepVariant (VCF candidate importer). I suspect this isn't what you want though. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172
https://github.com/google/deepvariant/issues/571#issuecomment-1274112172:268,Performance,perform,performed,268,"Hi @mdriller . My answer to your question will depend on what exactly you will need from Plink and what sort of cohort approach you have. . If you just want to be able to run Plink on the joint genotype results, I wonder if you can try following the process which was performed for UKBiobank to convert their DeepVariant exome joint calls into PLINK format. That is the section **Conversion of pVCF to PLINK and BGEN files** [from the UKBiobank WES Protocol](https://biobank.ctsu.ox.ac.uk/crystal/ukb/docs/UKB_WES_Protocol.pdf). I hope this will work, as it is not generally our preference to replicate the functionality of -ERC BP_RESOLUTION, and this is likely to make writing output much slower. If, instead, you want calls at specific sites (similar to a genotyping chip approach but with NGS data), I would say that is is possible to force genotyping at a given set of alleles with one of the modules of DeepVariant (VCF candidate importer). I suspect this isn't what you want though. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172
https://github.com/google/deepvariant/issues/571#issuecomment-1283869277:68,Integrability,Protocol,Protocol,68,"Hi all,. sorry about the late reply I was testing the UKBiobank WES Protocol provided by Andrew but unfortunately it does not seem fix our problem.; The general issue is that to identify runs of homozygosity(ROH) with plink you can also just provide a vcf file but this vcf file needs a base resolution e.g. an entry for each position, whether it is variable or not:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT PacBio_CCS; SUPER_1 1 . C . . . DP=49 GT:AD:DP:RGQ 0/0:49:49:99; SUPER_1 2 . C . . . DP=50 GT:AD:DP:RGQ 0/0:50:50:99; SUPER_1 3 . T . . . DP=54 GT:AD:DP:RGQ 0/0:54:54:99; SUPER_1 4 . A . . . DP=61 GT:AD:DP:RGQ 0/0:61:61:99; ... And we were just wondering if there is a possibility to generate such a vcf file using DeepVariant. Thanks again for all the replies and help. best regards,. Max",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277
https://github.com/google/deepvariant/issues/571#issuecomment-1283869277:350,Modifiability,variab,variable,350,"Hi all,. sorry about the late reply I was testing the UKBiobank WES Protocol provided by Andrew but unfortunately it does not seem fix our problem.; The general issue is that to identify runs of homozygosity(ROH) with plink you can also just provide a vcf file but this vcf file needs a base resolution e.g. an entry for each position, whether it is variable or not:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT PacBio_CCS; SUPER_1 1 . C . . . DP=49 GT:AD:DP:RGQ 0/0:49:49:99; SUPER_1 2 . C . . . DP=50 GT:AD:DP:RGQ 0/0:50:50:99; SUPER_1 3 . T . . . DP=54 GT:AD:DP:RGQ 0/0:54:54:99; SUPER_1 4 . A . . . DP=61 GT:AD:DP:RGQ 0/0:61:61:99; ... And we were just wondering if there is a possibility to generate such a vcf file using DeepVariant. Thanks again for all the replies and help. best regards,. Max",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277
https://github.com/google/deepvariant/issues/571#issuecomment-1283869277:42,Testability,test,testing,42,"Hi all,. sorry about the late reply I was testing the UKBiobank WES Protocol provided by Andrew but unfortunately it does not seem fix our problem.; The general issue is that to identify runs of homozygosity(ROH) with plink you can also just provide a vcf file but this vcf file needs a base resolution e.g. an entry for each position, whether it is variable or not:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT PacBio_CCS; SUPER_1 1 . C . . . DP=49 GT:AD:DP:RGQ 0/0:49:49:99; SUPER_1 2 . C . . . DP=50 GT:AD:DP:RGQ 0/0:50:50:99; SUPER_1 3 . T . . . DP=54 GT:AD:DP:RGQ 0/0:54:54:99; SUPER_1 4 . A . . . DP=61 GT:AD:DP:RGQ 0/0:61:61:99; ... And we were just wondering if there is a possibility to generate such a vcf file using DeepVariant. Thanks again for all the replies and help. best regards,. Max",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277
https://github.com/google/deepvariant/issues/571#issuecomment-1284818258:196,Energy Efficiency,efficient,efficiently,196,"Hi @mdriller . DeepVariant itself does not have this option. . However, as I understand you really just want to convert a single sample gVCF to a reference expanded VCF. bcftools may allow you to efficiently do this with the option:. `bcftools convert --gvcf2vcf ${VCF} --fasta-ref ${REF}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/571#issuecomment-1284818258
https://github.com/google/deepvariant/issues/572#issuecomment-1275106892:98,Availability,ping,ping,98,@FarmOmics we are planning on a release very soon that will enable RNA-seq variant calling. I can ping you on this issue once that release is out.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1275106892
https://github.com/google/deepvariant/issues/572#issuecomment-1275106892:32,Deployability,release,release,32,@FarmOmics we are planning on a release very soon that will enable RNA-seq variant calling. I can ping you on this issue once that release is out.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1275106892
https://github.com/google/deepvariant/issues/572#issuecomment-1275106892:131,Deployability,release,release,131,@FarmOmics we are planning on a release very soon that will enable RNA-seq variant calling. I can ping you on this issue once that release is out.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1275106892
https://github.com/google/deepvariant/issues/572#issuecomment-1280173477:19,Deployability,release,released,19,@FarmOmics we have released an RNA-seq model and case study. Let me know if you have any questions. https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1280173477
https://github.com/google/deepvariant/issues/572#issuecomment-1281335694:76,Integrability,wrap,wrapper,76,"Hi @FarmOmics ,. The `--model_type=WES` is a shorthand specifically for the wrapper script ""run_deepvariant.py"". Specifically here; https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L239. In https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md , you'll see that the RNAseq run actually overwrites these arguments:; with:; ```; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; ```. which basically means: If you run `make_examples` on your own (without using the wrapper script ""run_deepvariant.py""), you'll want to provide `--split_skip_reads=true`, but not providing `channels`. And we also provided RNAseq model with:; ```; --customized_model=model/model.ckpt \; ```. So, https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md isn't actually using WES model at all. (In the future we'll think about how to make this less confusing.). In terms of preprocessing for RNAseq data, the important flag to add is `--split_skip_reads` to make_examples. Let me know if I can help clarifying with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1281335694
https://github.com/google/deepvariant/issues/572#issuecomment-1281335694:542,Integrability,wrap,wrapper,542,"Hi @FarmOmics ,. The `--model_type=WES` is a shorthand specifically for the wrapper script ""run_deepvariant.py"". Specifically here; https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L239. In https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md , you'll see that the RNAseq run actually overwrites these arguments:; with:; ```; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; ```. which basically means: If you run `make_examples` on your own (without using the wrapper script ""run_deepvariant.py""), you'll want to provide `--split_skip_reads=true`, but not providing `channels`. And we also provided RNAseq model with:; ```; --customized_model=model/model.ckpt \; ```. So, https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md isn't actually using WES model at all. (In the future we'll think about how to make this less confusing.). In terms of preprocessing for RNAseq data, the important flag to add is `--split_skip_reads` to make_examples. Let me know if I can help clarifying with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1281335694
https://github.com/google/deepvariant/issues/572#issuecomment-1284746356:67,Usability,feedback,feedback,67,"Hi @FarmOmics ,; I'll close this issue. We would love to hear your feedback about the RNAseq caller. Please don't hesitate to reach out again if you have more questions or feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1284746356
https://github.com/google/deepvariant/issues/572#issuecomment-1284746356:172,Usability,feedback,feedback,172,"Hi @FarmOmics ,; I'll close this issue. We would love to hear your feedback about the RNAseq caller. Please don't hesitate to reach out again if you have more questions or feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/572#issuecomment-1284746356
https://github.com/google/deepvariant/issues/574#issuecomment-1276221161:134,Availability,error,error,134,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161
https://github.com/google/deepvariant/issues/574#issuecomment-1276221161:140,Integrability,message,messages,140,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161
https://github.com/google/deepvariant/issues/574#issuecomment-1276221161:234,Usability,learn,learn,234,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161
https://github.com/google/deepvariant/issues/574#issuecomment-1277509481:800,Testability,log,log,800,"Command Used: sudo docker run -v ""${PWD}"":""/input"" -v ""${PWD}/output"":""/output"" -v /Resources/:/Res deeptrio_gpu:latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type WGS --call_variants_extra_args=""use_openvino=true"" --ref=/Res/Hg19_chr/hg19.fa --reads_child /input/41420446-BABY.bam --reads_parent1 /input/41420446-FB.bam --reads_parent2 /input/41420446-MB.bam --output_vcf_child /output/Baby.deeptrio.vcf.gz --output_vcf_parent1 /output/Fb.deeptrio.vcf.gz --output_vcf_parent2 /output/Mb.deeptrio.vcf.gz --sample_name_child 'Baby' --sample_name_parent1 'Fb' --sample_name_parent2 'Mb' --num_shards=38 --output_gvcf_child /output/Baby.deeptrio.g.vcf.gz --output_gvcf_parent1 /output/Fb.deeptrio.g.vcf.gz --output_gvcf_parent2 /output/Mb.deeptrio.g.vcf.gz. I have captured the outputs in a log file. Do suggest how to share the same with you?. ![image](https://user-images.githubusercontent.com/27851922/195591985-6c022925-816b-4bce-af50-8d31377b84d9.png). ![image](https://user-images.githubusercontent.com/27851922/195592082-e7e60d2d-92d6-431e-b4a2-a35c22314417.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1277509481
https://github.com/google/deepvariant/issues/574#issuecomment-1277975462:151,Availability,error,error,151,"This is probably causing the issues `--call_variants_extra_args=""use_openvino=true""` as openvino is not currently supported (#541) and throws the same error you see here. Running without that extra arg should then work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/574#issuecomment-1277975462
https://github.com/google/deepvariant/issues/577#issuecomment-1284441913:141,Availability,error,error,141,"@akolesnikov ; Thank you for the answer. All paths (both directories and fiels) that I included in my script do exist. Still, it shows me an error as file doesn't exist as I wrote in my first message above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284441913
https://github.com/google/deepvariant/issues/577#issuecomment-1284441913:192,Integrability,message,message,192,"@akolesnikov ; Thank you for the answer. All paths (both directories and fiels) that I included in my script do exist. Still, it shows me an error as file doesn't exist as I wrote in my first message above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284441913
https://github.com/google/deepvariant/issues/577#issuecomment-1284947746:389,Security,access,access,389,"@pichuan ; I tried to run; ; ```; bash; BIN_VERSION=""1.4.0""; docker run --gpus all \; -v ""${INPUT_DIR}"":""/data/deepvariant/test_bam"" \; -v ""${OUTPUT_DIR}"":""/data/deepvariant/output"" \; -v ""${REF}"":""/data/PublicData/hg38"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /data/deepvariant/test_bam/*bam*; ```. Strangely, it showed that I have 3 files matching the wildcard; ```bash; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.du': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.md5': No such file or directory; ```; Then I tried to ls directory itself:; ```bash ; ls: cannot access '/data/deepvariant/test_bam/': No such file or directory; ```. I also checked all the permissions and rw perrmissions are granted recursively",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284947746
https://github.com/google/deepvariant/issues/577#issuecomment-1284947746:501,Security,access,access,501,"@pichuan ; I tried to run; ; ```; bash; BIN_VERSION=""1.4.0""; docker run --gpus all \; -v ""${INPUT_DIR}"":""/data/deepvariant/test_bam"" \; -v ""${OUTPUT_DIR}"":""/data/deepvariant/output"" \; -v ""${REF}"":""/data/PublicData/hg38"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /data/deepvariant/test_bam/*bam*; ```. Strangely, it showed that I have 3 files matching the wildcard; ```bash; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.du': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.md5': No such file or directory; ```; Then I tried to ls directory itself:; ```bash ; ls: cannot access '/data/deepvariant/test_bam/': No such file or directory; ```. I also checked all the permissions and rw perrmissions are granted recursively",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284947746
https://github.com/google/deepvariant/issues/577#issuecomment-1284947746:630,Security,access,access,630,"@pichuan ; I tried to run; ; ```; bash; BIN_VERSION=""1.4.0""; docker run --gpus all \; -v ""${INPUT_DIR}"":""/data/deepvariant/test_bam"" \; -v ""${OUTPUT_DIR}"":""/data/deepvariant/output"" \; -v ""${REF}"":""/data/PublicData/hg38"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /data/deepvariant/test_bam/*bam*; ```. Strangely, it showed that I have 3 files matching the wildcard; ```bash; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.du': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.md5': No such file or directory; ```; Then I tried to ls directory itself:; ```bash ; ls: cannot access '/data/deepvariant/test_bam/': No such file or directory; ```. I also checked all the permissions and rw perrmissions are granted recursively",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284947746
https://github.com/google/deepvariant/issues/577#issuecomment-1284947746:813,Security,access,access,813,"@pichuan ; I tried to run; ; ```; bash; BIN_VERSION=""1.4.0""; docker run --gpus all \; -v ""${INPUT_DIR}"":""/data/deepvariant/test_bam"" \; -v ""${OUTPUT_DIR}"":""/data/deepvariant/output"" \; -v ""${REF}"":""/data/PublicData/hg38"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /data/deepvariant/test_bam/*bam*; ```. Strangely, it showed that I have 3 files matching the wildcard; ```bash; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.du': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.md5': No such file or directory; ```; Then I tried to ls directory itself:; ```bash ; ls: cannot access '/data/deepvariant/test_bam/': No such file or directory; ```. I also checked all the permissions and rw perrmissions are granted recursively",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284947746
https://github.com/google/deepvariant/issues/577#issuecomment-1284961760:15,Availability,echo,echo,15,"Can you ; ```; echo ${INPUT_DIR}; ```; to confirm that the value is what you expected?; And, just to be sure:; ```; ls ${INPUT_DIR}; ```; to make sure there are files there?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284961760
https://github.com/google/deepvariant/issues/577#issuecomment-1284981058:22,Availability,echo,echo,22,"@pichuan ; When I run echo, the output is empty. ; When I run; ```bash; ls ${INPUT_DIR}; ```; it shows a list of files of pwd, but not /data/deepvariant/test_bam/. If I run the script from /data/deepvariant directory, then it shows me files in /data/deepvariant, and if I run it from /data, then it shows me ls of /data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1284981058
https://github.com/google/deepvariant/issues/577#issuecomment-1285622624:32,Deployability,update,update,32,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624
https://github.com/google/deepvariant/issues/577#issuecomment-1285622624:357,Modifiability,variab,variables,357,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624
https://github.com/google/deepvariant/issues/577#issuecomment-1285622624:303,Usability,simpl,simplifying,303,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624
https://github.com/google/deepvariant/issues/577#issuecomment-1700305170:864,Availability,error,error,864,"Hello, ; I have encountered the same problem as mentioned in this issue. ; And I have also tried the solutions provided above, but the deepvariant binary still cannot see the bam files.; The version I'm using is: google/deepvariant:""1.5.0"".; Here is the command i run:; ```; docker run \; -v ""/data2/share/home/liyi/TEs/dv/input"":""/input"" \; -v ""/data2/share/home/liyi/TEs/dv/output"":""/output"" \; google/deepvariant:""1.5.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/data2/share/home/liyi/TEs/dv/input/C162-2_final.fasta \; --reads=/data2/share/home/liyi/TEs/dv/input/C111_mapped.bam \; --output_vcf=/data2/share/home/liyi/TEs/dv/output/C111.vcf.gz \; --output_gvcf=/data2/share/home/liyi/TEs/dv/output/C111.g.vcf.gz \; --intermediate_results_dir /data2/share/home/liyi/TEs/dv/output/intermediate_results_dir \; --num_shards=5; ```; The error output is: ; ```; [E::hts_open_format] Failed to open file ""/data2/share/home/liyi/TEs/dv/input/C111_mapped.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3_jead3w/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3_jead3w/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3_jead3w/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3_jead3w/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_3_jead3w/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_3_jead3w/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_3_jead3w/runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1700305170
https://github.com/google/deepvariant/issues/577#issuecomment-1700327386:598,Availability,avail,available,598,"Hi @LiYi0604,. Just change your script to the following, and it should run:. ```; docker run \; -v ""/data2/share/home/liyi/TEs/dv/input"":""/input"" \; -v ""/data2/share/home/liyi/TEs/dv/output"":""/output"" \; google/deepvariant:""1.5.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/C162-2_final.fasta \; --reads=/input/C111_mapped.bam \; --output_vcf=/output/C111.vcf.gz \; --output_gvcf=/output/C111.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=5; ```. Basically `""/data2/share/home/liyi/TEs/dv/input"":""/input""` makes the folder available inside the docker container as `/input`, so you don't need the long name. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1700327386
https://github.com/google/deepvariant/issues/577#issuecomment-1700399250:79,Availability,error,error,79,"@pgrosu ; Thanks for the answer.; I've tried this command, and it got the same error.; ```; [E::hts_open_format] Failed to open file ""/input/C111_mapped.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_g1m5s08g/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: NOT_FOUND: Could not open /input/C111_mapped.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples --m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1700399250
https://github.com/google/deepvariant/issues/577#issuecomment-1702203397:66,Availability,reboot,rebooting,66,"Hi Paul,; Sorry for the late reply, the server has just completed rebooting. I ran the code, and the output is:; ```; lrwxrwxrwx 1 1010 1010 66 Aug 31 02:25 C111_mapped.bam -> /data2/share/home/liyi/TEs/Pan-genome/Giraffe/C111/C111_mapped.bam; lrwxrwxrwx 1 1010 1010 58 Aug 31 02:25 C162-2_final.fasta -> /data2/share/home/liyi/TEs/Genome/fasta/C162-2_final.fasta; ```; Looks the docker knows where the file is. And this is my Linux info and docker version:; ```; Linux mgr 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux; Docker version 18.03.1-ce, build 9ee9f40; ```; I'm new to Linux, hoping this can provide useful information. Thanks, ; LiYi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1702203397
https://github.com/google/deepvariant/issues/577#issuecomment-1702219580:272,Deployability,update,updated,272,"Hi LiYi,. No worries, and this is very helpful! So this is a good sign, but here's the issue. The files in `/data2/share/home/liyi/TEs/dv/input` actually point to other directories that are not connected to Docker. It is best to just refer to them directly. For that I've updated your script to the following:. ```; docker run \; -v ""/data2/share/home/liyi/TEs/Pan-genome/Giraffe/C111"":""/input"" \; -v ""/data2/share/home/liyi/TEs/dv/output"":""/output"" \; -v ""/data2/share/home/liyi/TEs/Genome/fasta"":""/fasta"" \; google/deepvariant:""1.5.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/fasta/C162-2_final.fasta \; --reads=/input/C111_mapped.bam \; --output_vcf=/output/C111.vcf.gz \; --output_gvcf=/output/C111.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=5. ```. Let me know if this works for you. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1702219580
https://github.com/google/deepvariant/issues/577#issuecomment-1702241369:74,Availability,error,error,74,"Hi Paul, ; Thank you for the answer, it works!; Although I still got some error about the format of BAM file, I can try if I can resolve it on my own. Thanks again!; LiYi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/577#issuecomment-1702241369
https://github.com/google/deepvariant/issues/578#issuecomment-1291218327:77,Availability,error,error-rate,77,"@olechnwin ,. Looking at the log, it is generating a lot of examples. Is the error-rate of the `scaffolds_FINAL.fasta` too high? Can you give a little more context on what type of genome you are running this on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/578#issuecomment-1291218327
https://github.com/google/deepvariant/issues/578#issuecomment-1291218327:29,Testability,log,log,29,"@olechnwin ,. Looking at the log, it is generating a lot of examples. Is the error-rate of the `scaffolds_FINAL.fasta` too high? Can you give a little more context on what type of genome you are running this on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/578#issuecomment-1291218327
https://github.com/google/deepvariant/issues/578#issuecomment-1291993883:68,Availability,error,error,68,"@kishwarshafin,. Thank you so much for your reply. How do I get the error rate of the genome assembly? This is a human cancer cell line. I ran quast on the assembly comparing it with hg19 and the number of misassemblies ~2000 and the number of mismatches per 100kb is 124.3. Why does it generate a lot of examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/578#issuecomment-1291993883
https://github.com/google/deepvariant/issues/578#issuecomment-1317450563:272,Energy Efficiency,adapt,adapt,272,"Hi @olechnwin ,. The thresholds are usually chosen empirically. Based on what tasks we're trying to achieve, we choose it to find the best tradeoff between sensitivity and the amount of noises we bring in. This is more a research problem, especially that you're trying to adapt DeepVariant code to a different problem. So we won't be able to easily share a recipe here for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/578#issuecomment-1317450563
https://github.com/google/deepvariant/issues/578#issuecomment-1317450563:272,Modifiability,adapt,adapt,272,"Hi @olechnwin ,. The thresholds are usually chosen empirically. Based on what tasks we're trying to achieve, we choose it to find the best tradeoff between sensitivity and the amount of noises we bring in. This is more a research problem, especially that you're trying to adapt DeepVariant code to a different problem. So we won't be able to easily share a recipe here for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/578#issuecomment-1317450563
https://github.com/google/deepvariant/issues/579#issuecomment-1292297608:26,Availability,error,error,26,"Hello, as you can see the error message is ""samtools: command not found"". Can you please see if there's a way you can install samtools for your environment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608
https://github.com/google/deepvariant/issues/579#issuecomment-1292297608:118,Deployability,install,install,118,"Hello, as you can see the error message is ""samtools: command not found"". Can you please see if there's a way you can install samtools for your environment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608
https://github.com/google/deepvariant/issues/579#issuecomment-1292297608:32,Integrability,message,message,32,"Hello, as you can see the error message is ""samtools: command not found"". Can you please see if there's a way you can install samtools for your environment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608
https://github.com/google/deepvariant/issues/579#issuecomment-1293113502:72,Deployability,install,installed,72,> . Thanks for answering. I forgot to say that the samtools are already installed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/579#issuecomment-1293113502
https://github.com/google/deepvariant/issues/579#issuecomment-1297628315:151,Availability,error,error,151,"@moyarod ,. In that case, it would be ideal if you seek help from the galaxy community: https://help.galaxyproject.org/ as this is a platform-specific error and we can't reproduce this on our end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/579#issuecomment-1297628315
https://github.com/google/deepvariant/issues/580#issuecomment-1303821922:33,Deployability,install,install,33,@weilu1998 you shouldn't have to install any additional software if you are using singularity. Can you try running singularity using the `--cleanenv` flag? This will prevent locally installed libraries from being used (which can cause conflicts).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1303821922
https://github.com/google/deepvariant/issues/580#issuecomment-1303821922:182,Deployability,install,installed,182,@weilu1998 you shouldn't have to install any additional software if you are using singularity. Can you try running singularity using the `--cleanenv` flag? This will prevent locally installed libraries from being used (which can cause conflicts).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1303821922
https://github.com/google/deepvariant/issues/580#issuecomment-1304582333:91,Availability,error,error,91,"Hi @danielecook ,. Thanks for the reply. I retried adding --cleanenv flag and get the same error. I also cleanup the tmp folder. Here is the command I tried. ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/tmp singularity run --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304582333
https://github.com/google/deepvariant/issues/580#issuecomment-1304596419:69,Deployability,install,installed,69,Does the message still appear to indicate that it is using libraries installed on your machine rather than those present in the container?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304596419
https://github.com/google/deepvariant/issues/580#issuecomment-1304596419:9,Integrability,message,message,9,Does the message still appear to indicate that it is using libraries installed on your machine rather than those present in the container?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304596419
https://github.com/google/deepvariant/issues/580#issuecomment-1304645106:9,Availability,error,error,9,"Yes, the error log is the same. I also added `unset PYTHONPATH` before the singularity command. How can I prevent it from ; using the local libraries? . ```; INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal import api_implementation; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/internal/api_implementation.py"", line 104, in <module>; from google.protobuf.pyext import _message; TypeError: bases must be types; INFO: Cleaning up image...; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106
https://github.com/google/deepvariant/issues/580#issuecomment-1304645106:197,Modifiability,sandbox,sandbox,197,"Yes, the error log is the same. I also added `unset PYTHONPATH` before the singularity command. How can I prevent it from ; using the local libraries? . ```; INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal import api_implementation; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/internal/api_implementation.py"", line 104, in <module>; from google.protobuf.pyext import _message; TypeError: bases must be types; INFO: Cleaning up image...; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106
https://github.com/google/deepvariant/issues/580#issuecomment-1304645106:15,Testability,log,log,15,"Yes, the error log is the same. I also added `unset PYTHONPATH` before the singularity command. How can I prevent it from ; using the local libraries? . ```; INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal import api_implementation; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/internal/api_implementation.py"", line 104, in <module>; from google.protobuf.pyext import _message; TypeError: bases must be types; INFO: Cleaning up image...; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106
https://github.com/google/deepvariant/issues/580#issuecomment-1304645106:197,Testability,sandbox,sandbox,197,"Yes, the error log is the same. I also added `unset PYTHONPATH` before the singularity command. How can I prevent it from ; using the local libraries? . ```; INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal import api_implementation; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/internal/api_implementation.py"", line 104, in <module>; from google.protobuf.pyext import _message; TypeError: bases must be types; INFO: Cleaning up image...; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106
https://github.com/google/deepvariant/issues/580#issuecomment-1304648870:95,Usability,guid,guides,95,"I am guessing it could be related to the use of the `--nv` flag ([docs](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html)), but I am not familiar with using GPUs with singularity containers. This [guide](https://modinst.lu.lv/wp-content/uploads/2021/03/Singularity_seminars_Aleksandrs_Gutcaits.pdf) suggests unsetting the `LD_LIBRARY_PATH`. Which version of singularity are you using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304648870
https://github.com/google/deepvariant/issues/580#issuecomment-1304648870:111,Usability,guid,guide,111,"I am guessing it could be related to the use of the `--nv` flag ([docs](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html)), but I am not familiar with using GPUs with singularity containers. This [guide](https://modinst.lu.lv/wp-content/uploads/2021/03/Singularity_seminars_Aleksandrs_Gutcaits.pdf) suggests unsetting the `LD_LIBRARY_PATH`. Which version of singularity are you using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304648870
https://github.com/google/deepvariant/issues/580#issuecomment-1304648870:202,Usability,guid,guide,202,"I am guessing it could be related to the use of the `--nv` flag ([docs](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html)), but I am not familiar with using GPUs with singularity containers. This [guide](https://modinst.lu.lv/wp-content/uploads/2021/03/Singularity_seminars_Aleksandrs_Gutcaits.pdf) suggests unsetting the `LD_LIBRARY_PATH`. Which version of singularity are you using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/580#issuecomment-1304648870
https://github.com/google/deepvariant/issues/581#issuecomment-1298853697:40,Availability,error,error,40,"@Rofidagamal it looks like this is your error:. ```; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.525670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 278310 end: 278449; Fatal Python error: Aborted; ```. Which suggests that the reference may be incomplete or truncated. Can you double check that the reference file is complete? Can you verify that all the contigs are present?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1298853697
https://github.com/google/deepvariant/issues/581#issuecomment-1298853697:334,Availability,error,error,334,"@Rofidagamal it looks like this is your error:. ```; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.525670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 278310 end: 278449; Fatal Python error: Aborted; ```. Which suggests that the reference may be incomplete or truncated. Can you double check that the reference file is complete? Can you verify that all the contigs are present?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1298853697
https://github.com/google/deepvariant/issues/581#issuecomment-1298853697:341,Safety,Abort,Aborted,341,"@Rofidagamal it looks like this is your error:. ```; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.525670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 278310 end: 278449; Fatal Python error: Aborted; ```. Which suggests that the reference may be incomplete or truncated. Can you double check that the reference file is complete? Can you verify that all the contigs are present?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1298853697
https://github.com/google/deepvariant/issues/581#issuecomment-1302733421:72,Availability,down,download,72,"Thanks for replying @danielecook ; But it still has a problem, I try to download the ref genome again. This is the error; 1103 22:00:13.799613 140256758277952 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; I1103 22:00:13.818571 139829647341376 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader. **and this is my code What is the problem please ?**; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Downloads/ref/Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302733421
https://github.com/google/deepvariant/issues/581#issuecomment-1302733421:115,Availability,error,error,115,"Thanks for replying @danielecook ; But it still has a problem, I try to download the ref genome again. This is the error; 1103 22:00:13.799613 140256758277952 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; I1103 22:00:13.818571 139829647341376 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader. **and this is my code What is the problem please ?**; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Downloads/ref/Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302733421
https://github.com/google/deepvariant/issues/581#issuecomment-1302733421:286,Availability,error,errors,286,"Thanks for replying @danielecook ; But it still has a problem, I try to download the ref genome again. This is the error; 1103 22:00:13.799613 140256758277952 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; I1103 22:00:13.818571 139829647341376 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader. **and this is my code What is the problem please ?**; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Downloads/ref/Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302733421
https://github.com/google/deepvariant/issues/581#issuecomment-1302733421:716,Availability,Down,Downloads,716,"Thanks for replying @danielecook ; But it still has a problem, I try to download the ref genome again. This is the error; 1103 22:00:13.799613 140256758277952 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; I1103 22:00:13.818571 139829647341376 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader. **and this is my code What is the problem please ?**; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Downloads/ref/Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302733421
https://github.com/google/deepvariant/issues/581#issuecomment-1302738730:5,Availability,error,error,5,This error suggests you didn't properly pass the ref flag: . ```; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; ```. One potential reason: `Downloads` is not being mounted within the image. The `-v` and `-w` flags do not mount the entire filesystem. Try moving Homo_sapiens.GRCh38.dna.alt.fa to your working directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302738730
https://github.com/google/deepvariant/issues/581#issuecomment-1302738730:104,Availability,error,errors,104,This error suggests you didn't properly pass the ref flag: . ```; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; ```. One potential reason: `Downloads` is not being mounted within the image. The `-v` and `-w` flags do not mount the entire filesystem. Try moving Homo_sapiens.GRCh38.dna.alt.fa to your working directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302738730
https://github.com/google/deepvariant/issues/581#issuecomment-1302738730:173,Availability,Down,Downloads,173,This error suggests you didn't properly pass the ref flag: . ```; E1103 22:00:13.965888 140256758277952 errors.py:61] ref argument is required.; ```. One potential reason: `Downloads` is not being mounted within the image. The `-v` and `-w` flags do not mount the entire filesystem. Try moving Homo_sapiens.GRCh38.dna.alt.fa to your working directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1302738730
https://github.com/google/deepvariant/issues/581#issuecomment-1303257810:22,Availability,error,error,22,@danielecook The same error after I put the reference in the working directory,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303257810
https://github.com/google/deepvariant/issues/581#issuecomment-1303540940:51,Availability,error,error,51,"Can you provide the updated command you used?; The error is telling you that you have not provided a reference genome (`--ref`) argument. . It may be helpful to launch the docker container interactively, then verify that all the expected files are present. You can try:. ```bash; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; /bin/bash; ```. This will put you in a terminal where you can do `ls`. Make sure the reference is present in the expected location.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303540940
https://github.com/google/deepvariant/issues/581#issuecomment-1303540940:20,Deployability,update,updated,20,"Can you provide the updated command you used?; The error is telling you that you have not provided a reference genome (`--ref`) argument. . It may be helpful to launch the docker container interactively, then verify that all the expected files are present. You can try:. ```bash; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; /bin/bash; ```. This will put you in a terminal where you can do `ls`. Make sure the reference is present in the expected location.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303540940
